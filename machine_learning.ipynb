{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\krish\\ai-power-converter\\machine-learning\\simulation_results_8000.csv\")\n",
    "\n",
    "# Specify the columns you want to remove\n",
    "columns_to_remove = [\"No\"]  # Specify the columns you want to remove\n",
    "\n",
    "# Drop the specified columns\n",
    "df_truncated = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Save the truncated DataFrame back to a CSV file\n",
    "df_truncated.to_csv(\"modified_8000_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search N and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter\\machine-learning\\modified_8000_dataset.csv\")\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Scale input features to range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "# Initialize a dictionary to store the training history for each configuration\n",
    "histories = {}\n",
    "\n",
    "# Initialize dictionaries to store the evaluation metrics for each configuration\n",
    "maes = {}\n",
    "rmses = {}\n",
    "r2_scores = {}\n",
    "mapes = {}\n",
    "\n",
    "# Define a list of different values for H and Nh to try\n",
    "# H_values = [1, 2, 3, 4, 5]  # Number of hidden layers\n",
    "# Nh_values = [32, 64, 128]  # Number of neurons per hidden layer\n",
    "# Additional values for H and Nh\n",
    "H_values = [1, 2, 3, 4, 5, 6, 7]  # Number of hidden layers\n",
    "Nh_values = [16, 32, 64, 128, 256]  # Number of neurons per hidden layer\n",
    "\n",
    "\n",
    "# Iterate over different values of H and Nh\n",
    "for H in H_values:\n",
    "    for Nh in Nh_values:\n",
    "        # Initialize a Sequential model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Add the first hidden layer with batch normalization\n",
    "        model.add(tf.keras.layers.Dense(Nh, activation='relu', input_shape=(3,)))  \n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # Add subsequent hidden layers with batch normalization\n",
    "        for _ in range(H - 1):\n",
    "            model.add(tf.keras.layers.Dense(Nh, activation='relu'))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # Output layer (assuming it follows the hidden layers)\n",
    "        model.add(tf.keras.layers.Dense(6))  \n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Train the model and store the training history\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0, callbacks=[PrintEpochInfo()])\n",
    "        histories[(H, Nh)] = history\n",
    "\n",
    "        # Evaluate the model on the testing set\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        maes[(H, Nh)] = mae\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        rmses[(H, Nh)] = rmse\n",
    "\n",
    "        # Calculate R-squared (R2) Score\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        r2_scores[(H, Nh)] = r2\n",
    "\n",
    "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "        mapes[(H, Nh)] = mape\n",
    "\n",
    "# Plot the validation loss for each configuration\n",
    "for (H, Nh), history in histories.items():\n",
    "    plt.plot(history.history['val_loss'], label=f'H={H}, Nh={Nh}')\n",
    "\n",
    "plt.title('Validation Loss for Different Model Configurations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the MSE for each configuration\n",
    "# Print the evaluation metrics for each configuration\n",
    "for (H, Nh), mse in mses.items():\n",
    "    print(f'Model with H={H} and Nh={Nh}:')\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'MAE: {maes[(H, Nh)]}')\n",
    "    print(f'RMSE: {rmses[(H, Nh)]}')\n",
    "    print(f'R-squared: {r2_scores[(H, Nh)]}')\n",
    "    print(f'MAPE: {mapes[(H, Nh)]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort configurations based on MSE in ascending order\n",
    "sorted_configs_mse = sorted(mses.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on MAE in ascending order\n",
    "sorted_configs_mae = sorted(maes.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on RMSE in ascending order\n",
    "sorted_configs_rmse = sorted(rmses.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on R-squared in descending order\n",
    "sorted_configs_r2 = sorted(r2_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort configurations based on MAPE in ascending order\n",
    "sorted_configs_mape = sorted(mapes.items(), key=lambda x: x[1])\n",
    "\n",
    "# Rank each combination based on its position in the sorted lists\n",
    "ranking = {}\n",
    "for i, (config, _) in enumerate(sorted_configs_mse):\n",
    "    ranking[config] = i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_mae):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_rmse):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_r2):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_mape):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "# Sort configurations based on their overall ranking\n",
    "sorted_ranking = sorted(ranking.items(), key=lambda x: x[1])\n",
    "\n",
    "# Display the rankings\n",
    "for rank, (config, _) in enumerate(sorted_ranking, 1):\n",
    "    print(f'{rank}. Combination {config}: Overall rank {rank}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try spesific N and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the combination you want to call\n",
    "H = 6\n",
    "Nh = 128\n",
    "\n",
    "# Retrieve the model for the specified combination\n",
    "selected_model = histories[(H, Nh)].model\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "predictions = selected_model.predict(X_test)\n",
    "\n",
    "# Print the first few predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Print the corresponding true values\n",
    "print(\"True Values:\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter\\machine-learning\\modified_8000_dataset.csv\")\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Scale input features to range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "# Define the neural network model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 1\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 3\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(6)                               # Output layer with 3 neurons\n",
    "# ])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 1\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),     \n",
    "    tf.keras.layers.Dense(6, activation='softplus')                               # Output layer with 6 neurons\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with custom callback to print information at the end of each epoch\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[PrintEpochInfo()], verbose=1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save(\"ripples_prediction_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save(\"ripples_prediction_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
