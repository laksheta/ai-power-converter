{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\krish\\ai-power-converter\\machine-learning\\simulation_results_8000.csv\")\n",
    "\n",
    "# Specify the columns you want to remove\n",
    "columns_to_remove = [\"No\"]  # Specify the columns you want to remove\n",
    "\n",
    "# Drop the specified columns\n",
    "df_truncated = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Save the truncated DataFrame back to a CSV file\n",
    "df_truncated.to_csv(\"modified_8000_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search N and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Krishna\\Python_3_92\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Krishna\\Python_3_92\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From d:\\Krishna\\Python_3_92\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "501/591 [========================>.....] - ETA: 0s - loss: 2.7090Epoch 1/100, Loss: 2.5982718467712402, Val Loss: 1.7637388706207275\n",
      "591/591 [==============================] - 1s 860us/step - loss: 2.5983 - val_loss: 1.7637 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 1.3605Epoch 2/100, Loss: 1.349205732345581, Val Loss: 1.1578278541564941\n",
      "591/591 [==============================] - 0s 672us/step - loss: 1.3492 - val_loss: 1.1578 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 1.1865Epoch 3/100, Loss: 1.1869181394577026, Val Loss: 1.134670615196228\n",
      "591/591 [==============================] - 0s 667us/step - loss: 1.1869 - val_loss: 1.1347 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 1.1638Epoch 4/100, Loss: 1.1657092571258545, Val Loss: 1.1120868921279907\n",
      "591/591 [==============================] - 0s 669us/step - loss: 1.1657 - val_loss: 1.1121 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 1.1556Epoch 5/100, Loss: 1.1529808044433594, Val Loss: 1.0828181505203247\n",
      "591/591 [==============================] - 0s 671us/step - loss: 1.1530 - val_loss: 1.0828 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 1.1421Epoch 6/100, Loss: 1.1431140899658203, Val Loss: 1.0729931592941284\n",
      "591/591 [==============================] - 0s 686us/step - loss: 1.1431 - val_loss: 1.0730 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "507/591 [========================>.....] - ETA: 0s - loss: 1.1304Epoch 7/100, Loss: 1.1348072290420532, Val Loss: 1.06808602809906\n",
      "591/591 [==============================] - 0s 709us/step - loss: 1.1348 - val_loss: 1.0681 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 1.1221Epoch 8/100, Loss: 1.1267534494400024, Val Loss: 1.0671957731246948\n",
      "591/591 [==============================] - 0s 670us/step - loss: 1.1268 - val_loss: 1.0672 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 1.1091Epoch 9/100, Loss: 1.1174873113632202, Val Loss: 1.0497698783874512\n",
      "591/591 [==============================] - 0s 665us/step - loss: 1.1175 - val_loss: 1.0498 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 1.1142Epoch 10/100, Loss: 1.1100794076919556, Val Loss: 1.0544718503952026\n",
      "591/591 [==============================] - 0s 694us/step - loss: 1.1101 - val_loss: 1.0545 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 1.1077Epoch 11/100, Loss: 1.1070761680603027, Val Loss: 1.03289794921875\n",
      "591/591 [==============================] - 0s 662us/step - loss: 1.1071 - val_loss: 1.0329 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 1.1065Epoch 12/100, Loss: 1.1054584980010986, Val Loss: 1.0320242643356323\n",
      "591/591 [==============================] - 0s 672us/step - loss: 1.1055 - val_loss: 1.0320 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 1.0983Epoch 13/100, Loss: 1.0977915525436401, Val Loss: 1.0294429063796997\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.0978 - val_loss: 1.0294 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 1.0955Epoch 14/100, Loss: 1.0944645404815674, Val Loss: 1.0222136974334717\n",
      "591/591 [==============================] - 0s 672us/step - loss: 1.0945 - val_loss: 1.0222 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 1.0879Epoch 15/100, Loss: 1.0903358459472656, Val Loss: 1.038716197013855\n",
      "591/591 [==============================] - 0s 659us/step - loss: 1.0903 - val_loss: 1.0387 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 1.0876Epoch 16/100, Loss: 1.0896711349487305, Val Loss: 1.0282609462738037\n",
      "591/591 [==============================] - 0s 669us/step - loss: 1.0897 - val_loss: 1.0283 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 1.0921Epoch 17/100, Loss: 1.0854746103286743, Val Loss: 1.0276868343353271\n",
      "591/591 [==============================] - 0s 666us/step - loss: 1.0855 - val_loss: 1.0277 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 1.0837Epoch 18/100, Loss: 1.078896164894104, Val Loss: 1.0051006078720093\n",
      "591/591 [==============================] - 0s 670us/step - loss: 1.0789 - val_loss: 1.0051 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 1.0748Epoch 19/100, Loss: 1.0752522945404053, Val Loss: 1.002925157546997\n",
      "591/591 [==============================] - 0s 654us/step - loss: 1.0753 - val_loss: 1.0029 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 1.0750Epoch 20/100, Loss: 1.0788007974624634, Val Loss: 1.0010275840759277\n",
      "591/591 [==============================] - 0s 657us/step - loss: 1.0788 - val_loss: 1.0010 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 1.0752Epoch 21/100, Loss: 1.07179856300354, Val Loss: 1.0092120170593262\n",
      "591/591 [==============================] - 0s 654us/step - loss: 1.0718 - val_loss: 1.0092 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 1.0627Epoch 22/100, Loss: 1.0665818452835083, Val Loss: 0.9943454265594482\n",
      "591/591 [==============================] - 0s 659us/step - loss: 1.0666 - val_loss: 0.9943 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 1.0664Epoch 23/100, Loss: 1.0664074420928955, Val Loss: 0.9994992017745972\n",
      "591/591 [==============================] - 0s 657us/step - loss: 1.0664 - val_loss: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 1.0737Epoch 24/100, Loss: 1.0729365348815918, Val Loss: 0.9965377449989319\n",
      "591/591 [==============================] - 0s 654us/step - loss: 1.0729 - val_loss: 0.9965 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 1.0689Epoch 25/100, Loss: 1.0690290927886963, Val Loss: 0.9929614663124084\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.0690 - val_loss: 0.9930 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 1.0724Epoch 26/100, Loss: 1.068313717842102, Val Loss: 0.9931260943412781\n",
      "591/591 [==============================] - 0s 659us/step - loss: 1.0683 - val_loss: 0.9931 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 1.0628Epoch 27/100, Loss: 1.0643867254257202, Val Loss: 0.9878599643707275\n",
      "591/591 [==============================] - 0s 666us/step - loss: 1.0644 - val_loss: 0.9879 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "507/591 [========================>.....] - ETA: 0s - loss: 1.0664Epoch 28/100, Loss: 1.0637476444244385, Val Loss: 0.9898712635040283\n",
      "591/591 [==============================] - 0s 704us/step - loss: 1.0637 - val_loss: 0.9899 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0632Epoch 29/100, Loss: 1.063517689704895, Val Loss: 0.9919003844261169\n",
      "591/591 [==============================] - 0s 665us/step - loss: 1.0635 - val_loss: 0.9919 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 1.0599Epoch 30/100, Loss: 1.0597902536392212, Val Loss: 0.9933732748031616\n",
      "591/591 [==============================] - 0s 663us/step - loss: 1.0598 - val_loss: 0.9934 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 1.0594Epoch 31/100, Loss: 1.057889699935913, Val Loss: 0.9810392260551453\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.0579 - val_loss: 0.9810 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 1.0655Epoch 32/100, Loss: 1.0668351650238037, Val Loss: 0.9869869351387024\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.0668 - val_loss: 0.9870 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 1.0589Epoch 33/100, Loss: 1.0562552213668823, Val Loss: 0.9834566116333008\n",
      "591/591 [==============================] - 0s 659us/step - loss: 1.0563 - val_loss: 0.9835 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 1.0544Epoch 34/100, Loss: 1.0547107458114624, Val Loss: 0.9830384254455566\n",
      "591/591 [==============================] - 0s 659us/step - loss: 1.0547 - val_loss: 0.9830 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 1.0531Epoch 35/100, Loss: 1.0526680946350098, Val Loss: 0.9767109751701355\n",
      "591/591 [==============================] - 0s 661us/step - loss: 1.0527 - val_loss: 0.9767 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 1.0582Epoch 36/100, Loss: 1.0583301782608032, Val Loss: 0.9790502190589905\n",
      "591/591 [==============================] - 0s 650us/step - loss: 1.0583 - val_loss: 0.9791 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0516Epoch 37/100, Loss: 1.0577622652053833, Val Loss: 0.9758423566818237\n",
      "591/591 [==============================] - 0s 672us/step - loss: 1.0578 - val_loss: 0.9758 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0524Epoch 38/100, Loss: 1.0505634546279907, Val Loss: 0.9773165583610535\n",
      "591/591 [==============================] - 0s 666us/step - loss: 1.0506 - val_loss: 0.9773 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 1.0561Epoch 39/100, Loss: 1.0532598495483398, Val Loss: 0.9799370765686035\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.0533 - val_loss: 0.9799 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 1.0521Epoch 40/100, Loss: 1.054579734802246, Val Loss: 0.9790568947792053\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "591/591 [==============================] - 0s 669us/step - loss: 1.0546 - val_loss: 0.9791 - lr: 1.2500e-04\n",
      "Epoch 40: early stopping\n",
      "127/127 [==============================] - 0s 482us/step\n",
      "127/127 [==============================] - 0s 459us/step - loss: 1.0378\n",
      "Epoch 1/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 2.1944Epoch 1/100, Loss: 2.106515407562256, Val Loss: 1.2828019857406616\n",
      "591/591 [==============================] - 1s 803us/step - loss: 2.1065 - val_loss: 1.2828 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 1.1966Epoch 2/100, Loss: 1.196338415145874, Val Loss: 1.1216310262680054\n",
      "591/591 [==============================] - 0s 675us/step - loss: 1.1963 - val_loss: 1.1216 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 1.1631Epoch 3/100, Loss: 1.1623767614364624, Val Loss: 1.1115494966506958\n",
      "591/591 [==============================] - 0s 664us/step - loss: 1.1624 - val_loss: 1.1115 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 1.1423Epoch 4/100, Loss: 1.1450426578521729, Val Loss: 1.0831890106201172\n",
      "591/591 [==============================] - 0s 683us/step - loss: 1.1450 - val_loss: 1.0832 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 1.1355Epoch 5/100, Loss: 1.1354118585586548, Val Loss: 1.0649888515472412\n",
      "591/591 [==============================] - 0s 718us/step - loss: 1.1354 - val_loss: 1.0650 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 1.1300Epoch 6/100, Loss: 1.13179612159729, Val Loss: 1.064664363861084\n",
      "591/591 [==============================] - 0s 681us/step - loss: 1.1318 - val_loss: 1.0647 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 1.1151Epoch 7/100, Loss: 1.122380018234253, Val Loss: 1.0459150075912476\n",
      "591/591 [==============================] - 0s 674us/step - loss: 1.1224 - val_loss: 1.0459 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 1.1068Epoch 8/100, Loss: 1.112563967704773, Val Loss: 1.048692226409912\n",
      "591/591 [==============================] - 0s 686us/step - loss: 1.1126 - val_loss: 1.0487 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 1.0931Epoch 9/100, Loss: 1.1017624139785767, Val Loss: 1.0267318487167358\n",
      "591/591 [==============================] - 0s 674us/step - loss: 1.1018 - val_loss: 1.0267 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 1.0861Epoch 10/100, Loss: 1.0878385305404663, Val Loss: 1.020595669746399\n",
      "591/591 [==============================] - 0s 673us/step - loss: 1.0878 - val_loss: 1.0206 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 1.0805Epoch 11/100, Loss: 1.0776991844177246, Val Loss: 0.9983657598495483\n",
      "591/591 [==============================] - 0s 667us/step - loss: 1.0777 - val_loss: 0.9984 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 1.0690Epoch 12/100, Loss: 1.0662639141082764, Val Loss: 0.9817711710929871\n",
      "591/591 [==============================] - 0s 674us/step - loss: 1.0663 - val_loss: 0.9818 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 1.0506Epoch 13/100, Loss: 1.0493664741516113, Val Loss: 0.9641176462173462\n",
      "591/591 [==============================] - 0s 679us/step - loss: 1.0494 - val_loss: 0.9641 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 1.0387Epoch 14/100, Loss: 1.0378321409225464, Val Loss: 0.9560278058052063\n",
      "591/591 [==============================] - 0s 673us/step - loss: 1.0378 - val_loss: 0.9560 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 1.0277Epoch 15/100, Loss: 1.0280100107192993, Val Loss: 0.9621001482009888\n",
      "591/591 [==============================] - 0s 681us/step - loss: 1.0280 - val_loss: 0.9621 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 1.0154Epoch 16/100, Loss: 1.0174540281295776, Val Loss: 0.9484918713569641\n",
      "591/591 [==============================] - 0s 676us/step - loss: 1.0175 - val_loss: 0.9485 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 1.0203Epoch 17/100, Loss: 1.012529969215393, Val Loss: 0.9465209245681763\n",
      "591/591 [==============================] - 0s 669us/step - loss: 1.0125 - val_loss: 0.9465 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 1.0084Epoch 18/100, Loss: 1.0047719478607178, Val Loss: 0.9364399909973145\n",
      "591/591 [==============================] - 0s 672us/step - loss: 1.0048 - val_loss: 0.9364 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 1.0004Epoch 19/100, Loss: 1.0016038417816162, Val Loss: 0.9272981882095337\n",
      "591/591 [==============================] - 0s 682us/step - loss: 1.0016 - val_loss: 0.9273 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "508/591 [========================>.....] - ETA: 0s - loss: 1.0014Epoch 20/100, Loss: 1.0044288635253906, Val Loss: 0.9237798452377319\n",
      "591/591 [==============================] - 0s 708us/step - loss: 1.0044 - val_loss: 0.9238 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 1.0010Epoch 21/100, Loss: 0.9992242455482483, Val Loss: 0.9301922917366028\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9992 - val_loss: 0.9302 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.9907Epoch 22/100, Loss: 0.9936977028846741, Val Loss: 0.9195016622543335\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.9937 - val_loss: 0.9195 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.9953Epoch 23/100, Loss: 0.9932347536087036, Val Loss: 0.9247143268585205\n",
      "591/591 [==============================] - 0s 667us/step - loss: 0.9932 - val_loss: 0.9247 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0007Epoch 24/100, Loss: 0.9989678263664246, Val Loss: 0.9128434658050537\n",
      "591/591 [==============================] - 0s 668us/step - loss: 0.9990 - val_loss: 0.9128 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.9928Epoch 25/100, Loss: 0.9934385418891907, Val Loss: 0.9125420451164246\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9934 - val_loss: 0.9125 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.9954Epoch 26/100, Loss: 0.992184579372406, Val Loss: 0.9259467720985413\n",
      "591/591 [==============================] - 0s 677us/step - loss: 0.9922 - val_loss: 0.9259 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.9869Epoch 27/100, Loss: 0.989870011806488, Val Loss: 0.919073760509491\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.9899 - val_loss: 0.9191 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.9898Epoch 28/100, Loss: 0.9881435036659241, Val Loss: 0.9183160662651062\n",
      "591/591 [==============================] - 0s 675us/step - loss: 0.9881 - val_loss: 0.9183 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.9848Epoch 29/100, Loss: 0.9830378890037537, Val Loss: 0.907736599445343\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.9830 - val_loss: 0.9077 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.9772Epoch 30/100, Loss: 0.9817773699760437, Val Loss: 0.9082505702972412\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9818 - val_loss: 0.9083 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.9781Epoch 31/100, Loss: 0.9797186851501465, Val Loss: 0.9025391936302185\n",
      "591/591 [==============================] - 0s 674us/step - loss: 0.9797 - val_loss: 0.9025 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 0.9909Epoch 32/100, Loss: 0.9884203672409058, Val Loss: 0.9009109139442444\n",
      "591/591 [==============================] - 0s 695us/step - loss: 0.9884 - val_loss: 0.9009 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.9774Epoch 33/100, Loss: 0.9770459532737732, Val Loss: 0.9024054408073425\n",
      "591/591 [==============================] - 0s 715us/step - loss: 0.9770 - val_loss: 0.9024 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.9752Epoch 34/100, Loss: 0.9753883481025696, Val Loss: 0.8961706161499023\n",
      "591/591 [==============================] - 0s 670us/step - loss: 0.9754 - val_loss: 0.8962 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.9734Epoch 35/100, Loss: 0.9740481376647949, Val Loss: 0.8983486890792847\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.9740 - val_loss: 0.8983 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.9800Epoch 36/100, Loss: 0.9788233637809753, Val Loss: 0.9052144289016724\n",
      "591/591 [==============================] - 0s 663us/step - loss: 0.9788 - val_loss: 0.9052 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.9684Epoch 37/100, Loss: 0.9756866693496704, Val Loss: 0.9094048738479614\n",
      "591/591 [==============================] - 0s 674us/step - loss: 0.9757 - val_loss: 0.9094 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.9669Epoch 38/100, Loss: 0.9654110074043274, Val Loss: 0.8997641801834106\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.9654 - val_loss: 0.8998 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.9738Epoch 39/100, Loss: 0.9709120988845825, Val Loss: 0.8953690528869629\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 0s 673us/step - loss: 0.9709 - val_loss: 0.8954 - lr: 2.5000e-04\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 455us/step\n",
      "127/127 [==============================] - 0s 483us/step - loss: 0.9513\n",
      "Epoch 1/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 2.0543Epoch 1/100, Loss: 1.9526008367538452, Val Loss: 1.2493783235549927\n",
      "591/591 [==============================] - 1s 813us/step - loss: 1.9526 - val_loss: 1.2494 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 1.1487Epoch 2/100, Loss: 1.149307370185852, Val Loss: 1.0613515377044678\n",
      "591/591 [==============================] - 0s 690us/step - loss: 1.1493 - val_loss: 1.0614 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 1.1047Epoch 3/100, Loss: 1.1038368940353394, Val Loss: 1.0242940187454224\n",
      "591/591 [==============================] - 0s 674us/step - loss: 1.1038 - val_loss: 1.0243 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 1.0626Epoch 4/100, Loss: 1.0642961263656616, Val Loss: 0.9840262532234192\n",
      "591/591 [==============================] - 0s 682us/step - loss: 1.0643 - val_loss: 0.9840 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "510/591 [========================>.....] - ETA: 0s - loss: 1.0303Epoch 5/100, Loss: 1.028395652770996, Val Loss: 0.9542178511619568\n",
      "591/591 [==============================] - 0s 702us/step - loss: 1.0284 - val_loss: 0.9542 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 1.0096Epoch 6/100, Loss: 1.00983464717865, Val Loss: 0.9274134635925293\n",
      "591/591 [==============================] - 0s 678us/step - loss: 1.0098 - val_loss: 0.9274 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.9838Epoch 7/100, Loss: 0.9914401173591614, Val Loss: 0.9040927886962891\n",
      "591/591 [==============================] - 0s 679us/step - loss: 0.9914 - val_loss: 0.9041 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.9730Epoch 8/100, Loss: 0.9774348735809326, Val Loss: 0.9016337394714355\n",
      "591/591 [==============================] - 0s 752us/step - loss: 0.9774 - val_loss: 0.9016 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.9528Epoch 9/100, Loss: 0.9607492089271545, Val Loss: 0.8860148787498474\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9607 - val_loss: 0.8860 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.9469Epoch 10/100, Loss: 0.9464728832244873, Val Loss: 0.8801543712615967\n",
      "591/591 [==============================] - 0s 685us/step - loss: 0.9465 - val_loss: 0.8802 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 0.9447Epoch 11/100, Loss: 0.9417816996574402, Val Loss: 0.8813452124595642\n",
      "591/591 [==============================] - 0s 683us/step - loss: 0.9418 - val_loss: 0.8813 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.9440Epoch 12/100, Loss: 0.9404667615890503, Val Loss: 0.877253532409668\n",
      "591/591 [==============================] - 0s 691us/step - loss: 0.9405 - val_loss: 0.8773 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.9314Epoch 13/100, Loss: 0.931427538394928, Val Loss: 0.856063723564148\n",
      "591/591 [==============================] - 0s 681us/step - loss: 0.9314 - val_loss: 0.8561 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.9278Epoch 14/100, Loss: 0.9269920587539673, Val Loss: 0.8644379377365112\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9270 - val_loss: 0.8644 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.9254Epoch 15/100, Loss: 0.9246340990066528, Val Loss: 0.8567054867744446\n",
      "591/591 [==============================] - 0s 681us/step - loss: 0.9246 - val_loss: 0.8567 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.9196Epoch 16/100, Loss: 0.9192986488342285, Val Loss: 0.8460914492607117\n",
      "591/591 [==============================] - 0s 721us/step - loss: 0.9193 - val_loss: 0.8461 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.9228Epoch 17/100, Loss: 0.9167410731315613, Val Loss: 0.8417630195617676\n",
      "591/591 [==============================] - 0s 682us/step - loss: 0.9167 - val_loss: 0.8418 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.9151Epoch 18/100, Loss: 0.9106945395469666, Val Loss: 0.8741370439529419\n",
      "591/591 [==============================] - 0s 673us/step - loss: 0.9107 - val_loss: 0.8741 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.9079Epoch 19/100, Loss: 0.9093726277351379, Val Loss: 0.832796037197113\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.9094 - val_loss: 0.8328 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 0.9012Epoch 20/100, Loss: 0.9098189473152161, Val Loss: 0.8230997323989868\n",
      "591/591 [==============================] - 0s 683us/step - loss: 0.9098 - val_loss: 0.8231 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.9056Epoch 21/100, Loss: 0.9036404490470886, Val Loss: 0.898192286491394\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.9036 - val_loss: 0.8982 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8932Epoch 22/100, Loss: 0.8956663608551025, Val Loss: 0.8333617448806763\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.8957 - val_loss: 0.8334 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8962Epoch 23/100, Loss: 0.8943380117416382, Val Loss: 0.8189221024513245\n",
      "591/591 [==============================] - 0s 678us/step - loss: 0.8943 - val_loss: 0.8189 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.9008Epoch 24/100, Loss: 0.8995475769042969, Val Loss: 0.8150335550308228\n",
      "591/591 [==============================] - 0s 677us/step - loss: 0.8995 - val_loss: 0.8150 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.8949Epoch 25/100, Loss: 0.8955115079879761, Val Loss: 0.8162927627563477\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8955 - val_loss: 0.8163 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8938Epoch 26/100, Loss: 0.8893656134605408, Val Loss: 0.8214414119720459\n",
      "591/591 [==============================] - 0s 687us/step - loss: 0.8894 - val_loss: 0.8214 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.8865Epoch 27/100, Loss: 0.8853037357330322, Val Loss: 0.8166729807853699\n",
      "591/591 [==============================] - 0s 718us/step - loss: 0.8853 - val_loss: 0.8167 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8754Epoch 28/100, Loss: 0.8747469186782837, Val Loss: 0.8025947213172913\n",
      "591/591 [==============================] - 0s 675us/step - loss: 0.8747 - val_loss: 0.8026 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.8706Epoch 29/100, Loss: 0.8740555644035339, Val Loss: 0.8183674216270447\n",
      "591/591 [==============================] - 0s 685us/step - loss: 0.8741 - val_loss: 0.8184 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8691Epoch 30/100, Loss: 0.871504008769989, Val Loss: 0.8000918030738831\n",
      "591/591 [==============================] - 0s 690us/step - loss: 0.8715 - val_loss: 0.8001 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.8699Epoch 31/100, Loss: 0.8700921535491943, Val Loss: 0.7873591780662537\n",
      "591/591 [==============================] - 0s 682us/step - loss: 0.8701 - val_loss: 0.7874 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.8791Epoch 32/100, Loss: 0.879685640335083, Val Loss: 0.7924160957336426\n",
      "591/591 [==============================] - 0s 671us/step - loss: 0.8797 - val_loss: 0.7924 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.8688Epoch 33/100, Loss: 0.8664511442184448, Val Loss: 0.7818722724914551\n",
      "591/591 [==============================] - 0s 669us/step - loss: 0.8665 - val_loss: 0.7819 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.8638Epoch 34/100, Loss: 0.8634751439094543, Val Loss: 0.7793980240821838\n",
      "591/591 [==============================] - 0s 672us/step - loss: 0.8635 - val_loss: 0.7794 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.8614Epoch 35/100, Loss: 0.8606029748916626, Val Loss: 0.7801260948181152\n",
      "591/591 [==============================] - 0s 669us/step - loss: 0.8606 - val_loss: 0.7801 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8687Epoch 36/100, Loss: 0.8661109805107117, Val Loss: 0.7819012999534607\n",
      "591/591 [==============================] - 0s 671us/step - loss: 0.8661 - val_loss: 0.7819 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "525/591 [=========================>....] - ETA: 0s - loss: 0.8561Epoch 37/100, Loss: 0.8604890704154968, Val Loss: 0.7882735729217529\n",
      "591/591 [==============================] - 0s 692us/step - loss: 0.8605 - val_loss: 0.7883 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8488Epoch 38/100, Loss: 0.8483288288116455, Val Loss: 0.7787952423095703\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8483 - val_loss: 0.7788 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.8566Epoch 39/100, Loss: 0.8532536625862122, Val Loss: 0.7691409587860107\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8533 - val_loss: 0.7691 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8494Epoch 40/100, Loss: 0.8520626425743103, Val Loss: 0.7729666233062744\n",
      "591/591 [==============================] - 0s 667us/step - loss: 0.8521 - val_loss: 0.7730 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.8456Epoch 41/100, Loss: 0.8470732569694519, Val Loss: 0.7651692628860474\n",
      "591/591 [==============================] - 0s 667us/step - loss: 0.8471 - val_loss: 0.7652 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.8532Epoch 42/100, Loss: 0.8494684100151062, Val Loss: 0.7638944983482361\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8495 - val_loss: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.8466Epoch 43/100, Loss: 0.8455729484558105, Val Loss: 0.767863929271698\n",
      "591/591 [==============================] - 0s 659us/step - loss: 0.8456 - val_loss: 0.7679 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.8457Epoch 44/100, Loss: 0.8457631468772888, Val Loss: 0.7756687998771667\n",
      "591/591 [==============================] - 0s 669us/step - loss: 0.8458 - val_loss: 0.7757 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.8432Epoch 45/100, Loss: 0.849273681640625, Val Loss: 0.7670385837554932\n",
      "591/591 [==============================] - 0s 679us/step - loss: 0.8493 - val_loss: 0.7670 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.8478Epoch 46/100, Loss: 0.8486432433128357, Val Loss: 0.770452618598938\n",
      "591/591 [==============================] - 0s 681us/step - loss: 0.8486 - val_loss: 0.7705 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "508/591 [========================>.....] - ETA: 0s - loss: 0.8524Epoch 47/100, Loss: 0.8441073894500732, Val Loss: 0.7547467947006226\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.8441 - val_loss: 0.7547 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.8468Epoch 48/100, Loss: 0.8422496318817139, Val Loss: 0.7588740587234497\n",
      "591/591 [==============================] - 0s 688us/step - loss: 0.8422 - val_loss: 0.7589 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "506/591 [========================>.....] - ETA: 0s - loss: 0.8479Epoch 49/100, Loss: 0.8418624997138977, Val Loss: 0.7537822127342224\n",
      "591/591 [==============================] - 0s 707us/step - loss: 0.8419 - val_loss: 0.7538 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8465Epoch 50/100, Loss: 0.8429021239280701, Val Loss: 0.7548508048057556\n",
      "591/591 [==============================] - 0s 673us/step - loss: 0.8429 - val_loss: 0.7549 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8366Epoch 51/100, Loss: 0.8386349081993103, Val Loss: 0.7520476579666138\n",
      "591/591 [==============================] - 0s 671us/step - loss: 0.8386 - val_loss: 0.7520 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.8392Epoch 52/100, Loss: 0.8350250720977783, Val Loss: 0.7505120635032654\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8350 - val_loss: 0.7505 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.8290Epoch 53/100, Loss: 0.8335761427879333, Val Loss: 0.7542176842689514\n",
      "591/591 [==============================] - 0s 669us/step - loss: 0.8336 - val_loss: 0.7542 - lr: 1.2500e-04\n",
      "Epoch 54/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.8439Epoch 54/100, Loss: 0.8408783078193665, Val Loss: 0.7495467066764832\n",
      "591/591 [==============================] - 0s 672us/step - loss: 0.8409 - val_loss: 0.7495 - lr: 1.2500e-04\n",
      "Epoch 55/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.8380Epoch 55/100, Loss: 0.8405433297157288, Val Loss: 0.750336229801178\n",
      "591/591 [==============================] - 0s 676us/step - loss: 0.8405 - val_loss: 0.7503 - lr: 1.2500e-04\n",
      "Epoch 56/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.8378Epoch 56/100, Loss: 0.8347358703613281, Val Loss: 0.7508280277252197\n",
      "591/591 [==============================] - 0s 666us/step - loss: 0.8347 - val_loss: 0.7508 - lr: 1.2500e-04\n",
      "Epoch 57/100\n",
      "505/591 [========================>.....] - ETA: 0s - loss: 0.8447Epoch 57/100, Loss: 0.8373172283172607, Val Loss: 0.7485648989677429\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.8373 - val_loss: 0.7486 - lr: 1.2500e-04\n",
      "Epoch 58/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8294Epoch 58/100, Loss: 0.8360083699226379, Val Loss: 0.7517126798629761\n",
      "591/591 [==============================] - 0s 682us/step - loss: 0.8360 - val_loss: 0.7517 - lr: 1.2500e-04\n",
      "Epoch 59/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.8344Epoch 59/100, Loss: 0.8317585587501526, Val Loss: 0.7500304579734802\n",
      "591/591 [==============================] - 0s 672us/step - loss: 0.8318 - val_loss: 0.7500 - lr: 1.2500e-04\n",
      "Epoch 60/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.8349Epoch 60/100, Loss: 0.8369171023368835, Val Loss: 0.7491658329963684\n",
      "591/591 [==============================] - 0s 674us/step - loss: 0.8369 - val_loss: 0.7492 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.8345Epoch 61/100, Loss: 0.8284366726875305, Val Loss: 0.744886040687561\n",
      "591/591 [==============================] - 0s 673us/step - loss: 0.8284 - val_loss: 0.7449 - lr: 6.2500e-05\n",
      "Epoch 62/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.8351Epoch 62/100, Loss: 0.8348738551139832, Val Loss: 0.7461543083190918\n",
      "591/591 [==============================] - 0s 672us/step - loss: 0.8349 - val_loss: 0.7462 - lr: 6.2500e-05\n",
      "Epoch 63/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.8299Epoch 63/100, Loss: 0.8307775259017944, Val Loss: 0.7456582188606262\n",
      "591/591 [==============================] - 0s 666us/step - loss: 0.8308 - val_loss: 0.7457 - lr: 6.2500e-05\n",
      "Epoch 64/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.8367Epoch 64/100, Loss: 0.8330941796302795, Val Loss: 0.7417529225349426\n",
      "591/591 [==============================] - 0s 667us/step - loss: 0.8331 - val_loss: 0.7418 - lr: 6.2500e-05\n",
      "Epoch 65/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.8368Epoch 65/100, Loss: 0.8366332650184631, Val Loss: 0.7504581809043884\n",
      "591/591 [==============================] - 0s 677us/step - loss: 0.8366 - val_loss: 0.7505 - lr: 6.2500e-05\n",
      "Epoch 66/100\n",
      "505/591 [========================>.....] - ETA: 0s - loss: 0.8488Epoch 66/100, Loss: 0.8386180996894836, Val Loss: 0.7449779510498047\n",
      "591/591 [==============================] - 0s 709us/step - loss: 0.8386 - val_loss: 0.7450 - lr: 6.2500e-05\n",
      "Epoch 67/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.8346Epoch 67/100, Loss: 0.8329253196716309, Val Loss: 0.7476261258125305\n",
      "591/591 [==============================] - 0s 677us/step - loss: 0.8329 - val_loss: 0.7476 - lr: 6.2500e-05\n",
      "Epoch 68/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.8275Epoch 68/100, Loss: 0.82895427942276, Val Loss: 0.746555745601654\n",
      "591/591 [==============================] - 0s 677us/step - loss: 0.8290 - val_loss: 0.7466 - lr: 3.1250e-05\n",
      "Epoch 69/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.8303Epoch 69/100, Loss: 0.8328091502189636, Val Loss: 0.7444277405738831\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "591/591 [==============================] - 0s 673us/step - loss: 0.8328 - val_loss: 0.7444 - lr: 3.1250e-05\n",
      "Epoch 69: early stopping\n",
      "127/127 [==============================] - 0s 447us/step\n",
      "127/127 [==============================] - 0s 459us/step - loss: 0.7937\n",
      "Epoch 1/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 1.7138Epoch 1/100, Loss: 1.713836431503296, Val Loss: 1.2394578456878662\n",
      "591/591 [==============================] - 1s 830us/step - loss: 1.7138 - val_loss: 1.2395 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 1.1339Epoch 2/100, Loss: 1.1354212760925293, Val Loss: 1.0613118410110474\n",
      "591/591 [==============================] - 0s 694us/step - loss: 1.1354 - val_loss: 1.0613 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 1.0946Epoch 3/100, Loss: 1.0898915529251099, Val Loss: 1.0274443626403809\n",
      "591/591 [==============================] - 0s 691us/step - loss: 1.0899 - val_loss: 1.0274 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 1.0408Epoch 4/100, Loss: 1.0407847166061401, Val Loss: 0.9521690607070923\n",
      "591/591 [==============================] - 0s 723us/step - loss: 1.0408 - val_loss: 0.9522 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 1.0053Epoch 5/100, Loss: 1.0041022300720215, Val Loss: 0.9333550930023193\n",
      "591/591 [==============================] - 0s 697us/step - loss: 1.0041 - val_loss: 0.9334 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.9918Epoch 6/100, Loss: 0.9917274117469788, Val Loss: 0.954788327217102\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.9917 - val_loss: 0.9548 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.9699Epoch 7/100, Loss: 0.9763454794883728, Val Loss: 0.8849056959152222\n",
      "591/591 [==============================] - 0s 688us/step - loss: 0.9763 - val_loss: 0.8849 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.9583Epoch 8/100, Loss: 0.9620438814163208, Val Loss: 0.8779517412185669\n",
      "591/591 [==============================] - 0s 680us/step - loss: 0.9620 - val_loss: 0.8780 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "520/591 [=========================>....] - ETA: 0s - loss: 0.9334Epoch 9/100, Loss: 0.9427581429481506, Val Loss: 0.8658565878868103\n",
      "591/591 [==============================] - 0s 694us/step - loss: 0.9428 - val_loss: 0.8659 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.9270Epoch 10/100, Loss: 0.9253570437431335, Val Loss: 0.8498296737670898\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.9254 - val_loss: 0.8498 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.9180Epoch 11/100, Loss: 0.9143844246864319, Val Loss: 0.8634403944015503\n",
      "591/591 [==============================] - 0s 681us/step - loss: 0.9144 - val_loss: 0.8634 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.9107Epoch 12/100, Loss: 0.9095613956451416, Val Loss: 0.8531715869903564\n",
      "591/591 [==============================] - 0s 740us/step - loss: 0.9096 - val_loss: 0.8532 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8933Epoch 13/100, Loss: 0.8949971199035645, Val Loss: 0.8328902125358582\n",
      "591/591 [==============================] - 0s 686us/step - loss: 0.8950 - val_loss: 0.8329 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8882Epoch 14/100, Loss: 0.8861759305000305, Val Loss: 0.7989242076873779\n",
      "591/591 [==============================] - 0s 686us/step - loss: 0.8862 - val_loss: 0.7989 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.8788Epoch 15/100, Loss: 0.878922700881958, Val Loss: 0.8349394202232361\n",
      "591/591 [==============================] - 0s 694us/step - loss: 0.8789 - val_loss: 0.8349 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.8691Epoch 16/100, Loss: 0.869827926158905, Val Loss: 0.8034490346908569\n",
      "591/591 [==============================] - 0s 685us/step - loss: 0.8698 - val_loss: 0.8034 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.8726Epoch 17/100, Loss: 0.866555392742157, Val Loss: 0.7693785429000854\n",
      "591/591 [==============================] - 0s 681us/step - loss: 0.8666 - val_loss: 0.7694 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8581Epoch 18/100, Loss: 0.8557179570198059, Val Loss: 0.7849632501602173\n",
      "591/591 [==============================] - 0s 686us/step - loss: 0.8557 - val_loss: 0.7850 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 0.8471Epoch 19/100, Loss: 0.8492765426635742, Val Loss: 0.754552960395813\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.8493 - val_loss: 0.7546 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.8464Epoch 20/100, Loss: 0.8488069176673889, Val Loss: 0.7394011616706848\n",
      "591/591 [==============================] - 0s 736us/step - loss: 0.8488 - val_loss: 0.7394 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "518/591 [=========================>....] - ETA: 0s - loss: 0.8474Epoch 21/100, Loss: 0.8417345881462097, Val Loss: 0.8445627689361572\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.8417 - val_loss: 0.8446 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 0.8312Epoch 22/100, Loss: 0.8318759202957153, Val Loss: 0.7496384382247925\n",
      "591/591 [==============================] - 0s 701us/step - loss: 0.8319 - val_loss: 0.7496 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "517/591 [=========================>....] - ETA: 0s - loss: 0.8292Epoch 23/100, Loss: 0.8306933641433716, Val Loss: 0.758531928062439\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.8307 - val_loss: 0.7585 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 0.8253Epoch 24/100, Loss: 0.8256633281707764, Val Loss: 0.7178046703338623\n",
      "591/591 [==============================] - 0s 702us/step - loss: 0.8257 - val_loss: 0.7178 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "517/591 [=========================>....] - ETA: 0s - loss: 0.8161Epoch 25/100, Loss: 0.8228257298469543, Val Loss: 0.7382765412330627\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.8228 - val_loss: 0.7383 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "505/591 [========================>.....] - ETA: 0s - loss: 0.8188Epoch 26/100, Loss: 0.8154646754264832, Val Loss: 0.7330724000930786\n",
      "591/591 [==============================] - 0s 710us/step - loss: 0.8155 - val_loss: 0.7331 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8142Epoch 27/100, Loss: 0.8118707537651062, Val Loss: 0.7378513813018799\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.8119 - val_loss: 0.7379 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.8055Epoch 28/100, Loss: 0.8063023686408997, Val Loss: 0.7123221755027771\n",
      "591/591 [==============================] - 0s 688us/step - loss: 0.8063 - val_loss: 0.7123 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "517/591 [=========================>....] - ETA: 0s - loss: 0.8032Epoch 29/100, Loss: 0.8047490119934082, Val Loss: 0.7163060903549194\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.8047 - val_loss: 0.7163 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.8014Epoch 30/100, Loss: 0.803886353969574, Val Loss: 0.71019047498703\n",
      "591/591 [==============================] - 0s 686us/step - loss: 0.8039 - val_loss: 0.7102 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.8014Epoch 31/100, Loss: 0.8030904531478882, Val Loss: 0.7216551899909973\n",
      "591/591 [==============================] - 0s 694us/step - loss: 0.8031 - val_loss: 0.7217 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "525/591 [=========================>....] - ETA: 0s - loss: 0.8182Epoch 32/100, Loss: 0.8152822256088257, Val Loss: 0.7005134224891663\n",
      "591/591 [==============================] - 0s 689us/step - loss: 0.8153 - val_loss: 0.7005 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.8022Epoch 33/100, Loss: 0.8006032705307007, Val Loss: 0.7113884091377258\n",
      "591/591 [==============================] - 0s 679us/step - loss: 0.8006 - val_loss: 0.7114 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7992Epoch 34/100, Loss: 0.7979786396026611, Val Loss: 0.6983681917190552\n",
      "591/591 [==============================] - 0s 733us/step - loss: 0.7980 - val_loss: 0.6984 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "514/591 [=========================>....] - ETA: 0s - loss: 0.8010Epoch 35/100, Loss: 0.7973890900611877, Val Loss: 0.6966609954833984\n",
      "591/591 [==============================] - 0s 707us/step - loss: 0.7974 - val_loss: 0.6967 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 0.8061Epoch 36/100, Loss: 0.8033809065818787, Val Loss: 0.7083679437637329\n",
      "591/591 [==============================] - 0s 695us/step - loss: 0.8034 - val_loss: 0.7084 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 0.7964Epoch 37/100, Loss: 0.7998109459877014, Val Loss: 0.7075577974319458\n",
      "591/591 [==============================] - 0s 697us/step - loss: 0.7998 - val_loss: 0.7076 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "518/591 [=========================>....] - ETA: 0s - loss: 0.7952Epoch 38/100, Loss: 0.7908608913421631, Val Loss: 0.7097925543785095\n",
      "591/591 [==============================] - 0s 704us/step - loss: 0.7909 - val_loss: 0.7098 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "509/591 [========================>.....] - ETA: 0s - loss: 0.8010Epoch 39/100, Loss: 0.794803261756897, Val Loss: 0.695615828037262\n",
      "591/591 [==============================] - 0s 707us/step - loss: 0.7948 - val_loss: 0.6956 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "522/591 [=========================>....] - ETA: 0s - loss: 0.7932Epoch 40/100, Loss: 0.7944591045379639, Val Loss: 0.6930525302886963\n",
      "591/591 [==============================] - 0s 696us/step - loss: 0.7945 - val_loss: 0.6931 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7888Epoch 41/100, Loss: 0.7875922918319702, Val Loss: 0.703506588935852\n",
      "591/591 [==============================] - 0s 755us/step - loss: 0.7876 - val_loss: 0.7035 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 0.7968Epoch 42/100, Loss: 0.7924756407737732, Val Loss: 0.6916515231132507\n",
      "591/591 [==============================] - 0s 696us/step - loss: 0.7925 - val_loss: 0.6917 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.7910Epoch 43/100, Loss: 0.7887153029441833, Val Loss: 0.7008293867111206\n",
      "591/591 [==============================] - 0s 688us/step - loss: 0.7887 - val_loss: 0.7008 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "520/591 [=========================>....] - ETA: 0s - loss: 0.7866Epoch 44/100, Loss: 0.7894498705863953, Val Loss: 0.6949058771133423\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.7894 - val_loss: 0.6949 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "525/591 [=========================>....] - ETA: 0s - loss: 0.7860Epoch 45/100, Loss: 0.7931742668151855, Val Loss: 0.6934475302696228\n",
      "591/591 [==============================] - 0s 693us/step - loss: 0.7932 - val_loss: 0.6934 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "515/591 [=========================>....] - ETA: 0s - loss: 0.7936Epoch 46/100, Loss: 0.7952677011489868, Val Loss: 0.6955842971801758\n",
      "591/591 [==============================] - 0s 708us/step - loss: 0.7953 - val_loss: 0.6956 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.7956Epoch 47/100, Loss: 0.788893461227417, Val Loss: 0.6836531162261963\n",
      "591/591 [==============================] - 0s 697us/step - loss: 0.7889 - val_loss: 0.6837 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.7890Epoch 48/100, Loss: 0.786505937576294, Val Loss: 0.688457727432251\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.7865 - val_loss: 0.6885 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.7901Epoch 49/100, Loss: 0.7866216897964478, Val Loss: 0.6879959106445312\n",
      "591/591 [==============================] - 0s 697us/step - loss: 0.7866 - val_loss: 0.6880 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.7905Epoch 50/100, Loss: 0.7885192632675171, Val Loss: 0.6860538125038147\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.7885 - val_loss: 0.6861 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 0.7834Epoch 51/100, Loss: 0.7843258380889893, Val Loss: 0.6861618757247925\n",
      "591/591 [==============================] - 0s 701us/step - loss: 0.7843 - val_loss: 0.6862 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "512/591 [========================>.....] - ETA: 0s - loss: 0.7893Epoch 52/100, Loss: 0.7817586660385132, Val Loss: 0.6850711107254028\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "591/591 [==============================] - 0s 718us/step - loss: 0.7818 - val_loss: 0.6851 - lr: 3.1250e-05\n",
      "Epoch 52: early stopping\n",
      "127/127 [==============================] - 0s 456us/step\n",
      "127/127 [==============================] - 0s 467us/step - loss: 0.7428\n",
      "Epoch 1/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 1.5715Epoch 1/100, Loss: 1.5650297403335571, Val Loss: 1.2593573331832886\n",
      "591/591 [==============================] - 1s 833us/step - loss: 1.5650 - val_loss: 1.2594 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.1269Epoch 2/100, Loss: 1.1268572807312012, Val Loss: 1.0641380548477173\n",
      "591/591 [==============================] - 0s 748us/step - loss: 1.1269 - val_loss: 1.0641 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "512/591 [========================>.....] - ETA: 0s - loss: 1.0851Epoch 3/100, Loss: 1.0794010162353516, Val Loss: 1.0039879083633423\n",
      "591/591 [==============================] - 0s 709us/step - loss: 1.0794 - val_loss: 1.0040 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 1.0193Epoch 4/100, Loss: 1.0191031694412231, Val Loss: 0.9309480786323547\n",
      "591/591 [==============================] - 0s 700us/step - loss: 1.0191 - val_loss: 0.9309 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "512/591 [========================>.....] - ETA: 0s - loss: 0.9712Epoch 5/100, Loss: 0.9700322151184082, Val Loss: 0.8959258794784546\n",
      "591/591 [==============================] - 0s 707us/step - loss: 0.9700 - val_loss: 0.8959 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "514/591 [=========================>....] - ETA: 0s - loss: 0.9599Epoch 6/100, Loss: 0.956808865070343, Val Loss: 0.8958120346069336\n",
      "591/591 [==============================] - 0s 706us/step - loss: 0.9568 - val_loss: 0.8958 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.9380Epoch 7/100, Loss: 0.9387212991714478, Val Loss: 0.8383890986442566\n",
      "591/591 [==============================] - 0s 715us/step - loss: 0.9387 - val_loss: 0.8384 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9254Epoch 8/100, Loss: 0.9255088567733765, Val Loss: 0.8289397954940796\n",
      "591/591 [==============================] - 0s 744us/step - loss: 0.9255 - val_loss: 0.8289 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9058Epoch 9/100, Loss: 0.9059526920318604, Val Loss: 0.8338505029678345\n",
      "591/591 [==============================] - 0s 718us/step - loss: 0.9060 - val_loss: 0.8339 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.8893Epoch 10/100, Loss: 0.8892741203308105, Val Loss: 0.8280715942382812\n",
      "591/591 [==============================] - 0s 714us/step - loss: 0.8893 - val_loss: 0.8281 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "515/591 [=========================>....] - ETA: 0s - loss: 0.8808Epoch 11/100, Loss: 0.8779564499855042, Val Loss: 0.7941614389419556\n",
      "591/591 [==============================] - 0s 700us/step - loss: 0.8780 - val_loss: 0.7942 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "517/591 [=========================>....] - ETA: 0s - loss: 0.8811Epoch 12/100, Loss: 0.8750348091125488, Val Loss: 0.8353292942047119\n",
      "591/591 [==============================] - 0s 702us/step - loss: 0.8750 - val_loss: 0.8353 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.8585Epoch 13/100, Loss: 0.8602224588394165, Val Loss: 0.7975571155548096\n",
      "591/591 [==============================] - 1s 952us/step - loss: 0.8602 - val_loss: 0.7976 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.8514Epoch 14/100, Loss: 0.8512590527534485, Val Loss: 0.7906569242477417\n",
      "591/591 [==============================] - 0s 758us/step - loss: 0.8513 - val_loss: 0.7907 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "516/591 [=========================>....] - ETA: 0s - loss: 0.8452Epoch 15/100, Loss: 0.844213604927063, Val Loss: 0.7605791687965393\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.8442 - val_loss: 0.7606 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.8355Epoch 16/100, Loss: 0.8365800976753235, Val Loss: 0.7531316876411438\n",
      "591/591 [==============================] - 0s 686us/step - loss: 0.8366 - val_loss: 0.7531 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8335Epoch 17/100, Loss: 0.8318477869033813, Val Loss: 0.7388654947280884\n",
      "591/591 [==============================] - 0s 745us/step - loss: 0.8318 - val_loss: 0.7389 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8231Epoch 18/100, Loss: 0.8214928507804871, Val Loss: 0.7372676134109497\n",
      "591/591 [==============================] - 0s 729us/step - loss: 0.8215 - val_loss: 0.7373 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "512/591 [========================>.....] - ETA: 0s - loss: 0.8099Epoch 19/100, Loss: 0.8168752193450928, Val Loss: 0.7276145219802856\n",
      "591/591 [==============================] - 0s 706us/step - loss: 0.8169 - val_loss: 0.7276 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "513/591 [=========================>....] - ETA: 0s - loss: 0.8131Epoch 20/100, Loss: 0.8164173364639282, Val Loss: 0.7051576375961304\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.8164 - val_loss: 0.7052 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8111Epoch 21/100, Loss: 0.809722900390625, Val Loss: 0.7806549668312073\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.8097 - val_loss: 0.7807 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "515/591 [=========================>....] - ETA: 0s - loss: 0.7995Epoch 22/100, Loss: 0.799479603767395, Val Loss: 0.7138062119483948\n",
      "591/591 [==============================] - 0s 703us/step - loss: 0.7995 - val_loss: 0.7138 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.7999Epoch 23/100, Loss: 0.7999173402786255, Val Loss: 0.7053532600402832\n",
      "591/591 [==============================] - 0s 755us/step - loss: 0.7999 - val_loss: 0.7054 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7926Epoch 24/100, Loss: 0.79149329662323, Val Loss: 0.6797106266021729\n",
      "591/591 [==============================] - 0s 749us/step - loss: 0.7915 - val_loss: 0.6797 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "511/591 [========================>.....] - ETA: 0s - loss: 0.7801Epoch 25/100, Loss: 0.7866171598434448, Val Loss: 0.682641327381134\n",
      "591/591 [==============================] - 0s 706us/step - loss: 0.7866 - val_loss: 0.6826 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "520/591 [=========================>....] - ETA: 0s - loss: 0.7867Epoch 26/100, Loss: 0.7808563709259033, Val Loss: 0.6939501762390137\n",
      "591/591 [==============================] - 0s 693us/step - loss: 0.7809 - val_loss: 0.6940 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.7782Epoch 27/100, Loss: 0.7777272462844849, Val Loss: 0.6839668154716492\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.7777 - val_loss: 0.6840 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "515/591 [=========================>....] - ETA: 0s - loss: 0.7704Epoch 28/100, Loss: 0.7692896723747253, Val Loss: 0.6617220044136047\n",
      "591/591 [==============================] - 0s 705us/step - loss: 0.7693 - val_loss: 0.6617 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "517/591 [=========================>....] - ETA: 0s - loss: 0.7679Epoch 29/100, Loss: 0.7691013216972351, Val Loss: 0.6806641221046448\n",
      "591/591 [==============================] - 0s 696us/step - loss: 0.7691 - val_loss: 0.6807 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "519/591 [=========================>....] - ETA: 0s - loss: 0.7667Epoch 30/100, Loss: 0.7672550082206726, Val Loss: 0.6657918095588684\n",
      "591/591 [==============================] - 0s 698us/step - loss: 0.7673 - val_loss: 0.6658 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.7656Epoch 31/100, Loss: 0.7656383514404297, Val Loss: 0.6735809445381165\n",
      "591/591 [==============================] - 0s 715us/step - loss: 0.7656 - val_loss: 0.6736 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "516/591 [=========================>....] - ETA: 0s - loss: 0.7794Epoch 32/100, Loss: 0.7757413387298584, Val Loss: 0.6554780602455139\n",
      "591/591 [==============================] - 0s 701us/step - loss: 0.7757 - val_loss: 0.6555 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7635Epoch 33/100, Loss: 0.7608340978622437, Val Loss: 0.6581138968467712\n",
      "591/591 [==============================] - 0s 725us/step - loss: 0.7608 - val_loss: 0.6581 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.7603Epoch 34/100, Loss: 0.7582134008407593, Val Loss: 0.6529362201690674\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.7582 - val_loss: 0.6529 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.7611Epoch 35/100, Loss: 0.758728563785553, Val Loss: 0.6522795557975769\n",
      "591/591 [==============================] - 0s 697us/step - loss: 0.7587 - val_loss: 0.6523 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 0.7689Epoch 36/100, Loss: 0.7657444477081299, Val Loss: 0.6569163203239441\n",
      "591/591 [==============================] - 0s 694us/step - loss: 0.7657 - val_loss: 0.6569 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.7568Epoch 37/100, Loss: 0.7626780271530151, Val Loss: 0.6595591306686401\n",
      "591/591 [==============================] - 0s 684us/step - loss: 0.7627 - val_loss: 0.6596 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.7574Epoch 38/100, Loss: 0.753352165222168, Val Loss: 0.6586349606513977\n",
      "591/591 [==============================] - 0s 689us/step - loss: 0.7534 - val_loss: 0.6586 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7590Epoch 39/100, Loss: 0.7585726976394653, Val Loss: 0.6527136564254761\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.7586 - val_loss: 0.6527 - lr: 6.2500e-05\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 467us/step\n",
      "127/127 [==============================] - 0s 443us/step - loss: 0.7106\n",
      "Epoch 1/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 1.4790Epoch 1/100, Loss: 1.4619985818862915, Val Loss: 1.2723783254623413\n",
      "591/591 [==============================] - 1s 854us/step - loss: 1.4620 - val_loss: 1.2724 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 1.1087Epoch 2/100, Loss: 1.1069468259811401, Val Loss: 1.0270572900772095\n",
      "591/591 [==============================] - 0s 736us/step - loss: 1.1069 - val_loss: 1.0271 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 1.0339Epoch 3/100, Loss: 1.034039855003357, Val Loss: 0.9869282841682434\n",
      "591/591 [==============================] - 0s 743us/step - loss: 1.0340 - val_loss: 0.9869 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.9773Epoch 4/100, Loss: 0.9793683886528015, Val Loss: 0.9043424725532532\n",
      "591/591 [==============================] - 0s 753us/step - loss: 0.9794 - val_loss: 0.9043 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.9498Epoch 5/100, Loss: 0.9483780860900879, Val Loss: 0.8836795687675476\n",
      "591/591 [==============================] - 0s 726us/step - loss: 0.9484 - val_loss: 0.8837 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.9350Epoch 6/100, Loss: 0.9345883727073669, Val Loss: 0.854512095451355\n",
      "591/591 [==============================] - 0s 731us/step - loss: 0.9346 - val_loss: 0.8545 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.9139Epoch 7/100, Loss: 0.9169058799743652, Val Loss: 0.822172224521637\n",
      "591/591 [==============================] - 0s 721us/step - loss: 0.9169 - val_loss: 0.8222 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.8988Epoch 8/100, Loss: 0.9026051163673401, Val Loss: 0.8058484196662903\n",
      "591/591 [==============================] - 0s 722us/step - loss: 0.9026 - val_loss: 0.8058 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.8723Epoch 9/100, Loss: 0.8780549168586731, Val Loss: 0.8127424716949463\n",
      "591/591 [==============================] - 0s 739us/step - loss: 0.8781 - val_loss: 0.8127 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8599Epoch 10/100, Loss: 0.8607269525527954, Val Loss: 0.8419991731643677\n",
      "591/591 [==============================] - 0s 767us/step - loss: 0.8607 - val_loss: 0.8420 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8511Epoch 11/100, Loss: 0.8508307933807373, Val Loss: 0.8060025572776794\n",
      "591/591 [==============================] - 0s 725us/step - loss: 0.8508 - val_loss: 0.8060 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.8393Epoch 12/100, Loss: 0.838438868522644, Val Loss: 0.7571166753768921\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.8384 - val_loss: 0.7571 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8274Epoch 13/100, Loss: 0.8290928602218628, Val Loss: 0.7469329833984375\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.8291 - val_loss: 0.7469 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8253Epoch 14/100, Loss: 0.822823703289032, Val Loss: 0.7442616820335388\n",
      "591/591 [==============================] - 0s 729us/step - loss: 0.8228 - val_loss: 0.7443 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.8223Epoch 15/100, Loss: 0.822046160697937, Val Loss: 0.7535251379013062\n",
      "591/591 [==============================] - 0s 776us/step - loss: 0.8220 - val_loss: 0.7535 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.8194Epoch 16/100, Loss: 0.8180121183395386, Val Loss: 0.7285603284835815\n",
      "591/591 [==============================] - 0s 728us/step - loss: 0.8180 - val_loss: 0.7286 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.8171Epoch 17/100, Loss: 0.8134480118751526, Val Loss: 0.732197642326355\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.8134 - val_loss: 0.7322 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8113Epoch 18/100, Loss: 0.8075304627418518, Val Loss: 0.726804256439209\n",
      "591/591 [==============================] - 0s 735us/step - loss: 0.8075 - val_loss: 0.7268 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.8006Epoch 19/100, Loss: 0.8017721176147461, Val Loss: 0.7401805520057678\n",
      "591/591 [==============================] - 0s 745us/step - loss: 0.8018 - val_loss: 0.7402 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8021Epoch 20/100, Loss: 0.8045011758804321, Val Loss: 0.6955013871192932\n",
      "591/591 [==============================] - 0s 730us/step - loss: 0.8045 - val_loss: 0.6955 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.8038Epoch 21/100, Loss: 0.8002201914787292, Val Loss: 0.7978702783584595\n",
      "591/591 [==============================] - 0s 776us/step - loss: 0.8002 - val_loss: 0.7979 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.7890Epoch 22/100, Loss: 0.7892853021621704, Val Loss: 0.6995579600334167\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.7893 - val_loss: 0.6996 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7932Epoch 23/100, Loss: 0.7914043664932251, Val Loss: 0.7242874503135681\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.7914 - val_loss: 0.7243 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7888Epoch 24/100, Loss: 0.788401186466217, Val Loss: 0.6986613869667053\n",
      "591/591 [==============================] - 0s 730us/step - loss: 0.7884 - val_loss: 0.6987 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.7856Epoch 25/100, Loss: 0.7847527861595154, Val Loss: 0.6882906556129456\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.7848 - val_loss: 0.6883 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.7836Epoch 26/100, Loss: 0.7791592478752136, Val Loss: 0.684926450252533\n",
      "591/591 [==============================] - 0s 778us/step - loss: 0.7792 - val_loss: 0.6849 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7788Epoch 27/100, Loss: 0.7767913937568665, Val Loss: 0.6848651766777039\n",
      "591/591 [==============================] - 0s 726us/step - loss: 0.7768 - val_loss: 0.6849 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7727Epoch 28/100, Loss: 0.7746279835700989, Val Loss: 0.6929344534873962\n",
      "591/591 [==============================] - 0s 725us/step - loss: 0.7746 - val_loss: 0.6929 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7735Epoch 29/100, Loss: 0.7741381525993347, Val Loss: 0.666000485420227\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.7741 - val_loss: 0.6660 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7707Epoch 30/100, Loss: 0.7721834778785706, Val Loss: 0.6655601263046265\n",
      "591/591 [==============================] - 0s 731us/step - loss: 0.7722 - val_loss: 0.6656 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.7718Epoch 31/100, Loss: 0.7712152600288391, Val Loss: 0.6636859774589539\n",
      "591/591 [==============================] - 0s 763us/step - loss: 0.7712 - val_loss: 0.6637 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7836Epoch 32/100, Loss: 0.7835780382156372, Val Loss: 0.6592612862586975\n",
      "591/591 [==============================] - 0s 735us/step - loss: 0.7836 - val_loss: 0.6593 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7700Epoch 33/100, Loss: 0.7678751349449158, Val Loss: 0.6739064455032349\n",
      "591/591 [==============================] - 0s 726us/step - loss: 0.7679 - val_loss: 0.6739 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7658Epoch 34/100, Loss: 0.7650277614593506, Val Loss: 0.6591196060180664\n",
      "591/591 [==============================] - 0s 727us/step - loss: 0.7650 - val_loss: 0.6591 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7656Epoch 35/100, Loss: 0.7649220824241638, Val Loss: 0.6695474982261658\n",
      "591/591 [==============================] - 0s 730us/step - loss: 0.7649 - val_loss: 0.6695 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.7738Epoch 36/100, Loss: 0.7717004418373108, Val Loss: 0.6737199425697327\n",
      "591/591 [==============================] - 0s 774us/step - loss: 0.7717 - val_loss: 0.6737 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7633Epoch 37/100, Loss: 0.7682955265045166, Val Loss: 0.683586835861206\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "591/591 [==============================] - 0s 734us/step - loss: 0.7683 - val_loss: 0.6836 - lr: 2.5000e-04\n",
      "Epoch 37: early stopping\n",
      "127/127 [==============================] - 0s 461us/step\n",
      "127/127 [==============================] - 0s 470us/step - loss: 0.7157\n",
      "Epoch 1/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 2.3769Epoch 1/100, Loss: 2.347769260406494, Val Loss: 1.4828157424926758\n",
      "591/591 [==============================] - 1s 870us/step - loss: 2.3478 - val_loss: 1.4828 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.3219Epoch 2/100, Loss: 1.3177472352981567, Val Loss: 1.0873390436172485\n",
      "591/591 [==============================] - 0s 729us/step - loss: 1.3177 - val_loss: 1.0873 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 1.1548Epoch 3/100, Loss: 1.1532769203186035, Val Loss: 1.0676851272583008\n",
      "591/591 [==============================] - 0s 767us/step - loss: 1.1533 - val_loss: 1.0677 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 1.1068Epoch 4/100, Loss: 1.1092318296432495, Val Loss: 1.0108612775802612\n",
      "591/591 [==============================] - 0s 725us/step - loss: 1.1092 - val_loss: 1.0109 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 1.0841Epoch 5/100, Loss: 1.0825386047363281, Val Loss: 0.9838179349899292\n",
      "591/591 [==============================] - 0s 733us/step - loss: 1.0825 - val_loss: 0.9838 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 1.0662Epoch 6/100, Loss: 1.0653746128082275, Val Loss: 0.9654103517532349\n",
      "591/591 [==============================] - 0s 736us/step - loss: 1.0654 - val_loss: 0.9654 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.0405Epoch 7/100, Loss: 1.0422612428665161, Val Loss: 0.9899523854255676\n",
      "591/591 [==============================] - 0s 721us/step - loss: 1.0423 - val_loss: 0.9900 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 1.0139Epoch 8/100, Loss: 1.0162761211395264, Val Loss: 0.9726911187171936\n",
      "591/591 [==============================] - 0s 763us/step - loss: 1.0163 - val_loss: 0.9727 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.9694Epoch 9/100, Loss: 0.9742468595504761, Val Loss: 0.8726580142974854\n",
      "591/591 [==============================] - 0s 734us/step - loss: 0.9742 - val_loss: 0.8727 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.9260Epoch 10/100, Loss: 0.9260076880455017, Val Loss: 0.8544344902038574\n",
      "591/591 [==============================] - 0s 723us/step - loss: 0.9260 - val_loss: 0.8544 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8997Epoch 11/100, Loss: 0.9000147581100464, Val Loss: 0.7797099351882935\n",
      "591/591 [==============================] - 0s 721us/step - loss: 0.9000 - val_loss: 0.7797 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.8919Epoch 12/100, Loss: 0.8896607160568237, Val Loss: 0.8005898594856262\n",
      "591/591 [==============================] - 0s 741us/step - loss: 0.8897 - val_loss: 0.8006 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.8780Epoch 13/100, Loss: 0.8788948059082031, Val Loss: 0.747294545173645\n",
      "591/591 [==============================] - 0s 766us/step - loss: 0.8789 - val_loss: 0.7473 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8665Epoch 14/100, Loss: 0.8667885661125183, Val Loss: 0.7982285618782043\n",
      "591/591 [==============================] - 0s 723us/step - loss: 0.8668 - val_loss: 0.7982 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8576Epoch 15/100, Loss: 0.8606528043746948, Val Loss: 0.7343510389328003\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.8607 - val_loss: 0.7344 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.8575Epoch 16/100, Loss: 0.857480525970459, Val Loss: 0.8086954951286316\n",
      "591/591 [==============================] - 0s 715us/step - loss: 0.8575 - val_loss: 0.8087 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8591Epoch 17/100, Loss: 0.8580437302589417, Val Loss: 0.7374762892723083\n",
      "591/591 [==============================] - 0s 718us/step - loss: 0.8580 - val_loss: 0.7375 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 0.8529Epoch 18/100, Loss: 0.8491206169128418, Val Loss: 0.7226428389549255\n",
      "591/591 [==============================] - 0s 776us/step - loss: 0.8491 - val_loss: 0.7226 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8400Epoch 19/100, Loss: 0.8410026431083679, Val Loss: 0.7883626818656921\n",
      "591/591 [==============================] - 0s 724us/step - loss: 0.8410 - val_loss: 0.7884 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8438Epoch 20/100, Loss: 0.8447874784469604, Val Loss: 0.745435893535614\n",
      "591/591 [==============================] - 0s 720us/step - loss: 0.8448 - val_loss: 0.7454 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8375Epoch 21/100, Loss: 0.8346794843673706, Val Loss: 0.7179232835769653\n",
      "591/591 [==============================] - 0s 766us/step - loss: 0.8347 - val_loss: 0.7179 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.8300Epoch 22/100, Loss: 0.8310157656669617, Val Loss: 0.7446972727775574\n",
      "591/591 [==============================] - 0s 763us/step - loss: 0.8310 - val_loss: 0.7447 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8326Epoch 23/100, Loss: 0.8324790000915527, Val Loss: 0.7101202607154846\n",
      "591/591 [==============================] - 0s 726us/step - loss: 0.8325 - val_loss: 0.7101 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.8402Epoch 24/100, Loss: 0.8384904861450195, Val Loss: 0.7378945350646973\n",
      "591/591 [==============================] - 0s 755us/step - loss: 0.8385 - val_loss: 0.7379 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.8328Epoch 25/100, Loss: 0.8317866921424866, Val Loss: 0.7605759501457214\n",
      "591/591 [==============================] - 0s 731us/step - loss: 0.8318 - val_loss: 0.7606 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "506/591 [========================>.....] - ETA: 0s - loss: 0.8318Epoch 26/100, Loss: 0.8278019428253174, Val Loss: 0.7132769227027893\n",
      "591/591 [==============================] - 0s 713us/step - loss: 0.8278 - val_loss: 0.7133 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.8136Epoch 27/100, Loss: 0.8154774308204651, Val Loss: 0.697555661201477\n",
      "591/591 [==============================] - 0s 768us/step - loss: 0.8155 - val_loss: 0.6976 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8167Epoch 28/100, Loss: 0.8163560628890991, Val Loss: 0.6897731423377991\n",
      "591/591 [==============================] - 0s 716us/step - loss: 0.8164 - val_loss: 0.6898 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.8086Epoch 29/100, Loss: 0.8091897964477539, Val Loss: 0.693818986415863\n",
      "591/591 [==============================] - 0s 730us/step - loss: 0.8092 - val_loss: 0.6938 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.8143Epoch 30/100, Loss: 0.8129855394363403, Val Loss: 0.7054855823516846\n",
      "591/591 [==============================] - 0s 728us/step - loss: 0.8130 - val_loss: 0.7055 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8129Epoch 31/100, Loss: 0.8123319149017334, Val Loss: 0.7249794602394104\n",
      "591/591 [==============================] - 0s 754us/step - loss: 0.8123 - val_loss: 0.7250 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.8139Epoch 32/100, Loss: 0.8135797381401062, Val Loss: 0.6780647039413452\n",
      "591/591 [==============================] - 0s 733us/step - loss: 0.8136 - val_loss: 0.6781 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8063Epoch 33/100, Loss: 0.8031274080276489, Val Loss: 0.674602210521698\n",
      "591/591 [==============================] - 0s 735us/step - loss: 0.8031 - val_loss: 0.6746 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8003Epoch 34/100, Loss: 0.799325168132782, Val Loss: 0.7014539837837219\n",
      "591/591 [==============================] - 0s 724us/step - loss: 0.7993 - val_loss: 0.7015 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8011Epoch 35/100, Loss: 0.8012353181838989, Val Loss: 0.6676672697067261\n",
      "591/591 [==============================] - 0s 759us/step - loss: 0.8012 - val_loss: 0.6677 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.8051Epoch 36/100, Loss: 0.8031669855117798, Val Loss: 0.6719786524772644\n",
      "591/591 [==============================] - 0s 758us/step - loss: 0.8032 - val_loss: 0.6720 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.7991Epoch 37/100, Loss: 0.8049542307853699, Val Loss: 0.6828497648239136\n",
      "591/591 [==============================] - 0s 719us/step - loss: 0.8050 - val_loss: 0.6828 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7972Epoch 38/100, Loss: 0.7968608736991882, Val Loss: 0.6752709746360779\n",
      "591/591 [==============================] - 0s 728us/step - loss: 0.7969 - val_loss: 0.6753 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.8012Epoch 39/100, Loss: 0.8004564046859741, Val Loss: 0.6684891581535339\n",
      "591/591 [==============================] - 0s 719us/step - loss: 0.8005 - val_loss: 0.6685 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7925Epoch 40/100, Loss: 0.7952576875686646, Val Loss: 0.670793890953064\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "591/591 [==============================] - 0s 743us/step - loss: 0.7953 - val_loss: 0.6708 - lr: 1.2500e-04\n",
      "Epoch 40: early stopping\n",
      "127/127 [==============================] - 0s 468us/step\n",
      "127/127 [==============================] - 0s 459us/step - loss: 0.7235\n",
      "Epoch 1/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 1.9742Epoch 1/100, Loss: 1.941635012626648, Val Loss: 1.3538326025009155\n",
      "591/591 [==============================] - 1s 876us/step - loss: 1.9416 - val_loss: 1.3538 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 1.1648Epoch 2/100, Loss: 1.1652278900146484, Val Loss: 1.0021238327026367\n",
      "591/591 [==============================] - 0s 723us/step - loss: 1.1652 - val_loss: 1.0021 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 1.0343Epoch 3/100, Loss: 1.0343153476715088, Val Loss: 0.9466636180877686\n",
      "591/591 [==============================] - 0s 722us/step - loss: 1.0343 - val_loss: 0.9467 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.9688Epoch 4/100, Loss: 0.9714694023132324, Val Loss: 0.878527045249939\n",
      "591/591 [==============================] - 0s 755us/step - loss: 0.9715 - val_loss: 0.8785 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.9418Epoch 5/100, Loss: 0.9402963519096375, Val Loss: 0.8556876182556152\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.9403 - val_loss: 0.8557 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9218Epoch 6/100, Loss: 0.9222896099090576, Val Loss: 0.8792296648025513\n",
      "591/591 [==============================] - 0s 719us/step - loss: 0.9223 - val_loss: 0.8792 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8986Epoch 7/100, Loss: 0.9018073081970215, Val Loss: 0.8043991327285767\n",
      "591/591 [==============================] - 0s 737us/step - loss: 0.9018 - val_loss: 0.8044 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.8842Epoch 8/100, Loss: 0.8852060437202454, Val Loss: 0.7876830697059631\n",
      "591/591 [==============================] - 0s 725us/step - loss: 0.8852 - val_loss: 0.7877 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.8550Epoch 9/100, Loss: 0.8634288311004639, Val Loss: 0.7698184251785278\n",
      "591/591 [==============================] - 0s 758us/step - loss: 0.8634 - val_loss: 0.7698 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.8509Epoch 10/100, Loss: 0.8529863953590393, Val Loss: 0.7628151178359985\n",
      "591/591 [==============================] - 0s 745us/step - loss: 0.8530 - val_loss: 0.7628 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8321Epoch 11/100, Loss: 0.8323465585708618, Val Loss: 0.7477951645851135\n",
      "591/591 [==============================] - 0s 721us/step - loss: 0.8323 - val_loss: 0.7478 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.8247Epoch 12/100, Loss: 0.8232479095458984, Val Loss: 0.7333096861839294\n",
      "591/591 [==============================] - 0s 728us/step - loss: 0.8232 - val_loss: 0.7333 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "520/591 [=========================>....] - ETA: 0s - loss: 0.8105Epoch 13/100, Loss: 0.813470721244812, Val Loss: 0.7093362808227539\n",
      "591/591 [==============================] - 0s 786us/step - loss: 0.8135 - val_loss: 0.7093 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.8045Epoch 14/100, Loss: 0.804630696773529, Val Loss: 0.7022154331207275\n",
      "591/591 [==============================] - 0s 719us/step - loss: 0.8046 - val_loss: 0.7022 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7909Epoch 15/100, Loss: 0.7938014268875122, Val Loss: 0.7478888034820557\n",
      "591/591 [==============================] - 0s 732us/step - loss: 0.7938 - val_loss: 0.7479 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7914Epoch 16/100, Loss: 0.7913723587989807, Val Loss: 0.6830689907073975\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.7914 - val_loss: 0.6831 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7839Epoch 17/100, Loss: 0.7828404307365417, Val Loss: 0.722173810005188\n",
      "591/591 [==============================] - 0s 758us/step - loss: 0.7828 - val_loss: 0.7222 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.7833Epoch 18/100, Loss: 0.7790334820747375, Val Loss: 0.6821130514144897\n",
      "591/591 [==============================] - 0s 744us/step - loss: 0.7790 - val_loss: 0.6821 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7672Epoch 19/100, Loss: 0.7678999900817871, Val Loss: 0.7413148880004883\n",
      "591/591 [==============================] - 0s 736us/step - loss: 0.7679 - val_loss: 0.7413 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7659Epoch 20/100, Loss: 0.7674040794372559, Val Loss: 0.708487331867218\n",
      "591/591 [==============================] - 0s 731us/step - loss: 0.7674 - val_loss: 0.7085 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7586Epoch 21/100, Loss: 0.75387042760849, Val Loss: 0.7275651693344116\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "591/591 [==============================] - 0s 768us/step - loss: 0.7539 - val_loss: 0.7276 - lr: 0.0010\n",
      "Epoch 21: early stopping\n",
      "127/127 [==============================] - 0s 487us/step\n",
      "127/127 [==============================] - 0s 479us/step - loss: 0.7467\n",
      "Epoch 1/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 1.7426Epoch 1/100, Loss: 1.6939904689788818, Val Loss: 1.2888175249099731\n",
      "591/591 [==============================] - 1s 908us/step - loss: 1.6940 - val_loss: 1.2888 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.1225Epoch 2/100, Loss: 1.1218384504318237, Val Loss: 1.0018924474716187\n",
      "591/591 [==============================] - 0s 755us/step - loss: 1.1218 - val_loss: 1.0019 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 1.0399Epoch 3/100, Loss: 1.0407326221466064, Val Loss: 0.966460645198822\n",
      "591/591 [==============================] - 0s 754us/step - loss: 1.0407 - val_loss: 0.9665 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.9805Epoch 4/100, Loss: 0.981101930141449, Val Loss: 0.8345251679420471\n",
      "591/591 [==============================] - 0s 787us/step - loss: 0.9811 - val_loss: 0.8345 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.9508Epoch 5/100, Loss: 0.9463939666748047, Val Loss: 0.8470130562782288\n",
      "591/591 [==============================] - 0s 756us/step - loss: 0.9464 - val_loss: 0.8470 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.9197Epoch 6/100, Loss: 0.9178498983383179, Val Loss: 0.887231171131134\n",
      "591/591 [==============================] - 0s 754us/step - loss: 0.9178 - val_loss: 0.8872 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.8714Epoch 7/100, Loss: 0.8754185438156128, Val Loss: 0.8044167757034302\n",
      "591/591 [==============================] - 0s 738us/step - loss: 0.8754 - val_loss: 0.8044 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8192Epoch 8/100, Loss: 0.8211207985877991, Val Loss: 0.7138871550559998\n",
      "591/591 [==============================] - 0s 809us/step - loss: 0.8211 - val_loss: 0.7139 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.7861Epoch 9/100, Loss: 0.7919501066207886, Val Loss: 0.6719651818275452\n",
      "591/591 [==============================] - 0s 753us/step - loss: 0.7920 - val_loss: 0.6720 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7641Epoch 10/100, Loss: 0.7644031047821045, Val Loss: 0.7021244168281555\n",
      "591/591 [==============================] - 0s 747us/step - loss: 0.7644 - val_loss: 0.7021 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.7458Epoch 11/100, Loss: 0.743009626865387, Val Loss: 0.7251508831977844\n",
      "591/591 [==============================] - 0s 764us/step - loss: 0.7430 - val_loss: 0.7252 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.7297Epoch 12/100, Loss: 0.7269694805145264, Val Loss: 0.6972158551216125\n",
      "591/591 [==============================] - 0s 771us/step - loss: 0.7270 - val_loss: 0.6972 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.6950Epoch 13/100, Loss: 0.694962739944458, Val Loss: 0.5799765586853027\n",
      "591/591 [==============================] - 0s 744us/step - loss: 0.6950 - val_loss: 0.5800 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.6821Epoch 14/100, Loss: 0.6834871768951416, Val Loss: 0.5857390761375427\n",
      "591/591 [==============================] - 0s 754us/step - loss: 0.6835 - val_loss: 0.5857 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.6756Epoch 15/100, Loss: 0.6775091886520386, Val Loss: 0.5391345620155334\n",
      "591/591 [==============================] - 0s 743us/step - loss: 0.6775 - val_loss: 0.5391 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.6773Epoch 16/100, Loss: 0.6761975288391113, Val Loss: 0.5449727773666382\n",
      "591/591 [==============================] - 0s 787us/step - loss: 0.6762 - val_loss: 0.5450 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.6820Epoch 17/100, Loss: 0.678013265132904, Val Loss: 0.526084840297699\n",
      "591/591 [==============================] - 0s 773us/step - loss: 0.6780 - val_loss: 0.5261 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.6705Epoch 18/100, Loss: 0.6662627458572388, Val Loss: 0.5358651280403137\n",
      "591/591 [==============================] - 0s 775us/step - loss: 0.6663 - val_loss: 0.5359 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6518Epoch 19/100, Loss: 0.6560584306716919, Val Loss: 0.5621140599250793\n",
      "591/591 [==============================] - 0s 762us/step - loss: 0.6561 - val_loss: 0.5621 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6606Epoch 20/100, Loss: 0.6617315411567688, Val Loss: 0.6421776413917542\n",
      "591/591 [==============================] - 0s 809us/step - loss: 0.6617 - val_loss: 0.6422 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.6471Epoch 21/100, Loss: 0.6429604291915894, Val Loss: 0.5288954973220825\n",
      "591/591 [==============================] - 0s 751us/step - loss: 0.6430 - val_loss: 0.5289 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.6349Epoch 22/100, Loss: 0.6346310973167419, Val Loss: 0.550421416759491\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "591/591 [==============================] - 0s 771us/step - loss: 0.6346 - val_loss: 0.5504 - lr: 2.5000e-04\n",
      "Epoch 22: early stopping\n",
      "127/127 [==============================] - 0s 459us/step\n",
      "127/127 [==============================] - 0s 459us/step - loss: 0.5729\n",
      "Epoch 1/100\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 1.6130Epoch 1/100, Loss: 1.5635712146759033, Val Loss: 1.2268462181091309\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.5636 - val_loss: 1.2268 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 1.0659Epoch 2/100, Loss: 1.0626641511917114, Val Loss: 0.9367449283599854\n",
      "591/591 [==============================] - 1s 868us/step - loss: 1.0627 - val_loss: 0.9367 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.9671Epoch 3/100, Loss: 0.9595203995704651, Val Loss: 0.851854145526886\n",
      "591/591 [==============================] - 1s 875us/step - loss: 0.9595 - val_loss: 0.8519 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.8740Epoch 4/100, Loss: 0.8745038509368896, Val Loss: 0.7218311429023743\n",
      "591/591 [==============================] - 1s 939us/step - loss: 0.8745 - val_loss: 0.7218 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.8266Epoch 5/100, Loss: 0.8232637047767639, Val Loss: 0.7056031227111816\n",
      "591/591 [==============================] - 1s 877us/step - loss: 0.8233 - val_loss: 0.7056 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8039Epoch 6/100, Loss: 0.8051456809043884, Val Loss: 0.7709898948669434\n",
      "591/591 [==============================] - 1s 901us/step - loss: 0.8051 - val_loss: 0.7710 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.7813Epoch 7/100, Loss: 0.787907600402832, Val Loss: 0.7286027669906616\n",
      "591/591 [==============================] - 1s 864us/step - loss: 0.7879 - val_loss: 0.7286 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 0.7704Epoch 8/100, Loss: 0.7722996473312378, Val Loss: 0.7555835843086243\n",
      "591/591 [==============================] - 1s 882us/step - loss: 0.7723 - val_loss: 0.7556 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.7262Epoch 9/100, Loss: 0.7325842976570129, Val Loss: 0.5944125652313232\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.7326 - val_loss: 0.5944 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.7249Epoch 10/100, Loss: 0.7256632447242737, Val Loss: 0.5948250889778137\n",
      "591/591 [==============================] - 0s 846us/step - loss: 0.7257 - val_loss: 0.5948 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.7161Epoch 11/100, Loss: 0.7134348750114441, Val Loss: 0.6340380311012268\n",
      "591/591 [==============================] - 1s 877us/step - loss: 0.7134 - val_loss: 0.6340 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.7111Epoch 12/100, Loss: 0.7083806991577148, Val Loss: 0.6099218726158142\n",
      "591/591 [==============================] - 1s 854us/step - loss: 0.7084 - val_loss: 0.6099 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.6868Epoch 13/100, Loss: 0.6875914335250854, Val Loss: 0.5453338027000427\n",
      "591/591 [==============================] - 0s 832us/step - loss: 0.6876 - val_loss: 0.5453 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.6791Epoch 14/100, Loss: 0.6776591539382935, Val Loss: 0.5681812763214111\n",
      "591/591 [==============================] - 1s 882us/step - loss: 0.6777 - val_loss: 0.5682 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.6694Epoch 15/100, Loss: 0.6706677079200745, Val Loss: 0.5435299277305603\n",
      "591/591 [==============================] - 1s 850us/step - loss: 0.6707 - val_loss: 0.5435 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.6707Epoch 16/100, Loss: 0.6717942357063293, Val Loss: 0.5497987866401672\n",
      "591/591 [==============================] - 0s 842us/step - loss: 0.6718 - val_loss: 0.5498 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.6743Epoch 17/100, Loss: 0.6700541973114014, Val Loss: 0.5535740256309509\n",
      "591/591 [==============================] - 0s 832us/step - loss: 0.6701 - val_loss: 0.5536 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.6644Epoch 18/100, Loss: 0.6611166000366211, Val Loss: 0.5471752882003784\n",
      "591/591 [==============================] - 1s 864us/step - loss: 0.6611 - val_loss: 0.5472 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6407Epoch 19/100, Loss: 0.6450030207633972, Val Loss: 0.5310725569725037\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.6450 - val_loss: 0.5311 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.6412Epoch 20/100, Loss: 0.6471898555755615, Val Loss: 0.5369470715522766\n",
      "591/591 [==============================] - 1s 852us/step - loss: 0.6472 - val_loss: 0.5369 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 0.6489Epoch 21/100, Loss: 0.6429427862167358, Val Loss: 0.5070081949234009\n",
      "591/591 [==============================] - 1s 889us/step - loss: 0.6429 - val_loss: 0.5070 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.6381Epoch 22/100, Loss: 0.6380695104598999, Val Loss: 0.5323818325996399\n",
      "591/591 [==============================] - 0s 838us/step - loss: 0.6381 - val_loss: 0.5324 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6364Epoch 23/100, Loss: 0.634844183921814, Val Loss: 0.5094284415245056\n",
      "591/591 [==============================] - 1s 852us/step - loss: 0.6348 - val_loss: 0.5094 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.6455Epoch 24/100, Loss: 0.64338219165802, Val Loss: 0.48451685905456543\n",
      "591/591 [==============================] - 0s 840us/step - loss: 0.6434 - val_loss: 0.4845 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.6402Epoch 25/100, Loss: 0.6401829719543457, Val Loss: 0.5026936531066895\n",
      "591/591 [==============================] - 1s 884us/step - loss: 0.6402 - val_loss: 0.5027 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.6337Epoch 26/100, Loss: 0.631491482257843, Val Loss: 0.4880934953689575\n",
      "591/591 [==============================] - 0s 829us/step - loss: 0.6315 - val_loss: 0.4881 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.6248Epoch 27/100, Loss: 0.6243232488632202, Val Loss: 0.5169689059257507\n",
      "591/591 [==============================] - 0s 842us/step - loss: 0.6243 - val_loss: 0.5170 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.6232Epoch 28/100, Loss: 0.6225061416625977, Val Loss: 0.46982502937316895\n",
      "591/591 [==============================] - 1s 865us/step - loss: 0.6225 - val_loss: 0.4698 - lr: 6.2500e-05\n",
      "Epoch 29/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6205Epoch 29/100, Loss: 0.6196355819702148, Val Loss: 0.4771157503128052\n",
      "591/591 [==============================] - 1s 850us/step - loss: 0.6196 - val_loss: 0.4771 - lr: 6.2500e-05\n",
      "Epoch 30/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.6188Epoch 30/100, Loss: 0.6181005835533142, Val Loss: 0.4719336926937103\n",
      "591/591 [==============================] - 0s 842us/step - loss: 0.6181 - val_loss: 0.4719 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.6139Epoch 31/100, Loss: 0.6157580614089966, Val Loss: 0.4713572859764099\n",
      "591/591 [==============================] - 1s 881us/step - loss: 0.6158 - val_loss: 0.4714 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.6227Epoch 32/100, Loss: 0.623899519443512, Val Loss: 0.46212038397789\n",
      "591/591 [==============================] - 0s 844us/step - loss: 0.6239 - val_loss: 0.4621 - lr: 3.1250e-05\n",
      "Epoch 33/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6125Epoch 33/100, Loss: 0.608006477355957, Val Loss: 0.45883357524871826\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.6080 - val_loss: 0.4588 - lr: 3.1250e-05\n",
      "Epoch 34/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.6098Epoch 34/100, Loss: 0.6081575751304626, Val Loss: 0.4593978226184845\n",
      "591/591 [==============================] - 1s 893us/step - loss: 0.6082 - val_loss: 0.4594 - lr: 3.1250e-05\n",
      "Epoch 35/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.6087Epoch 35/100, Loss: 0.6090428233146667, Val Loss: 0.45775744318962097\n",
      "591/591 [==============================] - 0s 840us/step - loss: 0.6090 - val_loss: 0.4578 - lr: 3.1250e-05\n",
      "Epoch 36/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.6171Epoch 36/100, Loss: 0.616592288017273, Val Loss: 0.4607013761997223\n",
      "591/591 [==============================] - 0s 840us/step - loss: 0.6166 - val_loss: 0.4607 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.6058Epoch 37/100, Loss: 0.6127240061759949, Val Loss: 0.4545517861843109\n",
      "591/591 [==============================] - 1s 879us/step - loss: 0.6127 - val_loss: 0.4546 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.6036Epoch 38/100, Loss: 0.6029617190361023, Val Loss: 0.4580255448818207\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.6030 - val_loss: 0.4580 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.6136Epoch 39/100, Loss: 0.611059308052063, Val Loss: 0.4521358907222748\n",
      "591/591 [==============================] - 0s 842us/step - loss: 0.6111 - val_loss: 0.4521 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.6042Epoch 40/100, Loss: 0.6067507863044739, Val Loss: 0.4547514021396637\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.6068 - val_loss: 0.4548 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.5979Epoch 41/100, Loss: 0.6000568866729736, Val Loss: 0.45408132672309875\n",
      "591/591 [==============================] - 1s 876us/step - loss: 0.6001 - val_loss: 0.4541 - lr: 3.1250e-05\n",
      "Epoch 42/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6078Epoch 42/100, Loss: 0.6043258905410767, Val Loss: 0.44392597675323486\n",
      "591/591 [==============================] - 1s 854us/step - loss: 0.6043 - val_loss: 0.4439 - lr: 3.1250e-05\n",
      "Epoch 43/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.6070Epoch 43/100, Loss: 0.6052608489990234, Val Loss: 0.44994527101516724\n",
      "591/591 [==============================] - 1s 849us/step - loss: 0.6053 - val_loss: 0.4499 - lr: 3.1250e-05\n",
      "Epoch 44/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.6024Epoch 44/100, Loss: 0.6048336029052734, Val Loss: 0.4549436867237091\n",
      "591/591 [==============================] - 1s 881us/step - loss: 0.6048 - val_loss: 0.4549 - lr: 3.1250e-05\n",
      "Epoch 45/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.6013Epoch 45/100, Loss: 0.6055554151535034, Val Loss: 0.4540960192680359\n",
      "591/591 [==============================] - 0s 841us/step - loss: 0.6056 - val_loss: 0.4541 - lr: 3.1250e-05\n",
      "Epoch 46/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.6084Epoch 46/100, Loss: 0.6072401404380798, Val Loss: 0.4454410970211029\n",
      "591/591 [==============================] - 1s 847us/step - loss: 0.6072 - val_loss: 0.4454 - lr: 1.5625e-05\n",
      "Epoch 47/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.6057Epoch 47/100, Loss: 0.6002442836761475, Val Loss: 0.4483034014701843\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "591/591 [==============================] - 1s 876us/step - loss: 0.6002 - val_loss: 0.4483 - lr: 1.5625e-05\n",
      "Epoch 47: early stopping\n",
      "127/127 [==============================] - 0s 475us/step\n",
      "127/127 [==============================] - 0s 475us/step - loss: 0.4950\n",
      "Epoch 1/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 1.4659Epoch 1/100, Loss: 1.4390910863876343, Val Loss: 1.3202084302902222\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.4391 - val_loss: 1.3202 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 1.0350Epoch 2/100, Loss: 1.0305132865905762, Val Loss: 0.9153258204460144\n",
      "591/591 [==============================] - 1s 969us/step - loss: 1.0305 - val_loss: 0.9153 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.9201Epoch 3/100, Loss: 0.9194469451904297, Val Loss: 0.824035108089447\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9194 - val_loss: 0.8240 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8267Epoch 4/100, Loss: 0.8275803923606873, Val Loss: 0.6828730702400208\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.8276 - val_loss: 0.6829 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7804Epoch 5/100, Loss: 0.7786557674407959, Val Loss: 0.7082231044769287\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7787 - val_loss: 0.7082 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7442Epoch 6/100, Loss: 0.7417137622833252, Val Loss: 0.736386239528656\n",
      "591/591 [==============================] - 1s 957us/step - loss: 0.7417 - val_loss: 0.7364 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7185Epoch 7/100, Loss: 0.7245831489562988, Val Loss: 0.6054538488388062\n",
      "591/591 [==============================] - 1s 953us/step - loss: 0.7246 - val_loss: 0.6055 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.7034Epoch 8/100, Loss: 0.7044615745544434, Val Loss: 0.525270402431488\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.7045 - val_loss: 0.5253 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.6682Epoch 9/100, Loss: 0.6747906804084778, Val Loss: 0.5400401949882507\n",
      "591/591 [==============================] - 1s 937us/step - loss: 0.6748 - val_loss: 0.5400 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.6564Epoch 10/100, Loss: 0.6582268476486206, Val Loss: 0.5337375402450562\n",
      "591/591 [==============================] - 1s 937us/step - loss: 0.6582 - val_loss: 0.5337 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6365Epoch 11/100, Loss: 0.6370806097984314, Val Loss: 0.5771979689598083\n",
      "591/591 [==============================] - 1s 993us/step - loss: 0.6371 - val_loss: 0.5772 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.6009Epoch 12/100, Loss: 0.6011993288993835, Val Loss: 0.44595468044281006\n",
      "591/591 [==============================] - 1s 937us/step - loss: 0.6012 - val_loss: 0.4460 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.5874Epoch 13/100, Loss: 0.5890762209892273, Val Loss: 0.4594154357910156\n",
      "591/591 [==============================] - 1s 946us/step - loss: 0.5891 - val_loss: 0.4594 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.5757Epoch 14/100, Loss: 0.5719488263130188, Val Loss: 0.43567708134651184\n",
      "591/591 [==============================] - 1s 964us/step - loss: 0.5719 - val_loss: 0.4357 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.5643Epoch 15/100, Loss: 0.5659196972846985, Val Loss: 0.43158140778541565\n",
      "591/591 [==============================] - 1s 942us/step - loss: 0.5659 - val_loss: 0.4316 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.5551Epoch 16/100, Loss: 0.556931734085083, Val Loss: 0.40980786085128784\n",
      "591/591 [==============================] - 1s 943us/step - loss: 0.5569 - val_loss: 0.4098 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.5591Epoch 17/100, Loss: 0.5552769303321838, Val Loss: 0.4679661989212036\n",
      "591/591 [==============================] - 1s 974us/step - loss: 0.5553 - val_loss: 0.4680 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.5473Epoch 18/100, Loss: 0.5432817935943604, Val Loss: 0.37922289967536926\n",
      "591/591 [==============================] - 1s 964us/step - loss: 0.5433 - val_loss: 0.3792 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.5295Epoch 19/100, Loss: 0.531082034111023, Val Loss: 0.40883395075798035\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.5311 - val_loss: 0.4088 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.5363Epoch 20/100, Loss: 0.5376970767974854, Val Loss: 0.500035285949707\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5377 - val_loss: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.5279Epoch 21/100, Loss: 0.5254558324813843, Val Loss: 0.3865106403827667\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.5255 - val_loss: 0.3865 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4970Epoch 22/100, Loss: 0.4976256191730499, Val Loss: 0.3517424166202545\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.4976 - val_loss: 0.3517 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4944Epoch 23/100, Loss: 0.4949922561645508, Val Loss: 0.37068745493888855\n",
      "591/591 [==============================] - 1s 983us/step - loss: 0.4950 - val_loss: 0.3707 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5007Epoch 24/100, Loss: 0.4992375075817108, Val Loss: 0.3673444986343384\n",
      "591/591 [==============================] - 1s 960us/step - loss: 0.4992 - val_loss: 0.3673 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.4984Epoch 25/100, Loss: 0.49900075793266296, Val Loss: 0.3514045178890228\n",
      "591/591 [==============================] - 1s 949us/step - loss: 0.4990 - val_loss: 0.3514 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4869Epoch 26/100, Loss: 0.4868401288986206, Val Loss: 0.34043774008750916\n",
      "591/591 [==============================] - 1s 991us/step - loss: 0.4868 - val_loss: 0.3404 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.4809Epoch 27/100, Loss: 0.4800828695297241, Val Loss: 0.3396008014678955\n",
      "591/591 [==============================] - 1s 961us/step - loss: 0.4801 - val_loss: 0.3396 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.4856Epoch 28/100, Loss: 0.4853369891643524, Val Loss: 0.36598777770996094\n",
      "591/591 [==============================] - 1s 991us/step - loss: 0.4853 - val_loss: 0.3660 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.4870Epoch 29/100, Loss: 0.4869116246700287, Val Loss: 0.37982580065727234\n",
      "591/591 [==============================] - 1s 942us/step - loss: 0.4869 - val_loss: 0.3798 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4799Epoch 30/100, Loss: 0.4790593385696411, Val Loss: 0.3626424968242645\n",
      "591/591 [==============================] - 1s 949us/step - loss: 0.4791 - val_loss: 0.3626 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4680Epoch 31/100, Loss: 0.46380719542503357, Val Loss: 0.32056206464767456\n",
      "591/591 [==============================] - 1s 951us/step - loss: 0.4638 - val_loss: 0.3206 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4795Epoch 32/100, Loss: 0.4812992811203003, Val Loss: 0.333971232175827\n",
      "591/591 [==============================] - 1s 993us/step - loss: 0.4813 - val_loss: 0.3340 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4659Epoch 33/100, Loss: 0.4629211723804474, Val Loss: 0.3532528877258301\n",
      "591/591 [==============================] - 1s 956us/step - loss: 0.4629 - val_loss: 0.3533 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4615Epoch 34/100, Loss: 0.4600752294063568, Val Loss: 0.3354358971118927\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4601 - val_loss: 0.3354 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.4594Epoch 35/100, Loss: 0.45920097827911377, Val Loss: 0.3040078580379486\n",
      "591/591 [==============================] - 1s 948us/step - loss: 0.4592 - val_loss: 0.3040 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.4668Epoch 36/100, Loss: 0.46447643637657166, Val Loss: 0.31514859199523926\n",
      "591/591 [==============================] - 1s 973us/step - loss: 0.4645 - val_loss: 0.3151 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4576Epoch 37/100, Loss: 0.46390795707702637, Val Loss: 0.32990705966949463\n",
      "591/591 [==============================] - 1s 997us/step - loss: 0.4639 - val_loss: 0.3299 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.4536Epoch 38/100, Loss: 0.4525871276855469, Val Loss: 0.3080781400203705\n",
      "591/591 [==============================] - 1s 961us/step - loss: 0.4526 - val_loss: 0.3081 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.4561Epoch 39/100, Loss: 0.45479947328567505, Val Loss: 0.3043747544288635\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4548 - val_loss: 0.3044 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.4475Epoch 40/100, Loss: 0.44942015409469604, Val Loss: 0.3173852562904358\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.4494 - val_loss: 0.3174 - lr: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "127/127 [==============================] - 0s 514us/step\n",
      "127/127 [==============================] - 0s 522us/step - loss: 0.3481\n",
      "Epoch 1/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 1.3507Epoch 1/100, Loss: 1.350719928741455, Val Loss: 1.4067378044128418\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.3507 - val_loss: 1.4067 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.9926Epoch 2/100, Loss: 0.9908826351165771, Val Loss: 0.892656147480011\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9909 - val_loss: 0.8927 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.8870Epoch 3/100, Loss: 0.8874119520187378, Val Loss: 0.7623634934425354\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8874 - val_loss: 0.7624 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.8147Epoch 4/100, Loss: 0.8158352375030518, Val Loss: 0.6657277941703796\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8158 - val_loss: 0.6657 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7736Epoch 5/100, Loss: 0.7729926705360413, Val Loss: 0.6293204426765442\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7730 - val_loss: 0.6293 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.7342Epoch 6/100, Loss: 0.7344384789466858, Val Loss: 0.7033625245094299\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7344 - val_loss: 0.7034 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.7110Epoch 7/100, Loss: 0.713585615158081, Val Loss: 0.6472756266593933\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7136 - val_loss: 0.6473 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.6910Epoch 8/100, Loss: 0.6929026246070862, Val Loss: 0.5686904788017273\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6929 - val_loss: 0.5687 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6664Epoch 9/100, Loss: 0.6717720031738281, Val Loss: 0.5467766523361206\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6718 - val_loss: 0.5468 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.6529Epoch 10/100, Loss: 0.6551405787467957, Val Loss: 0.5015270113945007\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6551 - val_loss: 0.5015 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6349Epoch 11/100, Loss: 0.634756863117218, Val Loss: 0.5560529828071594\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6348 - val_loss: 0.5561 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.6370Epoch 12/100, Loss: 0.6358796954154968, Val Loss: 0.6001265645027161\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6359 - val_loss: 0.6001 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.6288Epoch 13/100, Loss: 0.6305473446846008, Val Loss: 0.47594016790390015\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6305 - val_loss: 0.4759 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.6121Epoch 14/100, Loss: 0.6098846197128296, Val Loss: 0.5314537286758423\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6099 - val_loss: 0.5315 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.6123Epoch 15/100, Loss: 0.6140162944793701, Val Loss: 0.4511805772781372\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6140 - val_loss: 0.4512 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.5998Epoch 16/100, Loss: 0.5976914763450623, Val Loss: 0.4587819278240204\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5977 - val_loss: 0.4588 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5966Epoch 17/100, Loss: 0.5959470868110657, Val Loss: 0.5515852570533752\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5959 - val_loss: 0.5516 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5807Epoch 18/100, Loss: 0.5788416862487793, Val Loss: 0.4449237883090973\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5788 - val_loss: 0.4449 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.5690Epoch 19/100, Loss: 0.5693671107292175, Val Loss: 0.48218798637390137\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5694 - val_loss: 0.4822 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5706Epoch 20/100, Loss: 0.5722383260726929, Val Loss: 0.45679527521133423\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5722 - val_loss: 0.4568 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.5620Epoch 21/100, Loss: 0.5599912405014038, Val Loss: 0.4463277757167816\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5600 - val_loss: 0.4463 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5163Epoch 22/100, Loss: 0.5162713527679443, Val Loss: 0.40455496311187744\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5163 - val_loss: 0.4046 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.5084Epoch 23/100, Loss: 0.5077223777770996, Val Loss: 0.3826766908168793\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5077 - val_loss: 0.3827 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.5150Epoch 24/100, Loss: 0.5134879946708679, Val Loss: 0.3771511912345886\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5135 - val_loss: 0.3772 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5100Epoch 25/100, Loss: 0.5096453428268433, Val Loss: 0.3880249857902527\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5096 - val_loss: 0.3880 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.5054Epoch 26/100, Loss: 0.5028795003890991, Val Loss: 0.3897530138492584\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5029 - val_loss: 0.3898 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4986Epoch 27/100, Loss: 0.49748551845550537, Val Loss: 0.37823545932769775\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4975 - val_loss: 0.3782 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4772Epoch 28/100, Loss: 0.4790794849395752, Val Loss: 0.370527982711792\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4791 - val_loss: 0.3705 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4763Epoch 29/100, Loss: 0.4765723645687103, Val Loss: 0.38627904653549194\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4766 - val_loss: 0.3863 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4731Epoch 30/100, Loss: 0.4735148549079895, Val Loss: 0.31779149174690247\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4735 - val_loss: 0.3178 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4689Epoch 31/100, Loss: 0.4663999378681183, Val Loss: 0.3305498957633972\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4664 - val_loss: 0.3305 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.4820Epoch 32/100, Loss: 0.4840584695339203, Val Loss: 0.34589144587516785\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4841 - val_loss: 0.3459 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.4704Epoch 33/100, Loss: 0.4674743711948395, Val Loss: 0.36033913493156433\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4675 - val_loss: 0.3603 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4538Epoch 34/100, Loss: 0.4523414671421051, Val Loss: 0.30363139510154724\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4523 - val_loss: 0.3036 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.4539Epoch 35/100, Loss: 0.4549023509025574, Val Loss: 0.3043080270290375\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4549 - val_loss: 0.3043 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.4640Epoch 36/100, Loss: 0.4637744426727295, Val Loss: 0.323190838098526\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4638 - val_loss: 0.3232 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4614Epoch 37/100, Loss: 0.46111297607421875, Val Loss: 0.3588600754737854\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4611 - val_loss: 0.3589 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4423Epoch 38/100, Loss: 0.4422633647918701, Val Loss: 0.31786105036735535\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4423 - val_loss: 0.3179 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4518Epoch 39/100, Loss: 0.45132678747177124, Val Loss: 0.30830153822898865\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4513 - val_loss: 0.3083 - lr: 6.2500e-05\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 843us/step\n",
      "127/127 [==============================] - 0s 613us/step - loss: 0.3459\n",
      "Epoch 1/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 2.2404Epoch 1/100, Loss: 2.1780123710632324, Val Loss: 1.361087679862976\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 2.1780 - val_loss: 1.3611 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 1.2545Epoch 2/100, Loss: 1.253339409828186, Val Loss: 1.0908199548721313\n",
      "591/591 [==============================] - 0s 825us/step - loss: 1.2533 - val_loss: 1.0908 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 1.1374Epoch 3/100, Loss: 1.137803554534912, Val Loss: 1.0125864744186401\n",
      "591/591 [==============================] - 1s 851us/step - loss: 1.1378 - val_loss: 1.0126 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 1.0766Epoch 4/100, Loss: 1.0793309211730957, Val Loss: 0.9056358337402344\n",
      "591/591 [==============================] - 0s 828us/step - loss: 1.0793 - val_loss: 0.9056 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "516/591 [=========================>....] - ETA: 0s - loss: 1.0504Epoch 5/100, Loss: 1.0491617918014526, Val Loss: 0.9973640441894531\n",
      "591/591 [==============================] - 0s 800us/step - loss: 1.0492 - val_loss: 0.9974 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 1.0195Epoch 6/100, Loss: 1.0169731378555298, Val Loss: 0.9105724096298218\n",
      "591/591 [==============================] - 1s 863us/step - loss: 1.0170 - val_loss: 0.9106 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.9939Epoch 7/100, Loss: 0.9976013898849487, Val Loss: 0.7957653403282166\n",
      "591/591 [==============================] - 1s 847us/step - loss: 0.9976 - val_loss: 0.7958 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.9662Epoch 8/100, Loss: 0.9694197177886963, Val Loss: 0.8201017379760742\n",
      "591/591 [==============================] - 0s 842us/step - loss: 0.9694 - val_loss: 0.8201 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.9362Epoch 9/100, Loss: 0.9415029883384705, Val Loss: 0.7450278997421265\n",
      "591/591 [==============================] - 1s 866us/step - loss: 0.9415 - val_loss: 0.7450 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.9337Epoch 10/100, Loss: 0.9343302249908447, Val Loss: 0.7577778697013855\n",
      "591/591 [==============================] - 0s 818us/step - loss: 0.9343 - val_loss: 0.7578 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.9131Epoch 11/100, Loss: 0.9129087924957275, Val Loss: 0.7421091794967651\n",
      "591/591 [==============================] - 0s 819us/step - loss: 0.9129 - val_loss: 0.7421 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.9053Epoch 12/100, Loss: 0.9038377404212952, Val Loss: 0.745887279510498\n",
      "591/591 [==============================] - 0s 824us/step - loss: 0.9038 - val_loss: 0.7459 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.8875Epoch 13/100, Loss: 0.8899196982383728, Val Loss: 0.7183836102485657\n",
      "591/591 [==============================] - 1s 864us/step - loss: 0.8899 - val_loss: 0.7184 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8845Epoch 14/100, Loss: 0.8837307691574097, Val Loss: 0.7281572818756104\n",
      "591/591 [==============================] - 0s 822us/step - loss: 0.8837 - val_loss: 0.7282 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.8710Epoch 15/100, Loss: 0.8750878572463989, Val Loss: 0.7079736590385437\n",
      "591/591 [==============================] - 0s 816us/step - loss: 0.8751 - val_loss: 0.7080 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.8686Epoch 16/100, Loss: 0.8690530061721802, Val Loss: 0.7153600454330444\n",
      "591/591 [==============================] - 1s 861us/step - loss: 0.8691 - val_loss: 0.7154 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.8664Epoch 17/100, Loss: 0.8644458055496216, Val Loss: 0.6885802745819092\n",
      "591/591 [==============================] - 0s 819us/step - loss: 0.8644 - val_loss: 0.6886 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8595Epoch 18/100, Loss: 0.8578714728355408, Val Loss: 0.6550374031066895\n",
      "591/591 [==============================] - 0s 814us/step - loss: 0.8579 - val_loss: 0.6550 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.8420Epoch 19/100, Loss: 0.8445755839347839, Val Loss: 0.7058957815170288\n",
      "591/591 [==============================] - 1s 868us/step - loss: 0.8446 - val_loss: 0.7059 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8449Epoch 20/100, Loss: 0.8470618724822998, Val Loss: 0.7533000707626343\n",
      "591/591 [==============================] - 0s 824us/step - loss: 0.8471 - val_loss: 0.7533 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8426Epoch 21/100, Loss: 0.8402685523033142, Val Loss: 0.6912941336631775\n",
      "591/591 [==============================] - 0s 823us/step - loss: 0.8403 - val_loss: 0.6913 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.8199Epoch 22/100, Loss: 0.8211845755577087, Val Loss: 0.7046316862106323\n",
      "591/591 [==============================] - 0s 843us/step - loss: 0.8212 - val_loss: 0.7046 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.8203Epoch 23/100, Loss: 0.8188415765762329, Val Loss: 0.6307419538497925\n",
      "591/591 [==============================] - 0s 835us/step - loss: 0.8188 - val_loss: 0.6307 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.8265Epoch 24/100, Loss: 0.8263521194458008, Val Loss: 0.6386663913726807\n",
      "591/591 [==============================] - 0s 824us/step - loss: 0.8264 - val_loss: 0.6387 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.8158Epoch 25/100, Loss: 0.8203365802764893, Val Loss: 0.6542990803718567\n",
      "591/591 [==============================] - 1s 871us/step - loss: 0.8203 - val_loss: 0.6543 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.8119Epoch 26/100, Loss: 0.8120654821395874, Val Loss: 0.6158990263938904\n",
      "591/591 [==============================] - 0s 807us/step - loss: 0.8121 - val_loss: 0.6159 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.8075Epoch 27/100, Loss: 0.8083836436271667, Val Loss: 0.6397996544837952\n",
      "591/591 [==============================] - 0s 843us/step - loss: 0.8084 - val_loss: 0.6398 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8091Epoch 28/100, Loss: 0.811777651309967, Val Loss: 0.6616549491882324\n",
      "591/591 [==============================] - 0s 824us/step - loss: 0.8118 - val_loss: 0.6617 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "520/591 [=========================>....] - ETA: 0s - loss: 0.8043Epoch 29/100, Loss: 0.8048524856567383, Val Loss: 0.6742594242095947\n",
      "591/591 [==============================] - 0s 800us/step - loss: 0.8049 - val_loss: 0.6743 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.7978Epoch 30/100, Loss: 0.7958173751831055, Val Loss: 0.6156830787658691\n",
      "591/591 [==============================] - 1s 851us/step - loss: 0.7958 - val_loss: 0.6157 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7945Epoch 31/100, Loss: 0.7947955131530762, Val Loss: 0.6099795699119568\n",
      "591/591 [==============================] - 0s 820us/step - loss: 0.7948 - val_loss: 0.6100 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8067Epoch 32/100, Loss: 0.8072564601898193, Val Loss: 0.6082301139831543\n",
      "591/591 [==============================] - 1s 849us/step - loss: 0.8073 - val_loss: 0.6082 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7924Epoch 33/100, Loss: 0.7901370525360107, Val Loss: 0.6084202527999878\n",
      "591/591 [==============================] - 0s 818us/step - loss: 0.7901 - val_loss: 0.6084 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7923Epoch 34/100, Loss: 0.7914703488349915, Val Loss: 0.641606330871582\n",
      "591/591 [==============================] - 0s 813us/step - loss: 0.7915 - val_loss: 0.6416 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.7836Epoch 35/100, Loss: 0.784140944480896, Val Loss: 0.6651581525802612\n",
      "591/591 [==============================] - 1s 854us/step - loss: 0.7841 - val_loss: 0.6652 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.7896Epoch 36/100, Loss: 0.7885010838508606, Val Loss: 0.6088975667953491\n",
      "591/591 [==============================] - 0s 837us/step - loss: 0.7885 - val_loss: 0.6089 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.7783Epoch 37/100, Loss: 0.7859954237937927, Val Loss: 0.5934948921203613\n",
      "591/591 [==============================] - 1s 876us/step - loss: 0.7860 - val_loss: 0.5935 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.7784Epoch 38/100, Loss: 0.7782278060913086, Val Loss: 0.5919876098632812\n",
      "591/591 [==============================] - 1s 898us/step - loss: 0.7782 - val_loss: 0.5920 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.7921Epoch 39/100, Loss: 0.7875792384147644, Val Loss: 0.593790590763092\n",
      "591/591 [==============================] - 1s 879us/step - loss: 0.7876 - val_loss: 0.5938 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.7841Epoch 40/100, Loss: 0.7858383655548096, Val Loss: 0.5874490141868591\n",
      "591/591 [==============================] - 1s 877us/step - loss: 0.7858 - val_loss: 0.5874 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7819Epoch 41/100, Loss: 0.7805193066596985, Val Loss: 0.5874639749526978\n",
      "591/591 [==============================] - 1s 911us/step - loss: 0.7805 - val_loss: 0.5875 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7833Epoch 42/100, Loss: 0.7845452427864075, Val Loss: 0.5970234870910645\n",
      "591/591 [==============================] - 0s 828us/step - loss: 0.7845 - val_loss: 0.5970 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.7839Epoch 43/100, Loss: 0.7813155651092529, Val Loss: 0.6069625020027161\n",
      "591/591 [==============================] - 1s 860us/step - loss: 0.7813 - val_loss: 0.6070 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7828Epoch 44/100, Loss: 0.7810121774673462, Val Loss: 0.5989440679550171\n",
      "591/591 [==============================] - 0s 812us/step - loss: 0.7810 - val_loss: 0.5989 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7794Epoch 45/100, Loss: 0.7843153476715088, Val Loss: 0.5997400283813477\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "591/591 [==============================] - 0s 820us/step - loss: 0.7843 - val_loss: 0.5997 - lr: 6.2500e-05\n",
      "Epoch 45: early stopping\n",
      "127/127 [==============================] - 0s 503us/step\n",
      "127/127 [==============================] - 0s 502us/step - loss: 0.6335\n",
      "Epoch 1/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 1.9629Epoch 1/100, Loss: 1.9054443836212158, Val Loss: 1.2380796670913696\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.9054 - val_loss: 1.2381 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 1.1270Epoch 2/100, Loss: 1.1252840757369995, Val Loss: 0.9519650936126709\n",
      "591/591 [==============================] - 1s 906us/step - loss: 1.1253 - val_loss: 0.9520 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 1.0414Epoch 3/100, Loss: 1.0348095893859863, Val Loss: 0.8687611818313599\n",
      "591/591 [==============================] - 1s 888us/step - loss: 1.0348 - val_loss: 0.8688 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.9802Epoch 4/100, Loss: 0.9813219308853149, Val Loss: 0.8509800434112549\n",
      "591/591 [==============================] - 1s 871us/step - loss: 0.9813 - val_loss: 0.8510 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.9545Epoch 5/100, Loss: 0.953427791595459, Val Loss: 0.78354412317276\n",
      "591/591 [==============================] - 1s 917us/step - loss: 0.9534 - val_loss: 0.7835 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.9388Epoch 6/100, Loss: 0.9384526014328003, Val Loss: 0.890673816204071\n",
      "591/591 [==============================] - 1s 885us/step - loss: 0.9385 - val_loss: 0.8907 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9217Epoch 7/100, Loss: 0.9229515790939331, Val Loss: 0.6786465644836426\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9230 - val_loss: 0.6786 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8874Epoch 8/100, Loss: 0.8873797059059143, Val Loss: 0.7172646522521973\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8874 - val_loss: 0.7173 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.8537Epoch 9/100, Loss: 0.8616414666175842, Val Loss: 0.6557369828224182\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8616 - val_loss: 0.6557 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.8407Epoch 10/100, Loss: 0.8428505063056946, Val Loss: 0.6942650675773621\n",
      "591/591 [==============================] - 1s 965us/step - loss: 0.8429 - val_loss: 0.6943 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8181Epoch 11/100, Loss: 0.817987859249115, Val Loss: 0.6157543659210205\n",
      "591/591 [==============================] - 1s 929us/step - loss: 0.8180 - val_loss: 0.6158 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7958Epoch 12/100, Loss: 0.7942723631858826, Val Loss: 0.6575469970703125\n",
      "591/591 [==============================] - 1s 934us/step - loss: 0.7943 - val_loss: 0.6575 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7743Epoch 13/100, Loss: 0.7756540179252625, Val Loss: 0.6047572493553162\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7757 - val_loss: 0.6048 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7620Epoch 14/100, Loss: 0.7594932913780212, Val Loss: 0.6268318891525269\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7595 - val_loss: 0.6268 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7309Epoch 15/100, Loss: 0.7344873547554016, Val Loss: 0.6021791100502014\n",
      "591/591 [==============================] - 1s 920us/step - loss: 0.7345 - val_loss: 0.6022 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.7292Epoch 16/100, Loss: 0.7280923128128052, Val Loss: 0.5946030616760254\n",
      "591/591 [==============================] - 1s 883us/step - loss: 0.7281 - val_loss: 0.5946 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.7290Epoch 17/100, Loss: 0.7232580780982971, Val Loss: 0.5825010538101196\n",
      "591/591 [==============================] - 1s 867us/step - loss: 0.7233 - val_loss: 0.5825 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7099Epoch 18/100, Loss: 0.709775984287262, Val Loss: 0.5852438807487488\n",
      "591/591 [==============================] - 1s 900us/step - loss: 0.7098 - val_loss: 0.5852 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.6812Epoch 19/100, Loss: 0.6843191385269165, Val Loss: 0.7593873143196106\n",
      "591/591 [==============================] - 1s 870us/step - loss: 0.6843 - val_loss: 0.7594 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.6819Epoch 20/100, Loss: 0.6826295256614685, Val Loss: 0.5808992981910706\n",
      "591/591 [==============================] - 1s 913us/step - loss: 0.6826 - val_loss: 0.5809 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6729Epoch 21/100, Loss: 0.6683590412139893, Val Loss: 0.5389487743377686\n",
      "591/591 [==============================] - 1s 873us/step - loss: 0.6684 - val_loss: 0.5389 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.6617Epoch 22/100, Loss: 0.6615419387817383, Val Loss: 0.5699153542518616\n",
      "591/591 [==============================] - 1s 903us/step - loss: 0.6615 - val_loss: 0.5699 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "532/591 [==========================>...] - ETA: 0s - loss: 0.6600Epoch 23/100, Loss: 0.6583933234214783, Val Loss: 0.48028817772865295\n",
      "591/591 [==============================] - 1s 885us/step - loss: 0.6584 - val_loss: 0.4803 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.6625Epoch 24/100, Loss: 0.6634882092475891, Val Loss: 0.5334818363189697\n",
      "591/591 [==============================] - 1s 891us/step - loss: 0.6635 - val_loss: 0.5335 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.6483Epoch 25/100, Loss: 0.6481698751449585, Val Loss: 0.6499236822128296\n",
      "591/591 [==============================] - 1s 919us/step - loss: 0.6482 - val_loss: 0.6499 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.6473Epoch 26/100, Loss: 0.6440392732620239, Val Loss: 0.4919053912162781\n",
      "591/591 [==============================] - 1s 885us/step - loss: 0.6440 - val_loss: 0.4919 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.6111Epoch 27/100, Loss: 0.6099539399147034, Val Loss: 0.486502468585968\n",
      "591/591 [==============================] - 1s 914us/step - loss: 0.6100 - val_loss: 0.4865 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.6148Epoch 28/100, Loss: 0.6139895915985107, Val Loss: 0.4871372580528259\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "591/591 [==============================] - 1s 888us/step - loss: 0.6140 - val_loss: 0.4871 - lr: 5.0000e-04\n",
      "Epoch 28: early stopping\n",
      "127/127 [==============================] - 0s 506us/step\n",
      "127/127 [==============================] - 0s 483us/step - loss: 0.5239\n",
      "Epoch 1/100\n",
      "  1/591 [..............................] - ETA: 6:34 - loss: 3.0603WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0002s). Check your callbacks.\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.6846Epoch 1/100, Loss: 1.6512861251831055, Val Loss: 1.2152374982833862\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.6513 - val_loss: 1.2152 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 1.0344Epoch 2/100, Loss: 1.029401421546936, Val Loss: 0.8640668392181396\n",
      "591/591 [==============================] - 1s 885us/step - loss: 1.0294 - val_loss: 0.8641 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.9297Epoch 3/100, Loss: 0.9316312074661255, Val Loss: 0.7909046411514282\n",
      "591/591 [==============================] - 1s 932us/step - loss: 0.9316 - val_loss: 0.7909 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.8694Epoch 4/100, Loss: 0.8681595921516418, Val Loss: 0.6645335555076599\n",
      "591/591 [==============================] - 1s 883us/step - loss: 0.8682 - val_loss: 0.6645 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.8236Epoch 5/100, Loss: 0.8183461427688599, Val Loss: 0.631443977355957\n",
      "591/591 [==============================] - 1s 940us/step - loss: 0.8183 - val_loss: 0.6314 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.7750Epoch 6/100, Loss: 0.7749921679496765, Val Loss: 0.5817857980728149\n",
      "591/591 [==============================] - 1s 892us/step - loss: 0.7750 - val_loss: 0.5818 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.7329Epoch 7/100, Loss: 0.7367469668388367, Val Loss: 0.5694212317466736\n",
      "591/591 [==============================] - 1s 931us/step - loss: 0.7367 - val_loss: 0.5694 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.7026Epoch 8/100, Loss: 0.7030197381973267, Val Loss: 0.6278849840164185\n",
      "591/591 [==============================] - 1s 883us/step - loss: 0.7030 - val_loss: 0.6279 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.6727Epoch 9/100, Loss: 0.6776464581489563, Val Loss: 0.5459530353546143\n",
      "591/591 [==============================] - 1s 914us/step - loss: 0.6776 - val_loss: 0.5460 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.6552Epoch 10/100, Loss: 0.6560587882995605, Val Loss: 0.5482252240180969\n",
      "591/591 [==============================] - 1s 878us/step - loss: 0.6561 - val_loss: 0.5482 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.6320Epoch 11/100, Loss: 0.6313763856887817, Val Loss: 0.551259458065033\n",
      "591/591 [==============================] - 1s 942us/step - loss: 0.6314 - val_loss: 0.5513 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 0.6295Epoch 12/100, Loss: 0.625953197479248, Val Loss: 0.5668622851371765\n",
      "591/591 [==============================] - 1s 887us/step - loss: 0.6260 - val_loss: 0.5669 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.5914Epoch 13/100, Loss: 0.594947338104248, Val Loss: 0.467899888753891\n",
      "591/591 [==============================] - 1s 884us/step - loss: 0.5949 - val_loss: 0.4679 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.5851Epoch 14/100, Loss: 0.5831618905067444, Val Loss: 0.4786396324634552\n",
      "591/591 [==============================] - 1s 928us/step - loss: 0.5832 - val_loss: 0.4786 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 0.5764Epoch 15/100, Loss: 0.5761775374412537, Val Loss: 0.41115012764930725\n",
      "591/591 [==============================] - 1s 884us/step - loss: 0.5762 - val_loss: 0.4112 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.5700Epoch 16/100, Loss: 0.5701406598091125, Val Loss: 0.41606152057647705\n",
      "591/591 [==============================] - 1s 957us/step - loss: 0.5701 - val_loss: 0.4161 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.5743Epoch 17/100, Loss: 0.5723904967308044, Val Loss: 0.41710370779037476\n",
      "591/591 [==============================] - 1s 926us/step - loss: 0.5724 - val_loss: 0.4171 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5681Epoch 18/100, Loss: 0.5645256638526917, Val Loss: 0.4753848612308502\n",
      "591/591 [==============================] - 1s 935us/step - loss: 0.5645 - val_loss: 0.4754 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.5355Epoch 19/100, Loss: 0.5362627506256104, Val Loss: 0.3693675696849823\n",
      "591/591 [==============================] - 1s 928us/step - loss: 0.5363 - val_loss: 0.3694 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5426Epoch 20/100, Loss: 0.5441486835479736, Val Loss: 0.4188564717769623\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5441 - val_loss: 0.4189 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5357Epoch 21/100, Loss: 0.5357351899147034, Val Loss: 0.3870106637477875\n",
      "591/591 [==============================] - 1s 998us/step - loss: 0.5357 - val_loss: 0.3870 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.5298Epoch 22/100, Loss: 0.5293691754341125, Val Loss: 0.40099459886550903\n",
      "591/591 [==============================] - 1s 924us/step - loss: 0.5294 - val_loss: 0.4010 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.5219Epoch 23/100, Loss: 0.5209221839904785, Val Loss: 0.3425387442111969\n",
      "591/591 [==============================] - 1s 881us/step - loss: 0.5209 - val_loss: 0.3425 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "528/591 [=========================>....] - ETA: 0s - loss: 0.5287Epoch 24/100, Loss: 0.5298265814781189, Val Loss: 0.3403439521789551\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.5298 - val_loss: 0.3403 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.5273Epoch 25/100, Loss: 0.5276422500610352, Val Loss: 0.36097481846809387\n",
      "591/591 [==============================] - 1s 877us/step - loss: 0.5276 - val_loss: 0.3610 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5207Epoch 26/100, Loss: 0.5187904834747314, Val Loss: 0.3446463346481323\n",
      "591/591 [==============================] - 1s 919us/step - loss: 0.5188 - val_loss: 0.3446 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5149Epoch 27/100, Loss: 0.5153946876525879, Val Loss: 0.36004874110221863\n",
      "591/591 [==============================] - 1s 855us/step - loss: 0.5154 - val_loss: 0.3600 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.5164Epoch 28/100, Loss: 0.5153607726097107, Val Loss: 0.34225189685821533\n",
      "591/591 [==============================] - 1s 981us/step - loss: 0.5154 - val_loss: 0.3423 - lr: 6.2500e-05\n",
      "Epoch 29/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5159Epoch 29/100, Loss: 0.5161028504371643, Val Loss: 0.3290926516056061\n",
      "591/591 [==============================] - 1s 907us/step - loss: 0.5161 - val_loss: 0.3291 - lr: 6.2500e-05\n",
      "Epoch 30/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.5085Epoch 30/100, Loss: 0.511749804019928, Val Loss: 0.32675930857658386\n",
      "591/591 [==============================] - 1s 872us/step - loss: 0.5117 - val_loss: 0.3268 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.5131Epoch 31/100, Loss: 0.5103464722633362, Val Loss: 0.3364863693714142\n",
      "591/591 [==============================] - 1s 922us/step - loss: 0.5103 - val_loss: 0.3365 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.5239Epoch 32/100, Loss: 0.5253442525863647, Val Loss: 0.32442551851272583\n",
      "591/591 [==============================] - 1s 873us/step - loss: 0.5253 - val_loss: 0.3244 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.5088Epoch 33/100, Loss: 0.5060830116271973, Val Loss: 0.33490028977394104\n",
      "591/591 [==============================] - 1s 921us/step - loss: 0.5061 - val_loss: 0.3349 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.5069Epoch 34/100, Loss: 0.5067790746688843, Val Loss: 0.3333190977573395\n",
      "591/591 [==============================] - 1s 869us/step - loss: 0.5068 - val_loss: 0.3333 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5088Epoch 35/100, Loss: 0.5089887380599976, Val Loss: 0.3251156508922577\n",
      "591/591 [==============================] - 1s 913us/step - loss: 0.5090 - val_loss: 0.3251 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.5156Epoch 36/100, Loss: 0.5147563815116882, Val Loss: 0.3230192959308624\n",
      "591/591 [==============================] - 0s 844us/step - loss: 0.5148 - val_loss: 0.3230 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.5053Epoch 37/100, Loss: 0.5140204429626465, Val Loss: 0.33348309993743896\n",
      "591/591 [==============================] - 1s 953us/step - loss: 0.5140 - val_loss: 0.3335 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5032Epoch 38/100, Loss: 0.5020620226860046, Val Loss: 0.3240810036659241\n",
      "591/591 [==============================] - 1s 930us/step - loss: 0.5021 - val_loss: 0.3241 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.5142Epoch 39/100, Loss: 0.5104644298553467, Val Loss: 0.3265674412250519\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.5105 - val_loss: 0.3266 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.5022Epoch 40/100, Loss: 0.503020167350769, Val Loss: 0.3238845765590668\n",
      "591/591 [==============================] - 1s 931us/step - loss: 0.5030 - val_loss: 0.3239 - lr: 1.5625e-05\n",
      "Epoch 41/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.4974Epoch 41/100, Loss: 0.49837833642959595, Val Loss: 0.3223456144332886\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "591/591 [==============================] - 1s 985us/step - loss: 0.4984 - val_loss: 0.3223 - lr: 1.5625e-05\n",
      "Epoch 41: early stopping\n",
      "127/127 [==============================] - 0s 510us/step\n",
      "127/127 [==============================] - 0s 494us/step - loss: 0.3676\n",
      "Epoch 1/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.4582Epoch 1/100, Loss: 1.4519670009613037, Val Loss: 1.1224199533462524\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.4520 - val_loss: 1.1224 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 1.0010Epoch 2/100, Loss: 0.9976046681404114, Val Loss: 0.7838094234466553\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9976 - val_loss: 0.7838 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.9299Epoch 3/100, Loss: 0.9260920882225037, Val Loss: 0.790575385093689\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.9261 - val_loss: 0.7906 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.8724Epoch 4/100, Loss: 0.872780442237854, Val Loss: 0.6150689125061035\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8728 - val_loss: 0.6151 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.8199Epoch 5/100, Loss: 0.8146328330039978, Val Loss: 0.7053623199462891\n",
      "591/591 [==============================] - 1s 968us/step - loss: 0.8146 - val_loss: 0.7054 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.7639Epoch 6/100, Loss: 0.7636901140213013, Val Loss: 0.6829749941825867\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7637 - val_loss: 0.6830 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7254Epoch 7/100, Loss: 0.7270850539207458, Val Loss: 0.6002787947654724\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.7271 - val_loss: 0.6003 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.6951Epoch 8/100, Loss: 0.696365475654602, Val Loss: 0.6919679045677185\n",
      "591/591 [==============================] - 1s 965us/step - loss: 0.6964 - val_loss: 0.6920 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.6544Epoch 9/100, Loss: 0.6606655120849609, Val Loss: 0.5555936098098755\n",
      "591/591 [==============================] - 1s 965us/step - loss: 0.6607 - val_loss: 0.5556 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.6440Epoch 10/100, Loss: 0.6441925764083862, Val Loss: 0.5022132396697998\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6442 - val_loss: 0.5022 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.6239Epoch 11/100, Loss: 0.6185753345489502, Val Loss: 0.509523868560791\n",
      "591/591 [==============================] - 1s 971us/step - loss: 0.6186 - val_loss: 0.5095 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6121Epoch 12/100, Loss: 0.6115243434906006, Val Loss: 0.5372455716133118\n",
      "591/591 [==============================] - 1s 991us/step - loss: 0.6115 - val_loss: 0.5372 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.6062Epoch 13/100, Loss: 0.6089655160903931, Val Loss: 0.4900164008140564\n",
      "591/591 [==============================] - 1s 973us/step - loss: 0.6090 - val_loss: 0.4900 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.5946Epoch 14/100, Loss: 0.5926811695098877, Val Loss: 0.5646151304244995\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5927 - val_loss: 0.5646 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.5846Epoch 15/100, Loss: 0.5846323370933533, Val Loss: 0.4191226363182068\n",
      "591/591 [==============================] - 1s 979us/step - loss: 0.5846 - val_loss: 0.4191 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.5871Epoch 16/100, Loss: 0.5847452878952026, Val Loss: 0.468060165643692\n",
      "591/591 [==============================] - 1s 1000us/step - loss: 0.5847 - val_loss: 0.4681 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.5791Epoch 17/100, Loss: 0.5759629011154175, Val Loss: 0.46053123474121094\n",
      "591/591 [==============================] - 1s 974us/step - loss: 0.5760 - val_loss: 0.4605 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5724Epoch 18/100, Loss: 0.5699330568313599, Val Loss: 0.4820043444633484\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5699 - val_loss: 0.4820 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.5197Epoch 19/100, Loss: 0.5221362709999084, Val Loss: 0.3717825412750244\n",
      "591/591 [==============================] - 1s 969us/step - loss: 0.5221 - val_loss: 0.3718 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.5198Epoch 20/100, Loss: 0.5219208598136902, Val Loss: 0.40292665362358093\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5219 - val_loss: 0.4029 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.5164Epoch 21/100, Loss: 0.5122446417808533, Val Loss: 0.35300785303115845\n",
      "591/591 [==============================] - 1s 973us/step - loss: 0.5122 - val_loss: 0.3530 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.5081Epoch 22/100, Loss: 0.5083451271057129, Val Loss: 0.36714521050453186\n",
      "591/591 [==============================] - 1s 968us/step - loss: 0.5083 - val_loss: 0.3671 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.5030Epoch 23/100, Loss: 0.5012764930725098, Val Loss: 0.36087101697921753\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5013 - val_loss: 0.3609 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.5121Epoch 24/100, Loss: 0.5110567808151245, Val Loss: 0.3321172893047333\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5111 - val_loss: 0.3321 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5023Epoch 25/100, Loss: 0.5019118785858154, Val Loss: 0.35572922229766846\n",
      "591/591 [==============================] - 1s 958us/step - loss: 0.5019 - val_loss: 0.3557 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.5069Epoch 26/100, Loss: 0.5033923983573914, Val Loss: 0.3564489185810089\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5034 - val_loss: 0.3564 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.4923Epoch 27/100, Loss: 0.49216508865356445, Val Loss: 0.35025468468666077\n",
      "591/591 [==============================] - 1s 968us/step - loss: 0.4922 - val_loss: 0.3503 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.4786Epoch 28/100, Loss: 0.478588730096817, Val Loss: 0.3207206726074219\n",
      "591/591 [==============================] - 1s 977us/step - loss: 0.4786 - val_loss: 0.3207 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.4711Epoch 29/100, Loss: 0.4719450771808624, Val Loss: 0.32931816577911377\n",
      "591/591 [==============================] - 1s 999us/step - loss: 0.4719 - val_loss: 0.3293 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4725Epoch 30/100, Loss: 0.47211283445358276, Val Loss: 0.29797667264938354\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4721 - val_loss: 0.2980 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4656Epoch 31/100, Loss: 0.4651813209056854, Val Loss: 0.32348549365997314\n",
      "591/591 [==============================] - 1s 986us/step - loss: 0.4652 - val_loss: 0.3235 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.4773Epoch 32/100, Loss: 0.47844836115837097, Val Loss: 0.31857720017433167\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.4784 - val_loss: 0.3186 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4661Epoch 33/100, Loss: 0.46410807967185974, Val Loss: 0.32849234342575073\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4641 - val_loss: 0.3285 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.4506Epoch 34/100, Loss: 0.44953253865242004, Val Loss: 0.29579606652259827\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.4495 - val_loss: 0.2958 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4474Epoch 35/100, Loss: 0.4488227963447571, Val Loss: 0.3079273998737335\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4488 - val_loss: 0.3079 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.4615Epoch 36/100, Loss: 0.45902642607688904, Val Loss: 0.3009502589702606\n",
      "591/591 [==============================] - 1s 962us/step - loss: 0.4590 - val_loss: 0.3010 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4516Epoch 37/100, Loss: 0.45734700560569763, Val Loss: 0.2979954481124878\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4573 - val_loss: 0.2980 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.4454Epoch 38/100, Loss: 0.44326382875442505, Val Loss: 0.2954837381839752\n",
      "591/591 [==============================] - 1s 961us/step - loss: 0.4433 - val_loss: 0.2955 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.4509Epoch 39/100, Loss: 0.4500236213207245, Val Loss: 0.2880663275718689\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4500 - val_loss: 0.2881 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.4432Epoch 40/100, Loss: 0.44405046105384827, Val Loss: 0.28430816531181335\n",
      "591/591 [==============================] - 1s 971us/step - loss: 0.4441 - val_loss: 0.2843 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4354Epoch 41/100, Loss: 0.4365599751472473, Val Loss: 0.2958277463912964\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4366 - val_loss: 0.2958 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4429Epoch 42/100, Loss: 0.44326502084732056, Val Loss: 0.2809378504753113\n",
      "591/591 [==============================] - 1s 982us/step - loss: 0.4433 - val_loss: 0.2809 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4421Epoch 43/100, Loss: 0.4413529634475708, Val Loss: 0.27853840589523315\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4414 - val_loss: 0.2785 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.4385Epoch 44/100, Loss: 0.44024962186813354, Val Loss: 0.2860581874847412\n",
      "591/591 [==============================] - 1s 967us/step - loss: 0.4402 - val_loss: 0.2861 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4361Epoch 45/100, Loss: 0.44129103422164917, Val Loss: 0.29987409710884094\n",
      "591/591 [==============================] - 1s 996us/step - loss: 0.4413 - val_loss: 0.2999 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.4456Epoch 46/100, Loss: 0.44508230686187744, Val Loss: 0.28962400555610657\n",
      "591/591 [==============================] - 1s 969us/step - loss: 0.4451 - val_loss: 0.2896 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4386Epoch 47/100, Loss: 0.436152845621109, Val Loss: 0.28114455938339233\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4362 - val_loss: 0.2811 - lr: 3.1250e-05\n",
      "Epoch 48/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4391Epoch 48/100, Loss: 0.43690094351768494, Val Loss: 0.27982473373413086\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4369 - val_loss: 0.2798 - lr: 3.1250e-05\n",
      "Epoch 48: early stopping\n",
      "127/127 [==============================] - 0s 558us/step\n",
      "127/127 [==============================] - 0s 534us/step - loss: 0.3241\n",
      "Epoch 1/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 1.4116Epoch 1/100, Loss: 1.3936524391174316, Val Loss: 1.1886382102966309\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.3937 - val_loss: 1.1886 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.9901Epoch 2/100, Loss: 0.9865859746932983, Val Loss: 0.8300933241844177\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9866 - val_loss: 0.8301 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.9144Epoch 3/100, Loss: 0.9145956039428711, Val Loss: 0.713839590549469\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9146 - val_loss: 0.7138 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.8434Epoch 4/100, Loss: 0.8430113196372986, Val Loss: 0.6085981130599976\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8430 - val_loss: 0.6086 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.7766Epoch 5/100, Loss: 0.7701311111450195, Val Loss: 0.6161830425262451\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7701 - val_loss: 0.6162 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.7112Epoch 6/100, Loss: 0.7114670276641846, Val Loss: 0.6250977516174316\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7115 - val_loss: 0.6251 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.6765Epoch 7/100, Loss: 0.681831955909729, Val Loss: 0.5357438921928406\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6818 - val_loss: 0.5357 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.6528Epoch 8/100, Loss: 0.6554827690124512, Val Loss: 0.5811936259269714\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6555 - val_loss: 0.5812 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6287Epoch 9/100, Loss: 0.6294010281562805, Val Loss: 0.5159380435943604\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6294 - val_loss: 0.5159 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.6084Epoch 10/100, Loss: 0.6113194823265076, Val Loss: 0.4593404531478882\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6113 - val_loss: 0.4593 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5927Epoch 11/100, Loss: 0.5925601720809937, Val Loss: 0.5086380243301392\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5926 - val_loss: 0.5086 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.5904Epoch 12/100, Loss: 0.5884285569190979, Val Loss: 0.43879756331443787\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5884 - val_loss: 0.4388 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.5798Epoch 13/100, Loss: 0.5820745825767517, Val Loss: 0.4361041486263275\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5821 - val_loss: 0.4361 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5661Epoch 14/100, Loss: 0.5653642416000366, Val Loss: 0.5448219180107117\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5654 - val_loss: 0.5448 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.5658Epoch 15/100, Loss: 0.5676170587539673, Val Loss: 0.46204760670661926\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5676 - val_loss: 0.4620 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5551Epoch 16/100, Loss: 0.5541213750839233, Val Loss: 0.43627193570137024\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5541 - val_loss: 0.4363 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.5119Epoch 17/100, Loss: 0.508138120174408, Val Loss: 0.38386356830596924\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5081 - val_loss: 0.3839 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4997Epoch 18/100, Loss: 0.49833884835243225, Val Loss: 0.38067030906677246\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4983 - val_loss: 0.3807 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4892Epoch 19/100, Loss: 0.4891830384731293, Val Loss: 0.3724008798599243\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4892 - val_loss: 0.3724 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.4908Epoch 20/100, Loss: 0.49204567074775696, Val Loss: 0.40385130047798157\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4920 - val_loss: 0.4039 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4810Epoch 21/100, Loss: 0.4787601828575134, Val Loss: 0.3634676933288574\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4788 - val_loss: 0.3635 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.4778Epoch 22/100, Loss: 0.47884970903396606, Val Loss: 0.39378154277801514\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4788 - val_loss: 0.3938 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4725Epoch 23/100, Loss: 0.47266849875450134, Val Loss: 0.4000776708126068\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4727 - val_loss: 0.4001 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.4784Epoch 24/100, Loss: 0.47604209184646606, Val Loss: 0.3478040397167206\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4760 - val_loss: 0.3478 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4686Epoch 25/100, Loss: 0.4689655601978302, Val Loss: 0.40823474526405334\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4690 - val_loss: 0.4082 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4652Epoch 26/100, Loss: 0.46487051248550415, Val Loss: 0.3510027527809143\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4649 - val_loss: 0.3510 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.4606Epoch 27/100, Loss: 0.458369642496109, Val Loss: 0.34318798780441284\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4584 - val_loss: 0.3432 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4580Epoch 28/100, Loss: 0.46088284254074097, Val Loss: 0.38729047775268555\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4609 - val_loss: 0.3873 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4638Epoch 29/100, Loss: 0.46388572454452515, Val Loss: 0.3725539743900299\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4639 - val_loss: 0.3726 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4549Epoch 30/100, Loss: 0.45390698313713074, Val Loss: 0.35532718896865845\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4539 - val_loss: 0.3553 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.4273Epoch 31/100, Loss: 0.42603227496147156, Val Loss: 0.3152429163455963\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4260 - val_loss: 0.3152 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4345Epoch 32/100, Loss: 0.4368945360183716, Val Loss: 0.3125365376472473\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4369 - val_loss: 0.3125 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4260Epoch 33/100, Loss: 0.4239595830440521, Val Loss: 0.2915462553501129\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4240 - val_loss: 0.2915 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4191Epoch 34/100, Loss: 0.4187004566192627, Val Loss: 0.32043930888175964\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4187 - val_loss: 0.3204 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.4129Epoch 35/100, Loss: 0.4134039580821991, Val Loss: 0.29446133971214294\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4134 - val_loss: 0.2945 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4252Epoch 36/100, Loss: 0.42593133449554443, Val Loss: 0.3112828731536865\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4259 - val_loss: 0.3113 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.4036Epoch 37/100, Loss: 0.41005975008010864, Val Loss: 0.2776108980178833\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4101 - val_loss: 0.2776 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4027Epoch 38/100, Loss: 0.4008179306983948, Val Loss: 0.28505975008010864\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4008 - val_loss: 0.2851 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4069Epoch 39/100, Loss: 0.40620309114456177, Val Loss: 0.28261157870292664\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4062 - val_loss: 0.2826 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.3967Epoch 40/100, Loss: 0.39870360493659973, Val Loss: 0.26302820444107056\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3987 - val_loss: 0.2630 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3947Epoch 41/100, Loss: 0.39353147149086, Val Loss: 0.26379549503326416\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3935 - val_loss: 0.2638 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.3973Epoch 42/100, Loss: 0.3977755606174469, Val Loss: 0.2656388282775879\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3978 - val_loss: 0.2656 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3934Epoch 43/100, Loss: 0.39344921708106995, Val Loss: 0.273788183927536\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3934 - val_loss: 0.2738 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.3906Epoch 44/100, Loss: 0.388482004404068, Val Loss: 0.24574625492095947\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3885 - val_loss: 0.2457 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.3864Epoch 45/100, Loss: 0.3918572664260864, Val Loss: 0.26498013734817505\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3919 - val_loss: 0.2650 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.3957Epoch 46/100, Loss: 0.39201390743255615, Val Loss: 0.2512887418270111\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3920 - val_loss: 0.2513 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3855Epoch 47/100, Loss: 0.38494637608528137, Val Loss: 0.24850474298000336\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3849 - val_loss: 0.2485 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3856Epoch 48/100, Loss: 0.3858673572540283, Val Loss: 0.24194294214248657\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3859 - val_loss: 0.2419 - lr: 3.1250e-05\n",
      "Epoch 49/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.3824Epoch 49/100, Loss: 0.3791757822036743, Val Loss: 0.24276447296142578\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3792 - val_loss: 0.2428 - lr: 3.1250e-05\n",
      "Epoch 50/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.3840Epoch 50/100, Loss: 0.38111409544944763, Val Loss: 0.24789956212043762\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3811 - val_loss: 0.2479 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.3778Epoch 51/100, Loss: 0.3777073919773102, Val Loss: 0.24349406361579895\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3777 - val_loss: 0.2435 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.3832Epoch 52/100, Loss: 0.3776501715183258, Val Loss: 0.23923973739147186\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3777 - val_loss: 0.2392 - lr: 1.5625e-05\n",
      "Epoch 53/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3759Epoch 53/100, Loss: 0.3755103349685669, Val Loss: 0.2455851435661316\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3755 - val_loss: 0.2456 - lr: 1.5625e-05\n",
      "Epoch 54/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3878Epoch 54/100, Loss: 0.38627123832702637, Val Loss: 0.24626758694648743\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3863 - val_loss: 0.2463 - lr: 1.5625e-05\n",
      "Epoch 55/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.3845Epoch 55/100, Loss: 0.38506239652633667, Val Loss: 0.24231715500354767\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3851 - val_loss: 0.2423 - lr: 1.5625e-05\n",
      "Epoch 56/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3761Epoch 56/100, Loss: 0.3760560154914856, Val Loss: 0.2444121390581131\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3761 - val_loss: 0.2444 - lr: 7.8125e-06\n",
      "Epoch 57/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3854Epoch 57/100, Loss: 0.3855389654636383, Val Loss: 0.24228383600711823\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3855 - val_loss: 0.2423 - lr: 7.8125e-06\n",
      "Epoch 57: early stopping\n",
      "127/127 [==============================] - 0s 562us/step\n",
      "127/127 [==============================] - 0s 542us/step - loss: 0.2871\n",
      "Epoch 1/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 1.3249Epoch 1/100, Loss: 1.3211750984191895, Val Loss: 1.3345946073532104\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.3212 - val_loss: 1.3346 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.0008Epoch 2/100, Loss: 1.0003528594970703, Val Loss: 0.8644391298294067\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0004 - val_loss: 0.8644 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.9232Epoch 3/100, Loss: 0.923693060874939, Val Loss: 0.8301888704299927\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9237 - val_loss: 0.8302 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.8335Epoch 4/100, Loss: 0.8336596488952637, Val Loss: 0.6796429753303528\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8337 - val_loss: 0.6796 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.7666Epoch 5/100, Loss: 0.7651352286338806, Val Loss: 0.5600801110267639\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7651 - val_loss: 0.5601 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7087Epoch 6/100, Loss: 0.709276020526886, Val Loss: 0.669879674911499\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7093 - val_loss: 0.6699 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.6788Epoch 7/100, Loss: 0.6818915009498596, Val Loss: 0.5811880230903625\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6819 - val_loss: 0.5812 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.6464Epoch 8/100, Loss: 0.6467044949531555, Val Loss: 0.5971579551696777\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6467 - val_loss: 0.5972 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5715Epoch 9/100, Loss: 0.578185498714447, Val Loss: 0.5241205096244812\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5782 - val_loss: 0.5241 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5562Epoch 10/100, Loss: 0.5564778447151184, Val Loss: 0.387270987033844\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5565 - val_loss: 0.3873 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.5415Epoch 11/100, Loss: 0.541351854801178, Val Loss: 0.46897950768470764\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5414 - val_loss: 0.4690 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5396Epoch 12/100, Loss: 0.5390921235084534, Val Loss: 0.4393552243709564\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5391 - val_loss: 0.4394 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5312Epoch 13/100, Loss: 0.5329698324203491, Val Loss: 0.40841206908226013\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5330 - val_loss: 0.4084 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4929Epoch 14/100, Loss: 0.491386353969574, Val Loss: 0.41231489181518555\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4914 - val_loss: 0.4123 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4864Epoch 15/100, Loss: 0.488866925239563, Val Loss: 0.3626351058483124\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4889 - val_loss: 0.3626 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4773Epoch 16/100, Loss: 0.4755396544933319, Val Loss: 0.3223426043987274\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4755 - val_loss: 0.3223 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4775Epoch 17/100, Loss: 0.47633886337280273, Val Loss: 0.3302870988845825\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4763 - val_loss: 0.3303 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4707Epoch 18/100, Loss: 0.46915990114212036, Val Loss: 0.34527596831321716\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4692 - val_loss: 0.3453 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4591Epoch 19/100, Loss: 0.4590848684310913, Val Loss: 0.36175602674484253\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4591 - val_loss: 0.3618 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4488Epoch 20/100, Loss: 0.44984912872314453, Val Loss: 0.33328720927238464\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4498 - val_loss: 0.3333 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4402Epoch 21/100, Loss: 0.4388883709907532, Val Loss: 0.302435964345932\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4389 - val_loss: 0.3024 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4378Epoch 22/100, Loss: 0.43781745433807373, Val Loss: 0.3016360402107239\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4378 - val_loss: 0.3016 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4315Epoch 23/100, Loss: 0.4318050742149353, Val Loss: 0.2976801097393036\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4318 - val_loss: 0.2977 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4352Epoch 24/100, Loss: 0.4345560371875763, Val Loss: 0.28540298342704773\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4346 - val_loss: 0.2854 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4328Epoch 25/100, Loss: 0.43194371461868286, Val Loss: 0.31802335381507874\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4319 - val_loss: 0.3180 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.4289Epoch 26/100, Loss: 0.42715615034103394, Val Loss: 0.2833261489868164\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4272 - val_loss: 0.2833 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4230Epoch 27/100, Loss: 0.42146527767181396, Val Loss: 0.3004966974258423\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4215 - val_loss: 0.3005 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4225Epoch 28/100, Loss: 0.4245811998844147, Val Loss: 0.317793071269989\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4246 - val_loss: 0.3178 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4252Epoch 29/100, Loss: 0.4246404469013214, Val Loss: 0.29750344157218933\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4246 - val_loss: 0.2975 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4109Epoch 30/100, Loss: 0.41181379556655884, Val Loss: 0.273259699344635\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 0.2733 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4061Epoch 31/100, Loss: 0.40567564964294434, Val Loss: 0.2816372811794281\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4057 - val_loss: 0.2816 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4181Epoch 32/100, Loss: 0.41891175508499146, Val Loss: 0.2734415829181671\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4189 - val_loss: 0.2734 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4056Epoch 33/100, Loss: 0.4044508635997772, Val Loss: 0.27915045619010925\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4045 - val_loss: 0.2792 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3992Epoch 34/100, Loss: 0.39815911650657654, Val Loss: 0.2628854215145111\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3982 - val_loss: 0.2629 - lr: 3.1250e-05\n",
      "Epoch 35/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3985Epoch 35/100, Loss: 0.39780688285827637, Val Loss: 0.25912630558013916\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3978 - val_loss: 0.2591 - lr: 3.1250e-05\n",
      "Epoch 36/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4095Epoch 36/100, Loss: 0.40913453698158264, Val Loss: 0.26843348145484924\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4091 - val_loss: 0.2684 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4055Epoch 37/100, Loss: 0.4054541289806366, Val Loss: 0.269865483045578\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.2699 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3973Epoch 38/100, Loss: 0.3969815969467163, Val Loss: 0.2830405533313751\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.2830 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.4031Epoch 39/100, Loss: 0.3999694883823395, Val Loss: 0.2638430893421173\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4000 - val_loss: 0.2638 - lr: 1.5625e-05\n",
      "Epoch 40/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3921Epoch 40/100, Loss: 0.39362815022468567, Val Loss: 0.26152729988098145\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3936 - val_loss: 0.2615 - lr: 1.5625e-05\n",
      "Epoch 40: early stopping\n",
      "127/127 [==============================] - 0s 890us/step\n",
      "127/127 [==============================] - 0s 943us/step - loss: 0.3023\n",
      "Epoch 1/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 2.2707Epoch 1/100, Loss: 2.229947090148926, Val Loss: 1.2492992877960205\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 2.2299 - val_loss: 1.2493 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 1.2036Epoch 2/100, Loss: 1.199579119682312, Val Loss: 0.9818576574325562\n",
      "591/591 [==============================] - 1s 949us/step - loss: 1.1996 - val_loss: 0.9819 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 1.0775Epoch 3/100, Loss: 1.0776907205581665, Val Loss: 0.8594530820846558\n",
      "591/591 [==============================] - 1s 931us/step - loss: 1.0777 - val_loss: 0.8595 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 1.0393Epoch 4/100, Loss: 1.0403765439987183, Val Loss: 0.8116459846496582\n",
      "591/591 [==============================] - 1s 951us/step - loss: 1.0404 - val_loss: 0.8116 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 1.0024Epoch 5/100, Loss: 1.0026094913482666, Val Loss: 0.7882193326950073\n",
      "591/591 [==============================] - 1s 905us/step - loss: 1.0026 - val_loss: 0.7882 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.9840Epoch 6/100, Loss: 0.985454797744751, Val Loss: 0.8629179000854492\n",
      "591/591 [==============================] - 1s 964us/step - loss: 0.9855 - val_loss: 0.8629 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.9750Epoch 7/100, Loss: 0.9778241515159607, Val Loss: 0.7382413744926453\n",
      "591/591 [==============================] - 1s 950us/step - loss: 0.9778 - val_loss: 0.7382 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.9488Epoch 8/100, Loss: 0.9500604271888733, Val Loss: 0.7000440955162048\n",
      "591/591 [==============================] - 1s 904us/step - loss: 0.9501 - val_loss: 0.7000 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.9255Epoch 9/100, Loss: 0.9335188269615173, Val Loss: 0.6736370921134949\n",
      "591/591 [==============================] - 1s 946us/step - loss: 0.9335 - val_loss: 0.6736 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9100Epoch 10/100, Loss: 0.9105013012886047, Val Loss: 0.6818024516105652\n",
      "591/591 [==============================] - 1s 898us/step - loss: 0.9105 - val_loss: 0.6818 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.9070Epoch 11/100, Loss: 0.9041693806648254, Val Loss: 0.6792836785316467\n",
      "591/591 [==============================] - 1s 943us/step - loss: 0.9042 - val_loss: 0.6793 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.8908Epoch 12/100, Loss: 0.8890867829322815, Val Loss: 0.6718763709068298\n",
      "591/591 [==============================] - 1s 939us/step - loss: 0.8891 - val_loss: 0.6719 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.8763Epoch 13/100, Loss: 0.8791502118110657, Val Loss: 0.6460539698600769\n",
      "591/591 [==============================] - 1s 928us/step - loss: 0.8792 - val_loss: 0.6461 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8615Epoch 14/100, Loss: 0.8627101182937622, Val Loss: 0.6054508686065674\n",
      "591/591 [==============================] - 1s 937us/step - loss: 0.8627 - val_loss: 0.6055 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.8227Epoch 15/100, Loss: 0.8269540667533875, Val Loss: 0.6481443047523499\n",
      "591/591 [==============================] - 1s 913us/step - loss: 0.8270 - val_loss: 0.6481 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8237Epoch 16/100, Loss: 0.8229920268058777, Val Loss: 0.5922257900238037\n",
      "591/591 [==============================] - 1s 922us/step - loss: 0.8230 - val_loss: 0.5922 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.8111Epoch 17/100, Loss: 0.8065194487571716, Val Loss: 0.5864921808242798\n",
      "591/591 [==============================] - 1s 947us/step - loss: 0.8065 - val_loss: 0.5865 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.7985Epoch 18/100, Loss: 0.7938137054443359, Val Loss: 0.606266438961029\n",
      "591/591 [==============================] - 1s 922us/step - loss: 0.7938 - val_loss: 0.6063 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.7721Epoch 19/100, Loss: 0.7733103036880493, Val Loss: 0.5849328637123108\n",
      "591/591 [==============================] - 1s 953us/step - loss: 0.7733 - val_loss: 0.5849 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7735Epoch 20/100, Loss: 0.7762004733085632, Val Loss: 0.5482737421989441\n",
      "591/591 [==============================] - 1s 912us/step - loss: 0.7762 - val_loss: 0.5483 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7593Epoch 21/100, Loss: 0.75634765625, Val Loss: 0.559941828250885\n",
      "591/591 [==============================] - 1s 939us/step - loss: 0.7563 - val_loss: 0.5599 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7541Epoch 22/100, Loss: 0.7532275319099426, Val Loss: 0.5910684466362\n",
      "591/591 [==============================] - 1s 909us/step - loss: 0.7532 - val_loss: 0.5911 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.7456Epoch 23/100, Loss: 0.7457904815673828, Val Loss: 0.5575557351112366\n",
      "591/591 [==============================] - 1s 961us/step - loss: 0.7458 - val_loss: 0.5576 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7412Epoch 24/100, Loss: 0.740971028804779, Val Loss: 0.5201231837272644\n",
      "591/591 [==============================] - 1s 921us/step - loss: 0.7410 - val_loss: 0.5201 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.7311Epoch 25/100, Loss: 0.7347279191017151, Val Loss: 0.5340474247932434\n",
      "591/591 [==============================] - 1s 959us/step - loss: 0.7347 - val_loss: 0.5340 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7380Epoch 26/100, Loss: 0.7334296107292175, Val Loss: 0.525664746761322\n",
      "591/591 [==============================] - 1s 955us/step - loss: 0.7334 - val_loss: 0.5257 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7199Epoch 27/100, Loss: 0.7182706594467163, Val Loss: 0.5303887724876404\n",
      "591/591 [==============================] - 1s 920us/step - loss: 0.7183 - val_loss: 0.5304 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.7172Epoch 28/100, Loss: 0.7158161997795105, Val Loss: 0.5069831609725952\n",
      "591/591 [==============================] - 1s 950us/step - loss: 0.7158 - val_loss: 0.5070 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7116Epoch 29/100, Loss: 0.7118830680847168, Val Loss: 0.5167844295501709\n",
      "591/591 [==============================] - 1s 945us/step - loss: 0.7119 - val_loss: 0.5168 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.7138Epoch 30/100, Loss: 0.7137672305107117, Val Loss: 0.49229148030281067\n",
      "591/591 [==============================] - 1s 930us/step - loss: 0.7138 - val_loss: 0.4923 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.7121Epoch 31/100, Loss: 0.7083193063735962, Val Loss: 0.49517953395843506\n",
      "591/591 [==============================] - 1s 942us/step - loss: 0.7083 - val_loss: 0.4952 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7221Epoch 32/100, Loss: 0.7226917743682861, Val Loss: 0.49689778685569763\n",
      "591/591 [==============================] - 1s 908us/step - loss: 0.7227 - val_loss: 0.4969 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7099Epoch 33/100, Loss: 0.7055471539497375, Val Loss: 0.48928382992744446\n",
      "591/591 [==============================] - 1s 941us/step - loss: 0.7055 - val_loss: 0.4893 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.6976Epoch 34/100, Loss: 0.6973680853843689, Val Loss: 0.48742493987083435\n",
      "591/591 [==============================] - 1s 947us/step - loss: 0.6974 - val_loss: 0.4874 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7001Epoch 35/100, Loss: 0.7001359462738037, Val Loss: 0.5003268122673035\n",
      "591/591 [==============================] - 1s 915us/step - loss: 0.7001 - val_loss: 0.5003 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.7072Epoch 36/100, Loss: 0.7080721259117126, Val Loss: 0.47581958770751953\n",
      "591/591 [==============================] - 1s 944us/step - loss: 0.7081 - val_loss: 0.4758 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7044Epoch 37/100, Loss: 0.7099847793579102, Val Loss: 0.4816909730434418\n",
      "591/591 [==============================] - 1s 924us/step - loss: 0.7100 - val_loss: 0.4817 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6919Epoch 38/100, Loss: 0.6920682191848755, Val Loss: 0.4844086766242981\n",
      "591/591 [==============================] - 1s 944us/step - loss: 0.6921 - val_loss: 0.4844 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7078Epoch 39/100, Loss: 0.7072914242744446, Val Loss: 0.48418864607810974\n",
      "591/591 [==============================] - 1s 914us/step - loss: 0.7073 - val_loss: 0.4842 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.6994Epoch 40/100, Loss: 0.7000960111618042, Val Loss: 0.47934579849243164\n",
      "591/591 [==============================] - 1s 971us/step - loss: 0.7001 - val_loss: 0.4793 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.6864Epoch 41/100, Loss: 0.6853029131889343, Val Loss: 0.4678955078125\n",
      "591/591 [==============================] - 1s 923us/step - loss: 0.6853 - val_loss: 0.4679 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.6990Epoch 42/100, Loss: 0.6953707933425903, Val Loss: 0.4662630558013916\n",
      "591/591 [==============================] - 1s 960us/step - loss: 0.6954 - val_loss: 0.4663 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.6873Epoch 43/100, Loss: 0.6877606511116028, Val Loss: 0.47498998045921326\n",
      "591/591 [==============================] - 1s 946us/step - loss: 0.6878 - val_loss: 0.4750 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6887Epoch 44/100, Loss: 0.687501072883606, Val Loss: 0.47759172320365906\n",
      "591/591 [==============================] - 1s 907us/step - loss: 0.6875 - val_loss: 0.4776 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6945Epoch 45/100, Loss: 0.7000893950462341, Val Loss: 0.4742847979068756\n",
      "591/591 [==============================] - 1s 945us/step - loss: 0.7001 - val_loss: 0.4743 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.6947Epoch 46/100, Loss: 0.6936855912208557, Val Loss: 0.463512122631073\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.6937 - val_loss: 0.4635 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6915Epoch 47/100, Loss: 0.6905254125595093, Val Loss: 0.47083568572998047\n",
      "591/591 [==============================] - 1s 910us/step - loss: 0.6905 - val_loss: 0.4708 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.6937Epoch 48/100, Loss: 0.6916018128395081, Val Loss: 0.4709234833717346\n",
      "591/591 [==============================] - 1s 950us/step - loss: 0.6916 - val_loss: 0.4709 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.6881Epoch 49/100, Loss: 0.6878122091293335, Val Loss: 0.4654364585876465\n",
      "591/591 [==============================] - 1s 910us/step - loss: 0.6878 - val_loss: 0.4654 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.6938Epoch 50/100, Loss: 0.6901656985282898, Val Loss: 0.4644288718700409\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.6902 - val_loss: 0.4644 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.6816Epoch 51/100, Loss: 0.683553159236908, Val Loss: 0.4639248549938202\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "591/591 [==============================] - 1s 977us/step - loss: 0.6836 - val_loss: 0.4639 - lr: 3.1250e-05\n",
      "Epoch 51: early stopping\n",
      "127/127 [==============================] - 0s 522us/step\n",
      "127/127 [==============================] - 0s 514us/step - loss: 0.5083\n",
      "Epoch 1/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.9771Epoch 1/100, Loss: 1.9643510580062866, Val Loss: 1.7718826532363892\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.9644 - val_loss: 1.7719 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 1.1486Epoch 2/100, Loss: 1.1453133821487427, Val Loss: 0.9295181632041931\n",
      "591/591 [==============================] - 1s 927us/step - loss: 1.1453 - val_loss: 0.9295 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "531/591 [=========================>....] - ETA: 0s - loss: 1.0824Epoch 3/100, Loss: 1.076113224029541, Val Loss: 0.8975781798362732\n",
      "591/591 [==============================] - 1s 977us/step - loss: 1.0761 - val_loss: 0.8976 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 1.0321Epoch 4/100, Loss: 1.0325177907943726, Val Loss: 0.857495129108429\n",
      "591/591 [==============================] - 1s 950us/step - loss: 1.0325 - val_loss: 0.8575 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 1.0113Epoch 5/100, Loss: 1.0095056295394897, Val Loss: 0.791380763053894\n",
      "591/591 [==============================] - 1s 928us/step - loss: 1.0095 - val_loss: 0.7914 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.9910Epoch 6/100, Loss: 0.9930322170257568, Val Loss: 0.88868248462677\n",
      "591/591 [==============================] - 1s 962us/step - loss: 0.9930 - val_loss: 0.8887 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.9764Epoch 7/100, Loss: 0.9764408469200134, Val Loss: 0.7440624833106995\n",
      "591/591 [==============================] - 1s 982us/step - loss: 0.9764 - val_loss: 0.7441 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.9462Epoch 8/100, Loss: 0.9477123618125916, Val Loss: 0.7009045481681824\n",
      "591/591 [==============================] - 1s 927us/step - loss: 0.9477 - val_loss: 0.7009 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.9058Epoch 9/100, Loss: 0.9107894897460938, Val Loss: 0.7598501443862915\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.9108 - val_loss: 0.7599 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8955Epoch 10/100, Loss: 0.8957864046096802, Val Loss: 0.7727237939834595\n",
      "591/591 [==============================] - 1s 939us/step - loss: 0.8958 - val_loss: 0.7727 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.8728Epoch 11/100, Loss: 0.8724666237831116, Val Loss: 0.8732127547264099\n",
      "591/591 [==============================] - 1s 917us/step - loss: 0.8725 - val_loss: 0.8732 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.8452Epoch 12/100, Loss: 0.84361732006073, Val Loss: 0.6362494230270386\n",
      "591/591 [==============================] - 1s 952us/step - loss: 0.8436 - val_loss: 0.6362 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.8348Epoch 13/100, Loss: 0.8337780237197876, Val Loss: 0.6276067495346069\n",
      "591/591 [==============================] - 1s 977us/step - loss: 0.8338 - val_loss: 0.6276 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.8167Epoch 14/100, Loss: 0.8158307671546936, Val Loss: 0.7051972150802612\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8158 - val_loss: 0.7052 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.7996Epoch 15/100, Loss: 0.7999640703201294, Val Loss: 0.6113054156303406\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8000 - val_loss: 0.6113 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.7948Epoch 16/100, Loss: 0.7935622334480286, Val Loss: 0.640831708908081\n",
      "591/591 [==============================] - 1s 999us/step - loss: 0.7936 - val_loss: 0.6408 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.7973Epoch 17/100, Loss: 0.7913815379142761, Val Loss: 0.6193192601203918\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7914 - val_loss: 0.6193 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7868Epoch 18/100, Loss: 0.7838050127029419, Val Loss: 0.6322954297065735\n",
      "591/591 [==============================] - 1s 916us/step - loss: 0.7838 - val_loss: 0.6323 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.7518Epoch 19/100, Loss: 0.7563940286636353, Val Loss: 0.5930178165435791\n",
      "591/591 [==============================] - 1s 964us/step - loss: 0.7564 - val_loss: 0.5930 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.7601Epoch 20/100, Loss: 0.7624103426933289, Val Loss: 0.5865408182144165\n",
      "591/591 [==============================] - 1s 924us/step - loss: 0.7624 - val_loss: 0.5865 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.7544Epoch 21/100, Loss: 0.7495477199554443, Val Loss: 0.5538730621337891\n",
      "591/591 [==============================] - 1s 971us/step - loss: 0.7495 - val_loss: 0.5539 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.7444Epoch 22/100, Loss: 0.7442429661750793, Val Loss: 0.5662847757339478\n",
      "591/591 [==============================] - 1s 958us/step - loss: 0.7442 - val_loss: 0.5663 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7388Epoch 23/100, Loss: 0.7367679476737976, Val Loss: 0.541496217250824\n",
      "591/591 [==============================] - 1s 917us/step - loss: 0.7368 - val_loss: 0.5415 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7560Epoch 24/100, Loss: 0.7531726956367493, Val Loss: 0.5672594904899597\n",
      "591/591 [==============================] - 1s 955us/step - loss: 0.7532 - val_loss: 0.5673 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.7447Epoch 25/100, Loss: 0.7455065250396729, Val Loss: 0.5919061899185181\n",
      "591/591 [==============================] - 1s 945us/step - loss: 0.7455 - val_loss: 0.5919 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7363Epoch 26/100, Loss: 0.7366065979003906, Val Loss: 0.5514717102050781\n",
      "591/591 [==============================] - 1s 906us/step - loss: 0.7366 - val_loss: 0.5515 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "530/591 [=========================>....] - ETA: 0s - loss: 0.7193Epoch 27/100, Loss: 0.7207041382789612, Val Loss: 0.5476146936416626\n",
      "591/591 [==============================] - 1s 970us/step - loss: 0.7207 - val_loss: 0.5476 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.7229Epoch 28/100, Loss: 0.7226531505584717, Val Loss: 0.5339296460151672\n",
      "591/591 [==============================] - 1s 969us/step - loss: 0.7227 - val_loss: 0.5339 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.7214Epoch 29/100, Loss: 0.7228917479515076, Val Loss: 0.5125117301940918\n",
      "591/591 [==============================] - 1s 929us/step - loss: 0.7229 - val_loss: 0.5125 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7220Epoch 30/100, Loss: 0.7216989398002625, Val Loss: 0.5243331789970398\n",
      "591/591 [==============================] - 1s 981us/step - loss: 0.7217 - val_loss: 0.5243 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "535/591 [==========================>...] - ETA: 0s - loss: 0.7191Epoch 31/100, Loss: 0.718022346496582, Val Loss: 0.5204970240592957\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.7180 - val_loss: 0.5205 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7267Epoch 32/100, Loss: 0.7274628281593323, Val Loss: 0.5152233242988586\n",
      "591/591 [==============================] - 1s 940us/step - loss: 0.7275 - val_loss: 0.5152 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.7141Epoch 33/100, Loss: 0.7111710906028748, Val Loss: 0.4995849132537842\n",
      "591/591 [==============================] - 1s 948us/step - loss: 0.7112 - val_loss: 0.4996 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.7053Epoch 34/100, Loss: 0.7050808072090149, Val Loss: 0.5144490599632263\n",
      "591/591 [==============================] - 1s 954us/step - loss: 0.7051 - val_loss: 0.5144 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7015Epoch 35/100, Loss: 0.7015308737754822, Val Loss: 0.49236366152763367\n",
      "591/591 [==============================] - 1s 917us/step - loss: 0.7015 - val_loss: 0.4924 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.7114Epoch 36/100, Loss: 0.7112224102020264, Val Loss: 0.5003998875617981\n",
      "591/591 [==============================] - 1s 936us/step - loss: 0.7112 - val_loss: 0.5004 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.7022Epoch 37/100, Loss: 0.70858234167099, Val Loss: 0.5074294805526733\n",
      "591/591 [==============================] - 1s 961us/step - loss: 0.7086 - val_loss: 0.5074 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.6967Epoch 38/100, Loss: 0.6960934400558472, Val Loss: 0.5084843635559082\n",
      "591/591 [==============================] - 1s 924us/step - loss: 0.6961 - val_loss: 0.5085 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.7121Epoch 39/100, Loss: 0.7079765796661377, Val Loss: 0.5066629648208618\n",
      "591/591 [==============================] - 1s 943us/step - loss: 0.7080 - val_loss: 0.5067 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.7009Epoch 40/100, Loss: 0.703888475894928, Val Loss: 0.497826486825943\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "591/591 [==============================] - 1s 956us/step - loss: 0.7039 - val_loss: 0.4978 - lr: 3.1250e-05\n",
      "Epoch 40: early stopping\n",
      "127/127 [==============================] - 0s 506us/step\n",
      "127/127 [==============================] - 0s 490us/step - loss: 0.5421\n",
      "Epoch 1/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 1.6194Epoch 1/100, Loss: 1.592089295387268, Val Loss: 1.1257877349853516\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.5921 - val_loss: 1.1258 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 1.0262Epoch 2/100, Loss: 1.0232207775115967, Val Loss: 0.8255334496498108\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0232 - val_loss: 0.8255 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9682Epoch 3/100, Loss: 0.9687138199806213, Val Loss: 0.8022553324699402\n",
      "591/591 [==============================] - 1s 989us/step - loss: 0.9687 - val_loss: 0.8023 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.9210Epoch 4/100, Loss: 0.920288622379303, Val Loss: 0.6811730861663818\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9203 - val_loss: 0.6812 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.8774Epoch 5/100, Loss: 0.8748866319656372, Val Loss: 0.6888683438301086\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8749 - val_loss: 0.6889 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.8412Epoch 6/100, Loss: 0.8432505130767822, Val Loss: 0.8527125716209412\n",
      "591/591 [==============================] - 1s 938us/step - loss: 0.8433 - val_loss: 0.8527 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.8069Epoch 7/100, Loss: 0.8093000054359436, Val Loss: 0.6221616864204407\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8093 - val_loss: 0.6222 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7534Epoch 8/100, Loss: 0.7567131519317627, Val Loss: 0.5551139116287231\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7567 - val_loss: 0.5551 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.7220Epoch 9/100, Loss: 0.7287859916687012, Val Loss: 0.5640223026275635\n",
      "591/591 [==============================] - 1s 966us/step - loss: 0.7288 - val_loss: 0.5640 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.7031Epoch 10/100, Loss: 0.7028006911277771, Val Loss: 0.581750214099884\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7028 - val_loss: 0.5818 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.6851Epoch 11/100, Loss: 0.6830214262008667, Val Loss: 0.6376436352729797\n",
      "591/591 [==============================] - 1s 962us/step - loss: 0.6830 - val_loss: 0.6376 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.6378Epoch 12/100, Loss: 0.6368482708930969, Val Loss: 0.4885464906692505\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6368 - val_loss: 0.4885 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.6311Epoch 13/100, Loss: 0.6327652931213379, Val Loss: 0.4926919937133789\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6328 - val_loss: 0.4927 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6093Epoch 14/100, Loss: 0.6083038449287415, Val Loss: 0.4510411322116852\n",
      "591/591 [==============================] - 1s 949us/step - loss: 0.6083 - val_loss: 0.4510 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.6029Epoch 15/100, Loss: 0.6043972373008728, Val Loss: 0.5083664655685425\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6044 - val_loss: 0.5084 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.5978Epoch 16/100, Loss: 0.5956923365592957, Val Loss: 0.4368569552898407\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5957 - val_loss: 0.4369 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.5921Epoch 17/100, Loss: 0.5921221971511841, Val Loss: 0.46865737438201904\n",
      "591/591 [==============================] - 1s 979us/step - loss: 0.5921 - val_loss: 0.4687 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5884Epoch 18/100, Loss: 0.5844724774360657, Val Loss: 0.5199437141418457\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5845 - val_loss: 0.5199 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.5701Epoch 19/100, Loss: 0.5708774328231812, Val Loss: 0.4664411246776581\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5709 - val_loss: 0.4664 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.5545Epoch 20/100, Loss: 0.5591703653335571, Val Loss: 0.3959125876426697\n",
      "591/591 [==============================] - 1s 975us/step - loss: 0.5592 - val_loss: 0.3959 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5431Epoch 21/100, Loss: 0.5431089401245117, Val Loss: 0.40456053614616394\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5431 - val_loss: 0.4046 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5348Epoch 22/100, Loss: 0.5348036289215088, Val Loss: 0.41057202219963074\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5348 - val_loss: 0.4106 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5317Epoch 23/100, Loss: 0.5321736335754395, Val Loss: 0.38778001070022583\n",
      "591/591 [==============================] - 1s 987us/step - loss: 0.5322 - val_loss: 0.3878 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.5429Epoch 24/100, Loss: 0.5416604280471802, Val Loss: 0.3929462432861328\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5417 - val_loss: 0.3929 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.5391Epoch 25/100, Loss: 0.5379710793495178, Val Loss: 0.3871055245399475\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5380 - val_loss: 0.3871 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.5338Epoch 26/100, Loss: 0.5337610840797424, Val Loss: 0.3839438855648041\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.5338 - val_loss: 0.3839 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5230Epoch 27/100, Loss: 0.5221248865127563, Val Loss: 0.3876580595970154\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.5221 - val_loss: 0.3877 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.5214Epoch 28/100, Loss: 0.5227617025375366, Val Loss: 0.36665627360343933\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5228 - val_loss: 0.3667 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.5222Epoch 29/100, Loss: 0.5231016874313354, Val Loss: 0.37233638763427734\n",
      "591/591 [==============================] - 1s 968us/step - loss: 0.5231 - val_loss: 0.3723 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.5178Epoch 30/100, Loss: 0.5182468295097351, Val Loss: 0.37344875931739807\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5182 - val_loss: 0.3734 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5131Epoch 31/100, Loss: 0.5134017467498779, Val Loss: 0.3848177492618561\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5134 - val_loss: 0.3848 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5150Epoch 32/100, Loss: 0.5174593329429626, Val Loss: 0.35617634654045105\n",
      "591/591 [==============================] - 1s 960us/step - loss: 0.5175 - val_loss: 0.3562 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.5039Epoch 33/100, Loss: 0.501390814781189, Val Loss: 0.34866762161254883\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5014 - val_loss: 0.3487 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4983Epoch 34/100, Loss: 0.49633923172950745, Val Loss: 0.35947972536087036\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4963 - val_loss: 0.3595 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4965Epoch 35/100, Loss: 0.4968421757221222, Val Loss: 0.3454810082912445\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4968 - val_loss: 0.3455 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.5044Epoch 36/100, Loss: 0.5025830268859863, Val Loss: 0.33139315247535706\n",
      "591/591 [==============================] - 1s 950us/step - loss: 0.5026 - val_loss: 0.3314 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.4960Epoch 37/100, Loss: 0.5015749931335449, Val Loss: 0.3767916262149811\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5016 - val_loss: 0.3768 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.4937Epoch 38/100, Loss: 0.492064893245697, Val Loss: 0.3596299886703491\n",
      "591/591 [==============================] - 1s 994us/step - loss: 0.4921 - val_loss: 0.3596 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.5040Epoch 39/100, Loss: 0.4992280900478363, Val Loss: 0.3440481722354889\n",
      "591/591 [==============================] - 1s 965us/step - loss: 0.4992 - val_loss: 0.3440 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4857Epoch 40/100, Loss: 0.4863949418067932, Val Loss: 0.33207136392593384\n",
      "591/591 [==============================] - 1s 993us/step - loss: 0.4864 - val_loss: 0.3321 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4782Epoch 41/100, Loss: 0.47860780358314514, Val Loss: 0.338308721780777\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "591/591 [==============================] - 1s 955us/step - loss: 0.4786 - val_loss: 0.3383 - lr: 6.2500e-05\n",
      "Epoch 41: early stopping\n",
      "127/127 [==============================] - 0s 534us/step\n",
      "127/127 [==============================] - 0s 530us/step - loss: 0.3767\n",
      "Epoch 1/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 1.4466Epoch 1/100, Loss: 1.431514024734497, Val Loss: 1.094203233718872\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.4315 - val_loss: 1.0942 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 1.0292Epoch 2/100, Loss: 1.025713562965393, Val Loss: 0.7863814830780029\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0257 - val_loss: 0.7864 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.9558Epoch 3/100, Loss: 0.9569820761680603, Val Loss: 0.7509660124778748\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9570 - val_loss: 0.7510 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.9026Epoch 4/100, Loss: 0.9017722010612488, Val Loss: 0.7274854183197021\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9018 - val_loss: 0.7275 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8282Epoch 5/100, Loss: 0.8283188939094543, Val Loss: 0.6672426462173462\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8283 - val_loss: 0.6672 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7881Epoch 6/100, Loss: 0.7892429232597351, Val Loss: 0.815476655960083\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7892 - val_loss: 0.8155 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.7366Epoch 7/100, Loss: 0.7428376078605652, Val Loss: 0.6769459843635559\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7428 - val_loss: 0.6769 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.6898Epoch 8/100, Loss: 0.6916413903236389, Val Loss: 0.5123023390769958\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6916 - val_loss: 0.5123 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.6490Epoch 9/100, Loss: 0.6549928188323975, Val Loss: 0.5500125288963318\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6550 - val_loss: 0.5500 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.6435Epoch 10/100, Loss: 0.6435098648071289, Val Loss: 0.4743801951408386\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6435 - val_loss: 0.4744 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.6151Epoch 11/100, Loss: 0.6147428750991821, Val Loss: 0.5676073431968689\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6147 - val_loss: 0.5676 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.6152Epoch 12/100, Loss: 0.6136795878410339, Val Loss: 0.4685460329055786\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6137 - val_loss: 0.4685 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.6045Epoch 13/100, Loss: 0.6052773594856262, Val Loss: 0.46607592701911926\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6053 - val_loss: 0.4661 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.5868Epoch 14/100, Loss: 0.5850189328193665, Val Loss: 0.47745537757873535\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5850 - val_loss: 0.4775 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5824Epoch 15/100, Loss: 0.582244873046875, Val Loss: 0.48358461260795593\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5822 - val_loss: 0.4836 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.5706Epoch 16/100, Loss: 0.5704632997512817, Val Loss: 0.49305933713912964\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5705 - val_loss: 0.4931 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.5266Epoch 17/100, Loss: 0.5266278982162476, Val Loss: 0.3983941674232483\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5266 - val_loss: 0.3984 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.5153Epoch 18/100, Loss: 0.5126523375511169, Val Loss: 0.390656054019928\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5127 - val_loss: 0.3907 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.5020Epoch 19/100, Loss: 0.5031424164772034, Val Loss: 0.3564970791339874\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5031 - val_loss: 0.3565 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.5058Epoch 20/100, Loss: 0.5108223557472229, Val Loss: 0.4427150785923004\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5108 - val_loss: 0.4427 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4923Epoch 21/100, Loss: 0.4916611909866333, Val Loss: 0.3743368685245514\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4917 - val_loss: 0.3743 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.4882Epoch 22/100, Loss: 0.4885588586330414, Val Loss: 0.3763725459575653\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4886 - val_loss: 0.3764 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.4637Epoch 23/100, Loss: 0.4624672532081604, Val Loss: 0.3324609398841858\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4625 - val_loss: 0.3325 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.4652Epoch 24/100, Loss: 0.46288731694221497, Val Loss: 0.3336293697357178\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4629 - val_loss: 0.3336 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4578Epoch 25/100, Loss: 0.45819103717803955, Val Loss: 0.3225867450237274\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4582 - val_loss: 0.3226 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4548Epoch 26/100, Loss: 0.4510161280632019, Val Loss: 0.3065060079097748\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4510 - val_loss: 0.3065 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "543/591 [==========================>...] - ETA: 0s - loss: 0.4466Epoch 27/100, Loss: 0.44616377353668213, Val Loss: 0.3149358034133911\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4462 - val_loss: 0.3149 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.4472Epoch 28/100, Loss: 0.4465765953063965, Val Loss: 0.3367714583873749\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4466 - val_loss: 0.3368 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.4467Epoch 29/100, Loss: 0.4470486640930176, Val Loss: 0.3195491433143616\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4470 - val_loss: 0.3195 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.4329Epoch 30/100, Loss: 0.4319838285446167, Val Loss: 0.2950333058834076\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4320 - val_loss: 0.2950 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4279Epoch 31/100, Loss: 0.427460640668869, Val Loss: 0.29699915647506714\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4275 - val_loss: 0.2970 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.4347Epoch 32/100, Loss: 0.4355320930480957, Val Loss: 0.2897741198539734\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4355 - val_loss: 0.2898 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.4251Epoch 33/100, Loss: 0.42369574308395386, Val Loss: 0.2950778007507324\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4237 - val_loss: 0.2951 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.4227Epoch 34/100, Loss: 0.42263132333755493, Val Loss: 0.2828434109687805\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4226 - val_loss: 0.2828 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.4197Epoch 35/100, Loss: 0.4194174110889435, Val Loss: 0.28028416633605957\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4194 - val_loss: 0.2803 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.4254Epoch 36/100, Loss: 0.4233115315437317, Val Loss: 0.3030317425727844\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4233 - val_loss: 0.3030 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4159Epoch 37/100, Loss: 0.42165490984916687, Val Loss: 0.30186405777931213\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4217 - val_loss: 0.3019 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.4171Epoch 38/100, Loss: 0.4168381094932556, Val Loss: 0.2892559766769409\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4168 - val_loss: 0.2893 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.4195Epoch 39/100, Loss: 0.41545137763023376, Val Loss: 0.2692180275917053\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4155 - val_loss: 0.2692 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4085Epoch 40/100, Loss: 0.40967074036598206, Val Loss: 0.27535805106163025\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4097 - val_loss: 0.2754 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.4008Epoch 41/100, Loss: 0.40243563055992126, Val Loss: 0.28978005051612854\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4024 - val_loss: 0.2898 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4082Epoch 42/100, Loss: 0.40709295868873596, Val Loss: 0.28013867139816284\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4071 - val_loss: 0.2801 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4038Epoch 43/100, Loss: 0.4000095725059509, Val Loss: 0.2700447738170624\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4000 - val_loss: 0.2700 - lr: 3.1250e-05\n",
      "Epoch 44/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4053Epoch 44/100, Loss: 0.40202686190605164, Val Loss: 0.27257391810417175\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4020 - val_loss: 0.2726 - lr: 3.1250e-05\n",
      "Epoch 44: early stopping\n",
      "127/127 [==============================] - 0s 558us/step\n",
      "127/127 [==============================] - 0s 542us/step - loss: 0.3116\n",
      "Epoch 1/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.3715Epoch 1/100, Loss: 1.3696073293685913, Val Loss: 1.1792359352111816\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.3696 - val_loss: 1.1792 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.9987Epoch 2/100, Loss: 0.9971964955329895, Val Loss: 0.8253055214881897\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9972 - val_loss: 0.8253 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.9269Epoch 3/100, Loss: 0.9279995560646057, Val Loss: 0.7488211393356323\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9280 - val_loss: 0.7488 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.8644Epoch 4/100, Loss: 0.8633402585983276, Val Loss: 0.6158164143562317\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8633 - val_loss: 0.6158 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.8061Epoch 5/100, Loss: 0.8011871576309204, Val Loss: 0.5857078433036804\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8012 - val_loss: 0.5857 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.7313Epoch 6/100, Loss: 0.7312250733375549, Val Loss: 0.6223859786987305\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7312 - val_loss: 0.6224 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.6854Epoch 7/100, Loss: 0.6863561868667603, Val Loss: 0.5230363607406616\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6864 - val_loss: 0.5230 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.6478Epoch 8/100, Loss: 0.6488007307052612, Val Loss: 0.5423711538314819\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6488 - val_loss: 0.5424 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.6126Epoch 9/100, Loss: 0.6174796223640442, Val Loss: 0.5825848579406738\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6175 - val_loss: 0.5826 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.5915Epoch 10/100, Loss: 0.5925956964492798, Val Loss: 0.5482587814331055\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5926 - val_loss: 0.5483 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5336Epoch 11/100, Loss: 0.5339038968086243, Val Loss: 0.4297647476196289\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5339 - val_loss: 0.4298 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5273Epoch 12/100, Loss: 0.5269472002983093, Val Loss: 0.3945050835609436\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5269 - val_loss: 0.3945 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5150Epoch 13/100, Loss: 0.5156129598617554, Val Loss: 0.38908645510673523\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5156 - val_loss: 0.3891 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.5070Epoch 14/100, Loss: 0.506691575050354, Val Loss: 0.39315134286880493\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5067 - val_loss: 0.3932 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4976Epoch 15/100, Loss: 0.497331827878952, Val Loss: 0.35804739594459534\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4973 - val_loss: 0.3580 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4907Epoch 16/100, Loss: 0.4908319413661957, Val Loss: 0.4219370484352112\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4908 - val_loss: 0.4219 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4873Epoch 17/100, Loss: 0.48668769001960754, Val Loss: 0.3805612325668335\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4867 - val_loss: 0.3806 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4776Epoch 18/100, Loss: 0.4743014872074127, Val Loss: 0.4080878496170044\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4743 - val_loss: 0.4081 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4413Epoch 19/100, Loss: 0.44259682297706604, Val Loss: 0.29434728622436523\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4426 - val_loss: 0.2943 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4436Epoch 20/100, Loss: 0.4438503384590149, Val Loss: 0.36082953214645386\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4439 - val_loss: 0.3608 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4283Epoch 21/100, Loss: 0.42829060554504395, Val Loss: 0.3067392408847809\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4283 - val_loss: 0.3067 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4272Epoch 22/100, Loss: 0.4267036020755768, Val Loss: 0.32978492975234985\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4267 - val_loss: 0.3298 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4098Epoch 23/100, Loss: 0.4101020097732544, Val Loss: 0.28159433603286743\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4101 - val_loss: 0.2816 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4100Epoch 24/100, Loss: 0.4099178612232208, Val Loss: 0.29554232954978943\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4099 - val_loss: 0.2955 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4072Epoch 25/100, Loss: 0.4071935713291168, Val Loss: 0.2811294198036194\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4072 - val_loss: 0.2811 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4027Epoch 26/100, Loss: 0.4028148353099823, Val Loss: 0.2755819261074066\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4028 - val_loss: 0.2756 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3964Epoch 27/100, Loss: 0.3965049684047699, Val Loss: 0.26305434107780457\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3965 - val_loss: 0.2631 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3999Epoch 28/100, Loss: 0.3999060094356537, Val Loss: 0.27358055114746094\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.2736 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3988Epoch 29/100, Loss: 0.39835643768310547, Val Loss: 0.26207971572875977\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3984 - val_loss: 0.2621 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.3953Epoch 30/100, Loss: 0.3946608603000641, Val Loss: 0.28147637844085693\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3947 - val_loss: 0.2815 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3910Epoch 31/100, Loss: 0.3909445106983185, Val Loss: 0.2843671143054962\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.2844 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4009Epoch 32/100, Loss: 0.4017680287361145, Val Loss: 0.27647170424461365\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4018 - val_loss: 0.2765 - lr: 1.2500e-04\n",
      "Epoch 32: early stopping\n",
      "127/127 [==============================] - 0s 658us/step\n",
      "127/127 [==============================] - 0s 676us/step - loss: 0.3067\n",
      "Epoch 1/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 1.3138Epoch 1/100, Loss: 1.3091126680374146, Val Loss: 1.2519770860671997\n",
      "591/591 [==============================] - 3s 3ms/step - loss: 1.3091 - val_loss: 1.2520 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.0202Epoch 2/100, Loss: 1.0197947025299072, Val Loss: 1.0102858543395996\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 1.0198 - val_loss: 1.0103 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.9430Epoch 3/100, Loss: 0.9427055716514587, Val Loss: 0.780957043170929\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.9427 - val_loss: 0.7810 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8713Epoch 4/100, Loss: 0.873184859752655, Val Loss: 0.9186615943908691\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.8732 - val_loss: 0.9187 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.8054Epoch 5/100, Loss: 0.8042250275611877, Val Loss: 0.6962567567825317\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.8042 - val_loss: 0.6963 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7412Epoch 6/100, Loss: 0.7415632009506226, Val Loss: 0.6403060555458069\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.7416 - val_loss: 0.6403 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.6942Epoch 7/100, Loss: 0.6970149278640747, Val Loss: 0.6111650466918945\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.6970 - val_loss: 0.6112 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.6623Epoch 8/100, Loss: 0.6644003391265869, Val Loss: 0.5212558507919312\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.6644 - val_loss: 0.5213 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6266Epoch 9/100, Loss: 0.6329264044761658, Val Loss: 0.5870885848999023\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.6329 - val_loss: 0.5871 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.6067Epoch 10/100, Loss: 0.6074237823486328, Val Loss: 0.46836066246032715\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.6074 - val_loss: 0.4684 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5906Epoch 11/100, Loss: 0.5913341045379639, Val Loss: 0.5110268592834473\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.5913 - val_loss: 0.5110 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.5873Epoch 12/100, Loss: 0.5864774584770203, Val Loss: 0.4858822226524353\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.5865 - val_loss: 0.4859 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5723Epoch 13/100, Loss: 0.5739899277687073, Val Loss: 0.5086885094642639\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.5740 - val_loss: 0.5087 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5052Epoch 14/100, Loss: 0.5048612356185913, Val Loss: 0.44055482745170593\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.5049 - val_loss: 0.4406 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4930Epoch 15/100, Loss: 0.497362345457077, Val Loss: 0.4331476390361786\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4974 - val_loss: 0.4331 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4825Epoch 16/100, Loss: 0.4816947281360626, Val Loss: 0.43075424432754517\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4817 - val_loss: 0.4308 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4839Epoch 17/100, Loss: 0.48379552364349365, Val Loss: 0.36577051877975464\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4838 - val_loss: 0.3658 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4743Epoch 18/100, Loss: 0.47363927960395813, Val Loss: 0.37478139996528625\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4736 - val_loss: 0.3748 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.4629Epoch 19/100, Loss: 0.4638199508190155, Val Loss: 0.3711130917072296\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4638 - val_loss: 0.3711 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4614Epoch 20/100, Loss: 0.4625582695007324, Val Loss: 0.3641871213912964\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4626 - val_loss: 0.3642 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4523Epoch 21/100, Loss: 0.4515208601951599, Val Loss: 0.40854203701019287\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4515 - val_loss: 0.4085 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4475Epoch 22/100, Loss: 0.44712111353874207, Val Loss: 0.4309866726398468\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4471 - val_loss: 0.4310 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4498Epoch 23/100, Loss: 0.44990596175193787, Val Loss: 0.40376371145248413\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4499 - val_loss: 0.4038 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4178Epoch 24/100, Loss: 0.4176696836948395, Val Loss: 0.31220492720603943\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4177 - val_loss: 0.3122 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4077Epoch 25/100, Loss: 0.40744519233703613, Val Loss: 0.3130819499492645\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4074 - val_loss: 0.3131 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.4022Epoch 26/100, Loss: 0.402135968208313, Val Loss: 0.28775519132614136\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.4021 - val_loss: 0.2878 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3964Epoch 27/100, Loss: 0.39520344138145447, Val Loss: 0.2863486707210541\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3952 - val_loss: 0.2863 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.3942Epoch 28/100, Loss: 0.39586231112480164, Val Loss: 0.2965892553329468\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3959 - val_loss: 0.2966 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3963Epoch 29/100, Loss: 0.3963058590888977, Val Loss: 0.3107393682003021\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3963 - val_loss: 0.3107 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.3950Epoch 30/100, Loss: 0.39533868432044983, Val Loss: 0.3169757127761841\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3953 - val_loss: 0.3170 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3729Epoch 31/100, Loss: 0.3718501031398773, Val Loss: 0.2819901704788208\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3719 - val_loss: 0.2820 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.3771Epoch 32/100, Loss: 0.37763285636901855, Val Loss: 0.2694379687309265\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3776 - val_loss: 0.2694 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3671Epoch 33/100, Loss: 0.36703383922576904, Val Loss: 0.2694365382194519\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3670 - val_loss: 0.2694 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3602Epoch 34/100, Loss: 0.35985425114631653, Val Loss: 0.2869320809841156\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3599 - val_loss: 0.2869 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.3593Epoch 35/100, Loss: 0.3600063621997833, Val Loss: 0.26512306928634644\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3600 - val_loss: 0.2651 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3694Epoch 36/100, Loss: 0.3693831264972687, Val Loss: 0.2726675570011139\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3694 - val_loss: 0.2727 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3570Epoch 37/100, Loss: 0.362816721200943, Val Loss: 0.2647727429866791\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3628 - val_loss: 0.2648 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3563Epoch 38/100, Loss: 0.35586118698120117, Val Loss: 0.27735820412635803\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3559 - val_loss: 0.2774 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3602Epoch 39/100, Loss: 0.3597120940685272, Val Loss: 0.257767915725708\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3597 - val_loss: 0.2578 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3537Epoch 40/100, Loss: 0.35355696082115173, Val Loss: 0.2642233371734619\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3536 - val_loss: 0.2642 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.3508Epoch 41/100, Loss: 0.34947267174720764, Val Loss: 0.256613552570343\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3495 - val_loss: 0.2566 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3518Epoch 42/100, Loss: 0.35220927000045776, Val Loss: 0.2500855624675751\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3522 - val_loss: 0.2501 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3465Epoch 43/100, Loss: 0.3453501760959625, Val Loss: 0.25093570351600647\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3454 - val_loss: 0.2509 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3509Epoch 44/100, Loss: 0.3505769968032837, Val Loss: 0.25682756304740906\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3506 - val_loss: 0.2568 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3458Epoch 45/100, Loss: 0.35118767619132996, Val Loss: 0.2664332389831543\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3512 - val_loss: 0.2664 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3441Epoch 46/100, Loss: 0.34216606616973877, Val Loss: 0.23352684080600739\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3422 - val_loss: 0.2335 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3361Epoch 47/100, Loss: 0.3354678153991699, Val Loss: 0.23470914363861084\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3355 - val_loss: 0.2347 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3370Epoch 48/100, Loss: 0.3365267813205719, Val Loss: 0.24027647078037262\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3365 - val_loss: 0.2403 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3310Epoch 49/100, Loss: 0.33096978068351746, Val Loss: 0.23743528127670288\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3310 - val_loss: 0.2374 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3282Epoch 50/100, Loss: 0.3266693353652954, Val Loss: 0.23133447766304016\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3267 - val_loss: 0.2313 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3292Epoch 51/100, Loss: 0.3288029730319977, Val Loss: 0.23426736891269684\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3288 - val_loss: 0.2343 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3276Epoch 52/100, Loss: 0.32591772079467773, Val Loss: 0.22501195967197418\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3259 - val_loss: 0.2250 - lr: 3.1250e-05\n",
      "Epoch 53/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3249Epoch 53/100, Loss: 0.32438454031944275, Val Loss: 0.23125828802585602\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3244 - val_loss: 0.2313 - lr: 3.1250e-05\n",
      "Epoch 54/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3339Epoch 54/100, Loss: 0.332035094499588, Val Loss: 0.23555542528629303\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3320 - val_loss: 0.2356 - lr: 3.1250e-05\n",
      "Epoch 55/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3296Epoch 55/100, Loss: 0.3293873369693756, Val Loss: 0.22459015250205994\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3294 - val_loss: 0.2246 - lr: 3.1250e-05\n",
      "Epoch 56/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3254Epoch 56/100, Loss: 0.3250991106033325, Val Loss: 0.2344651073217392\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3251 - val_loss: 0.2345 - lr: 3.1250e-05\n",
      "Epoch 57/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3275Epoch 57/100, Loss: 0.3285486400127411, Val Loss: 0.23549675941467285\n",
      "Restoring model weights from the end of the best epoch: 52.\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3285 - val_loss: 0.2355 - lr: 3.1250e-05\n",
      "Epoch 57: early stopping\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 962us/step - loss: 0.2726\n",
      "Epoch 1/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 2.3412Epoch 1/100, Loss: 2.2913801670074463, Val Loss: 1.536560297012329\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 2.2914 - val_loss: 1.5366 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.3109Epoch 2/100, Loss: 1.307226300239563, Val Loss: 1.0428129434585571\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.3072 - val_loss: 1.0428 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 1.0897Epoch 3/100, Loss: 1.0890134572982788, Val Loss: 0.851997435092926\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0890 - val_loss: 0.8520 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 1.0408Epoch 4/100, Loss: 1.0423837900161743, Val Loss: 0.8119977116584778\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0424 - val_loss: 0.8120 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 1.0173Epoch 5/100, Loss: 1.0145907402038574, Val Loss: 0.7839105129241943\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0146 - val_loss: 0.7839 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.9985Epoch 6/100, Loss: 1.0000613927841187, Val Loss: 0.8209385275840759\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0001 - val_loss: 0.8209 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.9762Epoch 7/100, Loss: 0.9817711710929871, Val Loss: 0.6941004395484924\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9818 - val_loss: 0.6941 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9516Epoch 8/100, Loss: 0.9522619843482971, Val Loss: 0.701062798500061\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9523 - val_loss: 0.7011 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.9266Epoch 9/100, Loss: 0.9342412948608398, Val Loss: 0.6556679010391235\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9342 - val_loss: 0.6557 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.9292Epoch 10/100, Loss: 0.9293286204338074, Val Loss: 0.6348966956138611\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9293 - val_loss: 0.6349 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.9140Epoch 11/100, Loss: 0.9125321507453918, Val Loss: 0.6233826279640198\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9125 - val_loss: 0.6234 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.9064Epoch 12/100, Loss: 0.9073694944381714, Val Loss: 0.6593672633171082\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9074 - val_loss: 0.6594 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8957Epoch 13/100, Loss: 0.8965808153152466, Val Loss: 0.6666602492332458\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.8966 - val_loss: 0.6667 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.8891Epoch 14/100, Loss: 0.8917312622070312, Val Loss: 0.6446385383605957\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8917 - val_loss: 0.6446 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8550Epoch 15/100, Loss: 0.8594390749931335, Val Loss: 0.5962064862251282\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8594 - val_loss: 0.5962 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.8674Epoch 16/100, Loss: 0.8664200901985168, Val Loss: 0.6125262975692749\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8664 - val_loss: 0.6125 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.8598Epoch 17/100, Loss: 0.8532363772392273, Val Loss: 0.6128589510917664\n",
      "591/591 [==============================] - 1s 969us/step - loss: 0.8532 - val_loss: 0.6129 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.8531Epoch 18/100, Loss: 0.8498995304107666, Val Loss: 0.6229492425918579\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8499 - val_loss: 0.6229 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.8255Epoch 19/100, Loss: 0.8259636163711548, Val Loss: 0.5799294710159302\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8260 - val_loss: 0.5799 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.8353Epoch 20/100, Loss: 0.8365855813026428, Val Loss: 0.5610333681106567\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8366 - val_loss: 0.5610 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.8142Epoch 21/100, Loss: 0.8124573230743408, Val Loss: 0.5547066926956177\n",
      "591/591 [==============================] - 1s 969us/step - loss: 0.8125 - val_loss: 0.5547 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.8137Epoch 22/100, Loss: 0.8153628706932068, Val Loss: 0.5914357304573059\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8154 - val_loss: 0.5914 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.8119Epoch 23/100, Loss: 0.8105337619781494, Val Loss: 0.5729251503944397\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8105 - val_loss: 0.5729 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.8313Epoch 24/100, Loss: 0.8300850987434387, Val Loss: 0.5560453534126282\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8301 - val_loss: 0.5560 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "533/591 [==========================>...] - ETA: 0s - loss: 0.8050Epoch 25/100, Loss: 0.810977041721344, Val Loss: 0.5598094463348389\n",
      "591/591 [==============================] - 1s 974us/step - loss: 0.8110 - val_loss: 0.5598 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.8088Epoch 26/100, Loss: 0.8068117499351501, Val Loss: 0.5420230627059937\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8068 - val_loss: 0.5420 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.7974Epoch 27/100, Loss: 0.7982912659645081, Val Loss: 0.5413960814476013\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7983 - val_loss: 0.5414 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.7973Epoch 28/100, Loss: 0.7994422316551208, Val Loss: 0.5612191557884216\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7994 - val_loss: 0.5612 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.7977Epoch 29/100, Loss: 0.7980493307113647, Val Loss: 0.5386099815368652\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7980 - val_loss: 0.5386 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7996Epoch 30/100, Loss: 0.7999808192253113, Val Loss: 0.5389795303344727\n",
      "591/591 [==============================] - 1s 985us/step - loss: 0.8000 - val_loss: 0.5390 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.7980Epoch 31/100, Loss: 0.7976331114768982, Val Loss: 0.5443325638771057\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7976 - val_loss: 0.5443 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "537/591 [==========================>...] - ETA: 0s - loss: 0.8074Epoch 32/100, Loss: 0.8077623248100281, Val Loss: 0.5328836441040039\n",
      "591/591 [==============================] - 1s 978us/step - loss: 0.8078 - val_loss: 0.5329 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.7910Epoch 33/100, Loss: 0.7875509262084961, Val Loss: 0.5358878374099731\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7876 - val_loss: 0.5359 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.7860Epoch 34/100, Loss: 0.7867730259895325, Val Loss: 0.5376639366149902\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7868 - val_loss: 0.5377 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.7818Epoch 35/100, Loss: 0.78240966796875, Val Loss: 0.5495839715003967\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7824 - val_loss: 0.5496 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.7883Epoch 36/100, Loss: 0.788786768913269, Val Loss: 0.542550802230835\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7888 - val_loss: 0.5426 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.7818Epoch 37/100, Loss: 0.788429856300354, Val Loss: 0.5326722264289856\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7884 - val_loss: 0.5327 - lr: 6.2500e-05\n",
      "Epoch 37: early stopping\n",
      "127/127 [==============================] - 0s 546us/step\n",
      "127/127 [==============================] - 0s 506us/step - loss: 0.5778\n",
      "Epoch 1/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 1.8805Epoch 1/100, Loss: 1.8458712100982666, Val Loss: 1.238827109336853\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.8459 - val_loss: 1.2388 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 1.1073Epoch 2/100, Loss: 1.1033185720443726, Val Loss: 0.8293808698654175\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.1033 - val_loss: 0.8294 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 1.0156Epoch 3/100, Loss: 1.0158727169036865, Val Loss: 0.7994452118873596\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0159 - val_loss: 0.7994 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.9684Epoch 4/100, Loss: 0.970687747001648, Val Loss: 0.6969000101089478\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9707 - val_loss: 0.6969 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.9424Epoch 5/100, Loss: 0.9385051727294922, Val Loss: 0.6803242564201355\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9385 - val_loss: 0.6803 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.9254Epoch 6/100, Loss: 0.9232888221740723, Val Loss: 0.7996246218681335\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9233 - val_loss: 0.7996 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.9197Epoch 7/100, Loss: 0.9234890937805176, Val Loss: 0.7645272612571716\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9235 - val_loss: 0.7645 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.8910Epoch 8/100, Loss: 0.8909645676612854, Val Loss: 0.6675508618354797\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8910 - val_loss: 0.6676 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.8541Epoch 9/100, Loss: 0.8592039942741394, Val Loss: 0.5819971561431885\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8592 - val_loss: 0.5820 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.8407Epoch 10/100, Loss: 0.8413680195808411, Val Loss: 0.7200814485549927\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8414 - val_loss: 0.7201 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8165Epoch 11/100, Loss: 0.8171074390411377, Val Loss: 0.6547041535377502\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8171 - val_loss: 0.6547 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.8008Epoch 12/100, Loss: 0.7992061376571655, Val Loss: 0.5836806893348694\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7992 - val_loss: 0.5837 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7651Epoch 13/100, Loss: 0.7655503153800964, Val Loss: 0.5366798639297485\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7656 - val_loss: 0.5367 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7543Epoch 14/100, Loss: 0.7535629868507385, Val Loss: 0.5512585043907166\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7536 - val_loss: 0.5513 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.7395Epoch 15/100, Loss: 0.7397629022598267, Val Loss: 0.5231444835662842\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7398 - val_loss: 0.5231 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.7358Epoch 16/100, Loss: 0.7346636056900024, Val Loss: 0.5175577998161316\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7347 - val_loss: 0.5176 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.7325Epoch 17/100, Loss: 0.7276097536087036, Val Loss: 0.5108830332756042\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7276 - val_loss: 0.5109 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7217Epoch 18/100, Loss: 0.7170517444610596, Val Loss: 0.5436184406280518\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7171 - val_loss: 0.5436 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.7060Epoch 19/100, Loss: 0.7068184018135071, Val Loss: 0.5157403945922852\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7068 - val_loss: 0.5157 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.7101Epoch 20/100, Loss: 0.7120327353477478, Val Loss: 0.5330815315246582\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7120 - val_loss: 0.5331 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.6900Epoch 21/100, Loss: 0.6841664910316467, Val Loss: 0.4951876699924469\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6842 - val_loss: 0.4952 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.6731Epoch 22/100, Loss: 0.6742721199989319, Val Loss: 0.4862429201602936\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6743 - val_loss: 0.4862 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "536/591 [==========================>...] - ETA: 0s - loss: 0.6743Epoch 23/100, Loss: 0.6738911867141724, Val Loss: 0.4824462831020355\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6739 - val_loss: 0.4824 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.6878Epoch 24/100, Loss: 0.6870654821395874, Val Loss: 0.45375120639801025\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6871 - val_loss: 0.4538 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.6801Epoch 25/100, Loss: 0.6798326969146729, Val Loss: 0.47476431727409363\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6798 - val_loss: 0.4748 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.6729Epoch 26/100, Loss: 0.6688354015350342, Val Loss: 0.45839983224868774\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6688 - val_loss: 0.4584 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.6634Epoch 27/100, Loss: 0.6616131663322449, Val Loss: 0.45950233936309814\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6616 - val_loss: 0.4595 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.6578Epoch 28/100, Loss: 0.6596282720565796, Val Loss: 0.4918614625930786\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6596 - val_loss: 0.4919 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.6577Epoch 29/100, Loss: 0.6572622656822205, Val Loss: 0.42897871136665344\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6573 - val_loss: 0.4290 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.6544Epoch 30/100, Loss: 0.6567699313163757, Val Loss: 0.43309780955314636\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6568 - val_loss: 0.4331 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.6470Epoch 31/100, Loss: 0.6471499800682068, Val Loss: 0.4451231360435486\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6471 - val_loss: 0.4451 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6620Epoch 32/100, Loss: 0.6625053882598877, Val Loss: 0.4417929947376251\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6625 - val_loss: 0.4418 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.6471Epoch 33/100, Loss: 0.6463378071784973, Val Loss: 0.4614523947238922\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6463 - val_loss: 0.4615 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.6425Epoch 34/100, Loss: 0.6429121494293213, Val Loss: 0.41741666197776794\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6429 - val_loss: 0.4174 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.6418Epoch 35/100, Loss: 0.641440212726593, Val Loss: 0.4319239854812622\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6414 - val_loss: 0.4319 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6461Epoch 36/100, Loss: 0.6461692452430725, Val Loss: 0.4241715967655182\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6462 - val_loss: 0.4242 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.6404Epoch 37/100, Loss: 0.648760974407196, Val Loss: 0.4469938576221466\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6488 - val_loss: 0.4470 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.6336Epoch 38/100, Loss: 0.6338934898376465, Val Loss: 0.4260222911834717\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6339 - val_loss: 0.4260 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.6456Epoch 39/100, Loss: 0.6451422572135925, Val Loss: 0.42197898030281067\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6451 - val_loss: 0.4220 - lr: 3.1250e-05\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 550us/step\n",
      "127/127 [==============================] - 0s 530us/step - loss: 0.4624\n",
      "Epoch 1/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 1.5993Epoch 1/100, Loss: 1.5796650648117065, Val Loss: 1.0198134183883667\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.5797 - val_loss: 1.0198 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 1.0328Epoch 2/100, Loss: 1.0303122997283936, Val Loss: 0.8951895236968994\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0303 - val_loss: 0.8952 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.9665Epoch 3/100, Loss: 0.967796266078949, Val Loss: 0.7746028304100037\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9678 - val_loss: 0.7746 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.9117Epoch 4/100, Loss: 0.9115073084831238, Val Loss: 0.6485621333122253\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9115 - val_loss: 0.6486 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8739Epoch 5/100, Loss: 0.8714450001716614, Val Loss: 0.6202081441879272\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8714 - val_loss: 0.6202 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.8386Epoch 6/100, Loss: 0.839572548866272, Val Loss: 0.7793654799461365\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8396 - val_loss: 0.7794 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.7965Epoch 7/100, Loss: 0.7985150218009949, Val Loss: 0.5256534814834595\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7985 - val_loss: 0.5257 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7592Epoch 8/100, Loss: 0.7616756558418274, Val Loss: 0.5613604187965393\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7617 - val_loss: 0.5614 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7233Epoch 9/100, Loss: 0.7279273867607117, Val Loss: 0.692308783531189\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7279 - val_loss: 0.6923 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.7096Epoch 10/100, Loss: 0.7100633382797241, Val Loss: 0.5070317983627319\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7101 - val_loss: 0.5070 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6810Epoch 11/100, Loss: 0.6810340285301208, Val Loss: 0.5542413592338562\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6810 - val_loss: 0.5542 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.6744Epoch 12/100, Loss: 0.6747797727584839, Val Loss: 0.6013477444648743\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6748 - val_loss: 0.6013 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.6644Epoch 13/100, Loss: 0.6664687991142273, Val Loss: 1.0354193449020386\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6665 - val_loss: 1.0354 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.6197Epoch 14/100, Loss: 0.6174648404121399, Val Loss: 0.6374619007110596\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6175 - val_loss: 0.6375 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.5988Epoch 15/100, Loss: 0.603289783000946, Val Loss: 0.47688931226730347\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6033 - val_loss: 0.4769 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.5884Epoch 16/100, Loss: 0.5878164172172546, Val Loss: 0.4809384346008301\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5878 - val_loss: 0.4809 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.5847Epoch 17/100, Loss: 0.5841697454452515, Val Loss: 0.3977448344230652\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5842 - val_loss: 0.3977 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.5843Epoch 18/100, Loss: 0.579269528388977, Val Loss: 0.40964654088020325\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5793 - val_loss: 0.4096 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5641Epoch 19/100, Loss: 0.5648990273475647, Val Loss: 0.41891565918922424\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5649 - val_loss: 0.4189 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.5630Epoch 20/100, Loss: 0.5644177794456482, Val Loss: 0.3902561664581299\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5644 - val_loss: 0.3903 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5603Epoch 21/100, Loss: 0.5562726855278015, Val Loss: 0.47708818316459656\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5563 - val_loss: 0.4771 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.5492Epoch 22/100, Loss: 0.5499456524848938, Val Loss: 0.37760835886001587\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5499 - val_loss: 0.3776 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.5376Epoch 23/100, Loss: 0.5376307964324951, Val Loss: 0.4013749063014984\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5376 - val_loss: 0.4014 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.5436Epoch 24/100, Loss: 0.5409591794013977, Val Loss: 0.3887835741043091\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5410 - val_loss: 0.3888 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5405Epoch 25/100, Loss: 0.5392906069755554, Val Loss: 0.41938862204551697\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5393 - val_loss: 0.4194 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5148Epoch 26/100, Loss: 0.5128913521766663, Val Loss: 0.3402779698371887\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5129 - val_loss: 0.3403 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.5029Epoch 27/100, Loss: 0.5013187527656555, Val Loss: 0.3489637076854706\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5013 - val_loss: 0.3490 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5048Epoch 28/100, Loss: 0.5050413608551025, Val Loss: 0.33545374870300293\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5050 - val_loss: 0.3355 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.4994Epoch 29/100, Loss: 0.4996248185634613, Val Loss: 0.36321043968200684\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4996 - val_loss: 0.3632 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.4961Epoch 30/100, Loss: 0.49647998809814453, Val Loss: 0.3445951044559479\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4965 - val_loss: 0.3446 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4904Epoch 31/100, Loss: 0.49050724506378174, Val Loss: 0.3217655122280121\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4905 - val_loss: 0.3218 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.4961Epoch 32/100, Loss: 0.4966244101524353, Val Loss: 0.34786248207092285\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4966 - val_loss: 0.3479 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4909Epoch 33/100, Loss: 0.4880666732788086, Val Loss: 0.3832159638404846\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4881 - val_loss: 0.3832 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4841Epoch 34/100, Loss: 0.48382627964019775, Val Loss: 0.34748318791389465\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4838 - val_loss: 0.3475 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.4732Epoch 35/100, Loss: 0.4732869863510132, Val Loss: 0.333031564950943\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4733 - val_loss: 0.3330 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4751Epoch 36/100, Loss: 0.47551870346069336, Val Loss: 0.3192491829395294\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4755 - val_loss: 0.3192 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4674Epoch 37/100, Loss: 0.4743846654891968, Val Loss: 0.3356761634349823\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4744 - val_loss: 0.3357 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4691Epoch 38/100, Loss: 0.46910667419433594, Val Loss: 0.3188835680484772\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4691 - val_loss: 0.3189 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.4780Epoch 39/100, Loss: 0.47204214334487915, Val Loss: 0.31535136699676514\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4720 - val_loss: 0.3154 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4654Epoch 40/100, Loss: 0.46669915318489075, Val Loss: 0.321116179227829\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4667 - val_loss: 0.3211 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4580Epoch 41/100, Loss: 0.4577019214630127, Val Loss: 0.33758312463760376\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4577 - val_loss: 0.3376 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.4683Epoch 42/100, Loss: 0.4647819399833679, Val Loss: 0.319225937128067\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4648 - val_loss: 0.3192 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4558Epoch 43/100, Loss: 0.45508986711502075, Val Loss: 0.29963985085487366\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4551 - val_loss: 0.2996 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.4572Epoch 44/100, Loss: 0.4544490575790405, Val Loss: 0.3080997169017792\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4544 - val_loss: 0.3081 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4526Epoch 45/100, Loss: 0.45785239338874817, Val Loss: 0.3149643540382385\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4579 - val_loss: 0.3150 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4581Epoch 46/100, Loss: 0.4574708640575409, Val Loss: 0.3109723925590515\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4575 - val_loss: 0.3110 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4538Epoch 47/100, Loss: 0.4524099826812744, Val Loss: 0.2995392382144928\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4524 - val_loss: 0.2995 - lr: 3.1250e-05\n",
      "Epoch 48/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4540Epoch 48/100, Loss: 0.45260143280029297, Val Loss: 0.3054962456226349\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4526 - val_loss: 0.3055 - lr: 3.1250e-05\n",
      "Epoch 48: early stopping\n",
      "127/127 [==============================] - 0s 550us/step\n",
      "127/127 [==============================] - 0s 570us/step - loss: 0.3413\n",
      "Epoch 1/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 1.4312Epoch 1/100, Loss: 1.4253168106079102, Val Loss: 0.9844220876693726\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.4253 - val_loss: 0.9844 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.0131Epoch 2/100, Loss: 1.0114058256149292, Val Loss: 0.7838486433029175\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0114 - val_loss: 0.7838 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.9516Epoch 3/100, Loss: 0.9519704580307007, Val Loss: 0.7785990834236145\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9520 - val_loss: 0.7786 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.8861Epoch 4/100, Loss: 0.8865810036659241, Val Loss: 0.6714462637901306\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8866 - val_loss: 0.6714 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.8307Epoch 5/100, Loss: 0.8307327628135681, Val Loss: 0.613387942314148\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8307 - val_loss: 0.6134 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7695Epoch 6/100, Loss: 0.7679044604301453, Val Loss: 0.7368408441543579\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7679 - val_loss: 0.7368 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7242Epoch 7/100, Loss: 0.7258887887001038, Val Loss: 0.600825846195221\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7259 - val_loss: 0.6008 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7033Epoch 8/100, Loss: 0.7059276103973389, Val Loss: 0.4825221300125122\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7059 - val_loss: 0.4825 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6665Epoch 9/100, Loss: 0.6717246174812317, Val Loss: 0.6058957576751709\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6717 - val_loss: 0.6059 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6514Epoch 10/100, Loss: 0.65230792760849, Val Loss: 0.5419407486915588\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6523 - val_loss: 0.5419 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.6222Epoch 11/100, Loss: 0.6218807697296143, Val Loss: 0.46355289220809937\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6219 - val_loss: 0.4636 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.6157Epoch 12/100, Loss: 0.6144327521324158, Val Loss: 0.6778143644332886\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6144 - val_loss: 0.6778 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.5992Epoch 13/100, Loss: 0.6013519167900085, Val Loss: 0.7246841788291931\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6014 - val_loss: 0.7247 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.5851Epoch 14/100, Loss: 0.5833037495613098, Val Loss: 0.4634113311767578\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5833 - val_loss: 0.4634 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5747Epoch 15/100, Loss: 0.574104905128479, Val Loss: 2.3264026641845703\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5741 - val_loss: 2.3264 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.5626Epoch 16/100, Loss: 0.5603787302970886, Val Loss: 0.3929665684700012\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5604 - val_loss: 0.3930 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.5576Epoch 17/100, Loss: 0.5579463243484497, Val Loss: 10.531145095825195\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5579 - val_loss: 10.5311 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.5548Epoch 18/100, Loss: 0.5503453612327576, Val Loss: 1.5831931829452515\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5503 - val_loss: 1.5832 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5454Epoch 19/100, Loss: 0.5455342531204224, Val Loss: 0.6758389472961426\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5455 - val_loss: 0.6758 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.5087Epoch 20/100, Loss: 0.5096555948257446, Val Loss: 0.3812255263328552\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5097 - val_loss: 0.3812 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.4810Epoch 21/100, Loss: 0.47916465997695923, Val Loss: 0.3379526138305664\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4792 - val_loss: 0.3380 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.4699Epoch 22/100, Loss: 0.47013401985168457, Val Loss: 0.4043625593185425\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4701 - val_loss: 0.4044 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4643Epoch 23/100, Loss: 0.4633934199810028, Val Loss: 0.35682058334350586\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4634 - val_loss: 0.3568 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4767Epoch 24/100, Loss: 0.47538185119628906, Val Loss: 0.34841084480285645\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4754 - val_loss: 0.3484 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4407Epoch 25/100, Loss: 0.4392842650413513, Val Loss: 4.012193202972412\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4393 - val_loss: 4.0122 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4301Epoch 26/100, Loss: 0.42945224046707153, Val Loss: 0.3093835115432739\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4295 - val_loss: 0.3094 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.4255Epoch 27/100, Loss: 0.42354056239128113, Val Loss: 1.0101020336151123\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4235 - val_loss: 1.0101 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.4263Epoch 28/100, Loss: 0.42800775170326233, Val Loss: 0.3064807951450348\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4280 - val_loss: 0.3065 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4230Epoch 29/100, Loss: 0.42290863394737244, Val Loss: 0.4322613477706909\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4229 - val_loss: 0.4323 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.4150Epoch 30/100, Loss: 0.41724875569343567, Val Loss: 0.3648027181625366\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4172 - val_loss: 0.3648 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4111Epoch 31/100, Loss: 0.4105076491832733, Val Loss: 0.3130370080471039\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4105 - val_loss: 0.3130 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4075Epoch 32/100, Loss: 0.4081886410713196, Val Loss: 0.3024231493473053\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4082 - val_loss: 0.3024 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3976Epoch 33/100, Loss: 0.39698198437690735, Val Loss: 0.28397390246391296\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3970 - val_loss: 0.2840 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.3915Epoch 34/100, Loss: 0.3912019729614258, Val Loss: 0.27451443672180176\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3912 - val_loss: 0.2745 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3913Epoch 35/100, Loss: 0.39066222310066223, Val Loss: 0.29024118185043335\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3907 - val_loss: 0.2902 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.3972Epoch 36/100, Loss: 0.3962635397911072, Val Loss: 0.2873714864253998\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3963 - val_loss: 0.2874 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.3853Epoch 37/100, Loss: 0.39162319898605347, Val Loss: 0.2848360538482666\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3916 - val_loss: 0.2848 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3835Epoch 38/100, Loss: 0.3834153115749359, Val Loss: 0.2760893702507019\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3834 - val_loss: 0.2761 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3860Epoch 39/100, Loss: 0.3848193883895874, Val Loss: 0.27754443883895874\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3848 - val_loss: 0.2775 - lr: 6.2500e-05\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 588us/step\n",
      "127/127 [==============================] - 0s 724us/step - loss: 0.3180\n",
      "Epoch 1/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 1.3523Epoch 1/100, Loss: 1.3510478734970093, Val Loss: 1.1171023845672607\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.3510 - val_loss: 1.1171 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 1.0269Epoch 2/100, Loss: 1.0258302688598633, Val Loss: 0.8697836995124817\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0258 - val_loss: 0.8698 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.9666Epoch 3/100, Loss: 0.9676651358604431, Val Loss: 0.7183573246002197\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9677 - val_loss: 0.7184 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.8861Epoch 4/100, Loss: 0.8872933387756348, Val Loss: 0.7130030989646912\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8873 - val_loss: 0.7130 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.8182Epoch 5/100, Loss: 0.8181586265563965, Val Loss: 0.6220526099205017\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8182 - val_loss: 0.6221 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7611Epoch 6/100, Loss: 0.7603429555892944, Val Loss: 0.7982248067855835\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7603 - val_loss: 0.7982 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7127Epoch 7/100, Loss: 0.7132293581962585, Val Loss: 0.5286840200424194\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7132 - val_loss: 0.5287 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.6731Epoch 8/100, Loss: 0.6751283407211304, Val Loss: 0.5613035559654236\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6751 - val_loss: 0.5613 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.6385Epoch 9/100, Loss: 0.6385295987129211, Val Loss: 0.5970017910003662\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6385 - val_loss: 0.5970 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.6153Epoch 10/100, Loss: 0.6155936121940613, Val Loss: 0.4712561368942261\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6156 - val_loss: 0.4713 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5888Epoch 11/100, Loss: 0.5888991355895996, Val Loss: 0.5841063857078552\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5889 - val_loss: 0.5841 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5925Epoch 12/100, Loss: 0.5918938517570496, Val Loss: 0.4725300371646881\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5919 - val_loss: 0.4725 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.5700Epoch 13/100, Loss: 0.5720989108085632, Val Loss: 0.484336256980896\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5721 - val_loss: 0.4843 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.5084Epoch 14/100, Loss: 0.506676435470581, Val Loss: 0.40753427147865295\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5067 - val_loss: 0.4075 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4929Epoch 15/100, Loss: 0.4929890036582947, Val Loss: 0.361762136220932\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4930 - val_loss: 0.3618 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.4789Epoch 16/100, Loss: 0.47961005568504333, Val Loss: 0.38582393527030945\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4796 - val_loss: 0.3858 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.4771Epoch 17/100, Loss: 0.47569817304611206, Val Loss: 0.3917980492115021\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4757 - val_loss: 0.3918 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4692Epoch 18/100, Loss: 0.4683624804019928, Val Loss: 0.35102128982543945\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4684 - val_loss: 0.3510 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4578Epoch 19/100, Loss: 0.45783141255378723, Val Loss: 0.3626312017440796\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4578 - val_loss: 0.3626 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.4587Epoch 20/100, Loss: 0.46005979180336, Val Loss: 0.3650933802127838\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4601 - val_loss: 0.3651 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.4449Epoch 21/100, Loss: 0.44203299283981323, Val Loss: 0.3123982548713684\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4420 - val_loss: 0.3124 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4371Epoch 22/100, Loss: 0.43686142563819885, Val Loss: 0.3478447496891022\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4369 - val_loss: 0.3478 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4357Epoch 23/100, Loss: 0.43463483452796936, Val Loss: 0.46848559379577637\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4346 - val_loss: 0.4685 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4393Epoch 24/100, Loss: 0.43863752484321594, Val Loss: 0.31905633211135864\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4386 - val_loss: 0.3191 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4081Epoch 25/100, Loss: 0.40771257877349854, Val Loss: 0.2976790964603424\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4077 - val_loss: 0.2977 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3945Epoch 26/100, Loss: 0.39453431963920593, Val Loss: 0.3019939959049225\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3945 - val_loss: 0.3020 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3885Epoch 27/100, Loss: 0.3867626488208771, Val Loss: 0.2850940227508545\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.2851 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.3867Epoch 28/100, Loss: 0.3879946172237396, Val Loss: 0.2909579873085022\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3880 - val_loss: 0.2910 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.3866Epoch 29/100, Loss: 0.3871713876724243, Val Loss: 0.28185731172561646\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3872 - val_loss: 0.2819 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3814Epoch 30/100, Loss: 0.38168975710868835, Val Loss: 0.2830592393875122\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3817 - val_loss: 0.2831 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.3808Epoch 31/100, Loss: 0.3779151737689972, Val Loss: 0.27637696266174316\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3779 - val_loss: 0.2764 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.3805Epoch 32/100, Loss: 0.3803122043609619, Val Loss: 0.3071870803833008\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3803 - val_loss: 0.3072 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3735Epoch 33/100, Loss: 0.3712517023086548, Val Loss: 0.29388225078582764\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3713 - val_loss: 0.2939 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3743Epoch 34/100, Loss: 0.37335050106048584, Val Loss: 0.30509117245674133\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3734 - val_loss: 0.3051 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.3512Epoch 35/100, Loss: 0.35220202803611755, Val Loss: 0.2882690727710724\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3522 - val_loss: 0.2883 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3557Epoch 36/100, Loss: 0.3545708954334259, Val Loss: 0.2634354531764984\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3546 - val_loss: 0.2634 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3459Epoch 37/100, Loss: 0.35165083408355713, Val Loss: 0.2626252770423889\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3517 - val_loss: 0.2626 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.3449Epoch 38/100, Loss: 0.34373152256011963, Val Loss: 0.2673039436340332\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3437 - val_loss: 0.2673 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.3499Epoch 39/100, Loss: 0.34686821699142456, Val Loss: 0.25350815057754517\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.2535 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3436Epoch 40/100, Loss: 0.34455013275146484, Val Loss: 0.26668229699134827\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3446 - val_loss: 0.2667 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.3397Epoch 41/100, Loss: 0.33857330679893494, Val Loss: 0.2620329260826111\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3386 - val_loss: 0.2620 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3402Epoch 42/100, Loss: 0.34047284722328186, Val Loss: 0.24665109813213348\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3405 - val_loss: 0.2467 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.3388Epoch 43/100, Loss: 0.3373921513557434, Val Loss: 0.25081220269203186\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3374 - val_loss: 0.2508 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.3378Epoch 44/100, Loss: 0.3356100618839264, Val Loss: 0.2430744618177414\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3356 - val_loss: 0.2431 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.3356Epoch 45/100, Loss: 0.3403787612915039, Val Loss: 0.2549571394920349\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3404 - val_loss: 0.2550 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.3386Epoch 46/100, Loss: 0.33638107776641846, Val Loss: 0.263405442237854\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3364 - val_loss: 0.2634 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3364Epoch 47/100, Loss: 0.3347854018211365, Val Loss: 0.2703361213207245\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3348 - val_loss: 0.2703 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.3286Epoch 48/100, Loss: 0.3262355923652649, Val Loss: 0.24258679151535034\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.2426 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3199Epoch 49/100, Loss: 0.31836220622062683, Val Loss: 0.23624521493911743\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3184 - val_loss: 0.2362 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.3232Epoch 50/100, Loss: 0.3211376965045929, Val Loss: 0.23272733390331268\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3211 - val_loss: 0.2327 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3189Epoch 51/100, Loss: 0.31889599561691284, Val Loss: 0.232558012008667\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3189 - val_loss: 0.2326 - lr: 6.2500e-05\n",
      "Epoch 52/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.3170Epoch 52/100, Loss: 0.31559768319129944, Val Loss: 0.24754707515239716\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3156 - val_loss: 0.2475 - lr: 6.2500e-05\n",
      "Epoch 53/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.3133Epoch 53/100, Loss: 0.3138778507709503, Val Loss: 0.23876596987247467\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3139 - val_loss: 0.2388 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3215Epoch 54/100, Loss: 0.32087570428848267, Val Loss: 0.2351243495941162\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3209 - val_loss: 0.2351 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3172Epoch 55/100, Loss: 0.3186064660549164, Val Loss: 0.23278799653053284\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3186 - val_loss: 0.2328 - lr: 3.1250e-05\n",
      "Epoch 55: early stopping\n",
      "127/127 [==============================] - 0s 740us/step\n",
      "127/127 [==============================] - 0s 692us/step - loss: 0.2801\n",
      "Epoch 1/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 1.3177Epoch 1/100, Loss: 1.3175740242004395, Val Loss: 1.3072537183761597\n",
      "591/591 [==============================] - 4s 4ms/step - loss: 1.3176 - val_loss: 1.3073 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.0485Epoch 2/100, Loss: 1.0463502407073975, Val Loss: 0.8693540692329407\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 1.0464 - val_loss: 0.8694 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.9793Epoch 3/100, Loss: 0.9794203639030457, Val Loss: 0.798653244972229\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.9794 - val_loss: 0.7987 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9129Epoch 4/100, Loss: 0.9133235216140747, Val Loss: 0.6237227916717529\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.9133 - val_loss: 0.6237 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.8475Epoch 5/100, Loss: 0.8446188569068909, Val Loss: 0.7002301216125488\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.8446 - val_loss: 0.7002 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7745Epoch 6/100, Loss: 0.7746354937553406, Val Loss: 0.8446152806282043\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.7746 - val_loss: 0.8446 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.7134Epoch 7/100, Loss: 0.7133710384368896, Val Loss: 0.6349610686302185\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.7134 - val_loss: 0.6350 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.6144Epoch 8/100, Loss: 0.6159847974777222, Val Loss: 0.4776424169540405\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.6160 - val_loss: 0.4776 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.5879Epoch 9/100, Loss: 0.5878706574440002, Val Loss: 0.46766427159309387\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.5879 - val_loss: 0.4677 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5689Epoch 10/100, Loss: 0.5692685842514038, Val Loss: 0.40766188502311707\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.5693 - val_loss: 0.4077 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.5469Epoch 11/100, Loss: 0.5476522445678711, Val Loss: 0.47338706254959106\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.5477 - val_loss: 0.4734 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.5405Epoch 12/100, Loss: 0.5394344329833984, Val Loss: 0.45626458525657654\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.5394 - val_loss: 0.4563 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5279Epoch 13/100, Loss: 0.5281319618225098, Val Loss: 0.4881397485733032\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.5281 - val_loss: 0.4881 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4760Epoch 14/100, Loss: 0.47488874197006226, Val Loss: 0.37274643778800964\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4749 - val_loss: 0.3727 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4672Epoch 15/100, Loss: 0.46718043088912964, Val Loss: 0.3472886085510254\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4672 - val_loss: 0.3473 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4545Epoch 16/100, Loss: 0.4536108076572418, Val Loss: 0.3583495020866394\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4536 - val_loss: 0.3583 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4509Epoch 17/100, Loss: 0.4506179690361023, Val Loss: 0.3322121500968933\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4506 - val_loss: 0.3322 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4472Epoch 18/100, Loss: 0.4453103244304657, Val Loss: 0.363003671169281\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4453 - val_loss: 0.3630 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4382Epoch 19/100, Loss: 0.4381982684135437, Val Loss: 0.31819576025009155\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4382 - val_loss: 0.3182 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4329Epoch 20/100, Loss: 0.43402519822120667, Val Loss: 0.38248583674430847\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4340 - val_loss: 0.3825 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4208Epoch 21/100, Loss: 0.41948747634887695, Val Loss: 0.34562569856643677\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4195 - val_loss: 0.3456 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.4212Epoch 22/100, Loss: 0.4211321771144867, Val Loss: 0.3384495973587036\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.4211 - val_loss: 0.3384 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3944Epoch 23/100, Loss: 0.39452746510505676, Val Loss: 0.3052629232406616\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3945 - val_loss: 0.3053 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3908Epoch 24/100, Loss: 0.3898903429508209, Val Loss: 0.3074585199356079\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3899 - val_loss: 0.3075 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3917Epoch 25/100, Loss: 0.3904208838939667, Val Loss: 0.33108413219451904\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3904 - val_loss: 0.3311 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3821Epoch 26/100, Loss: 0.3819965720176697, Val Loss: 0.27611789107322693\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3820 - val_loss: 0.2761 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3766Epoch 27/100, Loss: 0.3763587474822998, Val Loss: 0.27381598949432373\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3764 - val_loss: 0.2738 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3741Epoch 28/100, Loss: 0.3763507902622223, Val Loss: 0.28370481729507446\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3764 - val_loss: 0.2837 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.3773Epoch 29/100, Loss: 0.37762778997421265, Val Loss: 0.2898383140563965\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3776 - val_loss: 0.2898 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3678Epoch 30/100, Loss: 0.3675142526626587, Val Loss: 0.3045618236064911\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3675 - val_loss: 0.3046 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3559Epoch 31/100, Loss: 0.35460832715034485, Val Loss: 0.26626867055892944\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3546 - val_loss: 0.2663 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.3600Epoch 32/100, Loss: 0.36138489842414856, Val Loss: 0.27489393949508667\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3614 - val_loss: 0.2749 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3504Epoch 33/100, Loss: 0.35012370347976685, Val Loss: 0.277762234210968\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3501 - val_loss: 0.2778 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3495Epoch 34/100, Loss: 0.34875261783599854, Val Loss: 0.3021058440208435\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3488 - val_loss: 0.3021 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3414Epoch 35/100, Loss: 0.3410751521587372, Val Loss: 0.2738201320171356\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3411 - val_loss: 0.2738 - lr: 3.1250e-05\n",
      "Epoch 36/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3464Epoch 36/100, Loss: 0.3457892835140228, Val Loss: 0.2666398882865906\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "591/591 [==============================] - 2s 4ms/step - loss: 0.3458 - val_loss: 0.2666 - lr: 3.1250e-05\n",
      "Epoch 36: early stopping\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.3110\n",
      "Epoch 1/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 2.2823Epoch 1/100, Loss: 2.2316572666168213, Val Loss: 1.3928849697113037\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 2.2317 - val_loss: 1.3929 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 1.1912Epoch 2/100, Loss: 1.186509609222412, Val Loss: 0.9608493447303772\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.1865 - val_loss: 0.9608 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 1.0940Epoch 3/100, Loss: 1.0908585786819458, Val Loss: 0.8867914080619812\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0909 - val_loss: 0.8868 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 1.0374Epoch 4/100, Loss: 1.038900375366211, Val Loss: 0.793463945388794\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0389 - val_loss: 0.7935 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 1.0128Epoch 5/100, Loss: 1.0104756355285645, Val Loss: 0.7877347469329834\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0105 - val_loss: 0.7877 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.9947Epoch 6/100, Loss: 0.9968523979187012, Val Loss: 0.7566321492195129\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9969 - val_loss: 0.7566 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.9756Epoch 7/100, Loss: 0.9813522100448608, Val Loss: 0.7049053907394409\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9814 - val_loss: 0.7049 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.9616Epoch 8/100, Loss: 0.9624423384666443, Val Loss: 0.6725998520851135\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9624 - val_loss: 0.6726 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.9227Epoch 9/100, Loss: 0.9292526245117188, Val Loss: 0.6219270825386047\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9293 - val_loss: 0.6219 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.9174Epoch 10/100, Loss: 0.9176024198532104, Val Loss: 0.6390719413757324\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9176 - val_loss: 0.6391 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.9020Epoch 11/100, Loss: 0.8986156582832336, Val Loss: 0.6489644646644592\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8986 - val_loss: 0.6490 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.8824Epoch 12/100, Loss: 0.8806716799736023, Val Loss: 0.6321171522140503\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8807 - val_loss: 0.6321 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.8545Epoch 13/100, Loss: 0.8557091355323792, Val Loss: 0.6000862717628479\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8557 - val_loss: 0.6001 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.8498Epoch 14/100, Loss: 0.8495254516601562, Val Loss: 0.6295799016952515\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8495 - val_loss: 0.6296 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8238Epoch 15/100, Loss: 0.8277976512908936, Val Loss: 0.5832873582839966\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8278 - val_loss: 0.5833 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.8283Epoch 16/100, Loss: 0.8295504450798035, Val Loss: 0.5840156078338623\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8296 - val_loss: 0.5840 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.8268Epoch 17/100, Loss: 0.8225244879722595, Val Loss: 0.5487255454063416\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8225 - val_loss: 0.5487 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.8199Epoch 18/100, Loss: 0.8156634569168091, Val Loss: 0.554266095161438\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8157 - val_loss: 0.5543 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.8013Epoch 19/100, Loss: 0.8028967976570129, Val Loss: 0.5677763819694519\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8029 - val_loss: 0.5678 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.8079Epoch 20/100, Loss: 0.8123182654380798, Val Loss: 0.5428123474121094\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8123 - val_loss: 0.5428 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7947Epoch 21/100, Loss: 0.7923288941383362, Val Loss: 0.5445005893707275\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7923 - val_loss: 0.5445 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.7830Epoch 22/100, Loss: 0.7851302623748779, Val Loss: 0.5626521110534668\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7851 - val_loss: 0.5627 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.7883Epoch 23/100, Loss: 0.7867221832275391, Val Loss: 0.5521165132522583\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7867 - val_loss: 0.5521 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.7924Epoch 24/100, Loss: 0.7910463809967041, Val Loss: 0.5114145278930664\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7910 - val_loss: 0.5114 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7829Epoch 25/100, Loss: 0.7815576195716858, Val Loss: 0.5431682467460632\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7816 - val_loss: 0.5432 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7783Epoch 26/100, Loss: 0.7739915251731873, Val Loss: 0.557904839515686\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7740 - val_loss: 0.5579 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.7658Epoch 27/100, Loss: 0.7642011046409607, Val Loss: 0.53759765625\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7642 - val_loss: 0.5376 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.7579Epoch 28/100, Loss: 0.7587307691574097, Val Loss: 0.5008605718612671\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7587 - val_loss: 0.5009 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.7637Epoch 29/100, Loss: 0.7612085342407227, Val Loss: 0.5090247988700867\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7612 - val_loss: 0.5090 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.7679Epoch 30/100, Loss: 0.7695533037185669, Val Loss: 0.4963257908821106\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7696 - val_loss: 0.4963 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7592Epoch 31/100, Loss: 0.7578528523445129, Val Loss: 0.4953351318836212\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7579 - val_loss: 0.4953 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.7758Epoch 32/100, Loss: 0.7755659222602844, Val Loss: 0.4923357665538788\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7756 - val_loss: 0.4923 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.7588Epoch 33/100, Loss: 0.7568301558494568, Val Loss: 0.4965028166770935\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7568 - val_loss: 0.4965 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.7530Epoch 34/100, Loss: 0.7521951794624329, Val Loss: 0.4985438287258148\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7522 - val_loss: 0.4985 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7520Epoch 35/100, Loss: 0.7529402375221252, Val Loss: 0.49904873967170715\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7529 - val_loss: 0.4990 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.7552Epoch 36/100, Loss: 0.7535433173179626, Val Loss: 0.4868985414505005\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7535 - val_loss: 0.4869 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7546Epoch 37/100, Loss: 0.7605490684509277, Val Loss: 0.4973834753036499\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7605 - val_loss: 0.4974 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.7456Epoch 38/100, Loss: 0.7438725233078003, Val Loss: 0.4882366955280304\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7439 - val_loss: 0.4882 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7587Epoch 39/100, Loss: 0.7549337148666382, Val Loss: 0.5079289674758911\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7549 - val_loss: 0.5079 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "556/591 [===========================>..] - ETA: 0s - loss: 0.7493Epoch 40/100, Loss: 0.7510164380073547, Val Loss: 0.49083206057548523\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7510 - val_loss: 0.4908 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.7393Epoch 41/100, Loss: 0.7407344579696655, Val Loss: 0.49149441719055176\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7407 - val_loss: 0.4915 - lr: 3.1250e-05\n",
      "Epoch 41: early stopping\n",
      "127/127 [==============================] - 0s 534us/step\n",
      "127/127 [==============================] - 0s 542us/step - loss: 0.5324\n",
      "Epoch 1/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 1.8674Epoch 1/100, Loss: 1.850113034248352, Val Loss: 1.3014427423477173\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.8501 - val_loss: 1.3014 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 1.1370Epoch 2/100, Loss: 1.1370056867599487, Val Loss: 0.8694353103637695\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.1370 - val_loss: 0.8694 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 1.0539Epoch 3/100, Loss: 1.0537610054016113, Val Loss: 0.8316348195075989\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0538 - val_loss: 0.8316 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.9878Epoch 4/100, Loss: 0.9890183806419373, Val Loss: 0.7238193154335022\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9890 - val_loss: 0.7238 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.9510Epoch 5/100, Loss: 0.9470491409301758, Val Loss: 0.8037835955619812\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9470 - val_loss: 0.8038 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.9135Epoch 6/100, Loss: 0.9139291644096375, Val Loss: 0.8969717025756836\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9139 - val_loss: 0.8970 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.8922Epoch 7/100, Loss: 0.8975062966346741, Val Loss: 0.7661639451980591\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8975 - val_loss: 0.7662 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.8387Epoch 8/100, Loss: 0.8390994071960449, Val Loss: 0.5932033658027649\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8391 - val_loss: 0.5932 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.8102Epoch 9/100, Loss: 0.8150642514228821, Val Loss: 0.6130141615867615\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8151 - val_loss: 0.6130 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.7934Epoch 10/100, Loss: 0.793610155582428, Val Loss: 0.5967898964881897\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7936 - val_loss: 0.5968 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7800Epoch 11/100, Loss: 0.7805385589599609, Val Loss: 0.6038934588432312\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7805 - val_loss: 0.6039 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.7643Epoch 12/100, Loss: 0.7638036608695984, Val Loss: 0.5545312762260437\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7638 - val_loss: 0.5545 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7576Epoch 13/100, Loss: 0.7578445672988892, Val Loss: 0.5207003355026245\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7578 - val_loss: 0.5207 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.7518Epoch 14/100, Loss: 0.7501203417778015, Val Loss: 0.5345824956893921\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7501 - val_loss: 0.5346 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.7381Epoch 15/100, Loss: 0.7398926019668579, Val Loss: 0.49795734882354736\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7399 - val_loss: 0.4980 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "542/591 [==========================>...] - ETA: 0s - loss: 0.7299Epoch 16/100, Loss: 0.729360818862915, Val Loss: 0.5151097774505615\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7294 - val_loss: 0.5151 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7251Epoch 17/100, Loss: 0.7251490950584412, Val Loss: 0.4911319315433502\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7251 - val_loss: 0.4911 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.7225Epoch 18/100, Loss: 0.7184227705001831, Val Loss: 0.5047693252563477\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7184 - val_loss: 0.5048 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7042Epoch 19/100, Loss: 0.7042712569236755, Val Loss: 0.47735705971717834\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7043 - val_loss: 0.4774 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.7064Epoch 20/100, Loss: 0.710922122001648, Val Loss: 0.47099918127059937\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7109 - val_loss: 0.4710 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.6967Epoch 21/100, Loss: 0.6948469877243042, Val Loss: 0.49375465512275696\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6948 - val_loss: 0.4938 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.6869Epoch 22/100, Loss: 0.6868101358413696, Val Loss: 0.4807933568954468\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6868 - val_loss: 0.4808 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.6801Epoch 23/100, Loss: 0.680178165435791, Val Loss: 0.46657830476760864\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6802 - val_loss: 0.4666 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.6942Epoch 24/100, Loss: 0.6938611268997192, Val Loss: 0.46873676776885986\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6939 - val_loss: 0.4687 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.6823Epoch 25/100, Loss: 0.6835426092147827, Val Loss: 0.47407805919647217\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6835 - val_loss: 0.4741 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.6816Epoch 26/100, Loss: 0.678022563457489, Val Loss: 0.4412806034088135\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6780 - val_loss: 0.4413 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6697Epoch 27/100, Loss: 0.6689398884773254, Val Loss: 0.47978872060775757\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6689 - val_loss: 0.4798 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.6634Epoch 28/100, Loss: 0.6609094142913818, Val Loss: 0.4586566984653473\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6609 - val_loss: 0.4587 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.6648Epoch 29/100, Loss: 0.6662910580635071, Val Loss: 0.4429169297218323\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6663 - val_loss: 0.4429 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.6573Epoch 30/100, Loss: 0.6578444838523865, Val Loss: 0.44894203543663025\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6578 - val_loss: 0.4489 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.6455Epoch 31/100, Loss: 0.645451545715332, Val Loss: 0.43895891308784485\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6455 - val_loss: 0.4390 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.6549Epoch 32/100, Loss: 0.6562803983688354, Val Loss: 0.4298940598964691\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6563 - val_loss: 0.4299 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "540/591 [==========================>...] - ETA: 0s - loss: 0.6394Epoch 33/100, Loss: 0.6386191248893738, Val Loss: 0.4278114140033722\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6386 - val_loss: 0.4278 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "541/591 [==========================>...] - ETA: 0s - loss: 0.6412Epoch 34/100, Loss: 0.6401812434196472, Val Loss: 0.4362594485282898\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6402 - val_loss: 0.4363 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.6358Epoch 35/100, Loss: 0.6382134556770325, Val Loss: 0.4300844669342041\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6382 - val_loss: 0.4301 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.6428Epoch 36/100, Loss: 0.6428465247154236, Val Loss: 0.42695507407188416\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6428 - val_loss: 0.4270 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.6429Epoch 37/100, Loss: 0.6429717540740967, Val Loss: 0.4380413591861725\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6430 - val_loss: 0.4380 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.6290Epoch 38/100, Loss: 0.6275976300239563, Val Loss: 0.4556221067905426\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6276 - val_loss: 0.4556 - lr: 1.2500e-04\n",
      "Epoch 38: early stopping\n",
      "127/127 [==============================] - 0s 550us/step\n",
      "127/127 [==============================] - 0s 570us/step - loss: 0.4734\n",
      "Epoch 1/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 1.6386Epoch 1/100, Loss: 1.617735505104065, Val Loss: 1.2449254989624023\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.6177 - val_loss: 1.2449 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 1.0391Epoch 2/100, Loss: 1.0394842624664307, Val Loss: 0.751184344291687\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0395 - val_loss: 0.7512 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.9790Epoch 3/100, Loss: 0.9807348251342773, Val Loss: 0.7484183311462402\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9807 - val_loss: 0.7484 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.9249Epoch 4/100, Loss: 0.9257878661155701, Val Loss: 0.6570402979850769\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9258 - val_loss: 0.6570 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.8846Epoch 5/100, Loss: 0.8801098465919495, Val Loss: 0.763454794883728\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8801 - val_loss: 0.7635 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.8422Epoch 6/100, Loss: 0.8428541421890259, Val Loss: 0.7224530577659607\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8429 - val_loss: 0.7225 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.7976Epoch 7/100, Loss: 0.7995879054069519, Val Loss: 0.6002413630485535\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7996 - val_loss: 0.6002 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7684Epoch 8/100, Loss: 0.769277811050415, Val Loss: 0.5257584452629089\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7693 - val_loss: 0.5258 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.7315Epoch 9/100, Loss: 0.7353715300559998, Val Loss: 0.6977438926696777\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7354 - val_loss: 0.6977 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.7143Epoch 10/100, Loss: 0.7142541408538818, Val Loss: 0.626386284828186\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7143 - val_loss: 0.6264 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6887Epoch 11/100, Loss: 0.6888834238052368, Val Loss: 0.5271831750869751\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6889 - val_loss: 0.5272 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.6522Epoch 12/100, Loss: 0.6522045731544495, Val Loss: 0.45890012383461\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6522 - val_loss: 0.4589 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6384Epoch 13/100, Loss: 0.6404975056648254, Val Loss: 0.47153353691101074\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6405 - val_loss: 0.4715 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.6233Epoch 14/100, Loss: 0.6231491565704346, Val Loss: 0.45338091254234314\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6231 - val_loss: 0.4534 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.6146Epoch 15/100, Loss: 0.616248369216919, Val Loss: 0.4459867775440216\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6162 - val_loss: 0.4460 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.6053Epoch 16/100, Loss: 0.605061411857605, Val Loss: 0.42096757888793945\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6051 - val_loss: 0.4210 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.6010Epoch 17/100, Loss: 0.6008982062339783, Val Loss: 0.4480345845222473\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6009 - val_loss: 0.4480 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.5899Epoch 18/100, Loss: 0.5886923670768738, Val Loss: 0.39895567297935486\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5887 - val_loss: 0.3990 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5746Epoch 19/100, Loss: 0.5757686495780945, Val Loss: 0.4334692656993866\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5758 - val_loss: 0.4335 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5740Epoch 20/100, Loss: 0.5752348303794861, Val Loss: 0.49601778388023376\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5752 - val_loss: 0.4960 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5655Epoch 21/100, Loss: 0.5653521418571472, Val Loss: 0.46833479404449463\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5654 - val_loss: 0.4683 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5327Epoch 22/100, Loss: 0.5324928760528564, Val Loss: 0.3919021189212799\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5325 - val_loss: 0.3919 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5299Epoch 23/100, Loss: 0.5303440690040588, Val Loss: 0.35200902819633484\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5303 - val_loss: 0.3520 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5365Epoch 24/100, Loss: 0.535613477230072, Val Loss: 0.4021531343460083\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5356 - val_loss: 0.4022 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5301Epoch 25/100, Loss: 0.5295396447181702, Val Loss: 0.37629732489585876\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5295 - val_loss: 0.3763 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5241Epoch 26/100, Loss: 0.5242345333099365, Val Loss: 0.3685416579246521\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5242 - val_loss: 0.3685 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.5069Epoch 27/100, Loss: 0.5039677619934082, Val Loss: 0.35153117775917053\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5040 - val_loss: 0.3515 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5067Epoch 28/100, Loss: 0.5065626502037048, Val Loss: 0.34345898032188416\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5066 - val_loss: 0.3435 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5032Epoch 29/100, Loss: 0.5033554434776306, Val Loss: 0.33359023928642273\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5034 - val_loss: 0.3336 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.5010Epoch 30/100, Loss: 0.4991670250892639, Val Loss: 0.3284725844860077\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4992 - val_loss: 0.3285 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4998Epoch 31/100, Loss: 0.4971255362033844, Val Loss: 0.33570587635040283\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4971 - val_loss: 0.3357 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5012Epoch 32/100, Loss: 0.5023056268692017, Val Loss: 0.3336144983768463\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5023 - val_loss: 0.3336 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4930Epoch 33/100, Loss: 0.49074333906173706, Val Loss: 0.3353078365325928\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4907 - val_loss: 0.3353 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.4866Epoch 34/100, Loss: 0.4851287007331848, Val Loss: 0.31347373127937317\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4851 - val_loss: 0.3135 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4806Epoch 35/100, Loss: 0.48107898235321045, Val Loss: 0.3314985930919647\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4811 - val_loss: 0.3315 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4882Epoch 36/100, Loss: 0.4883366823196411, Val Loss: 0.31542932987213135\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 0.3154 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4831Epoch 37/100, Loss: 0.48906388878822327, Val Loss: 0.3192589581012726\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4891 - val_loss: 0.3193 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4763Epoch 38/100, Loss: 0.47667038440704346, Val Loss: 0.31661731004714966\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4767 - val_loss: 0.3166 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.4853Epoch 39/100, Loss: 0.4814551770687103, Val Loss: 0.3073835074901581\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4815 - val_loss: 0.3074 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4749Epoch 40/100, Loss: 0.47690683603286743, Val Loss: 0.3125556409358978\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4769 - val_loss: 0.3126 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.4704Epoch 41/100, Loss: 0.47041749954223633, Val Loss: 0.30530691146850586\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4704 - val_loss: 0.3053 - lr: 3.1250e-05\n",
      "Epoch 42/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 0.4792Epoch 42/100, Loss: 0.47607043385505676, Val Loss: 0.3078876733779907\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4761 - val_loss: 0.3079 - lr: 3.1250e-05\n",
      "Epoch 43/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4763Epoch 43/100, Loss: 0.4754098057746887, Val Loss: 0.3039436340332031\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4754 - val_loss: 0.3039 - lr: 3.1250e-05\n",
      "Epoch 44/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4744Epoch 44/100, Loss: 0.47052526473999023, Val Loss: 0.3071075975894928\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4705 - val_loss: 0.3071 - lr: 3.1250e-05\n",
      "Epoch 45/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4738Epoch 45/100, Loss: 0.4787943363189697, Val Loss: 0.3081813454627991\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4788 - val_loss: 0.3082 - lr: 3.1250e-05\n",
      "Epoch 46/100\n",
      "545/591 [==========================>...] - ETA: 0s - loss: 0.4762Epoch 46/100, Loss: 0.47625792026519775, Val Loss: 0.3072020411491394\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4763 - val_loss: 0.3072 - lr: 3.1250e-05\n",
      "Epoch 47/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4740Epoch 47/100, Loss: 0.47400566935539246, Val Loss: 0.30503779649734497\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4740 - val_loss: 0.3050 - lr: 1.5625e-05\n",
      "Epoch 48/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.4749Epoch 48/100, Loss: 0.4725738763809204, Val Loss: 0.30415380001068115\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4726 - val_loss: 0.3042 - lr: 1.5625e-05\n",
      "Epoch 48: early stopping\n",
      "127/127 [==============================] - 0s 605us/step\n",
      "127/127 [==============================] - 0s 542us/step - loss: 0.3480\n",
      "Epoch 1/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 1.4453Epoch 1/100, Loss: 1.4447797536849976, Val Loss: 0.9988841414451599\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.4448 - val_loss: 0.9989 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 1.0267Epoch 2/100, Loss: 1.0271130800247192, Val Loss: 0.8407873511314392\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0271 - val_loss: 0.8408 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.9609Epoch 3/100, Loss: 0.960921049118042, Val Loss: 0.8108183145523071\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9609 - val_loss: 0.8108 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.9050Epoch 4/100, Loss: 0.9045831561088562, Val Loss: 0.6250647902488708\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9046 - val_loss: 0.6251 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.8390Epoch 5/100, Loss: 0.8383826017379761, Val Loss: 0.570232629776001\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8384 - val_loss: 0.5702 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.7797Epoch 6/100, Loss: 0.7801676988601685, Val Loss: 0.5942583680152893\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7802 - val_loss: 0.5943 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.7396Epoch 7/100, Loss: 0.742081344127655, Val Loss: 0.5172988772392273\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7421 - val_loss: 0.5173 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.6879Epoch 8/100, Loss: 0.6884584426879883, Val Loss: 0.5173228979110718\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6885 - val_loss: 0.5173 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.6593Epoch 9/100, Loss: 0.6596775054931641, Val Loss: 0.528598427772522\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6597 - val_loss: 0.5286 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.6334Epoch 10/100, Loss: 0.6330631375312805, Val Loss: 0.48045724630355835\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6331 - val_loss: 0.4805 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6096Epoch 11/100, Loss: 0.6097899675369263, Val Loss: 0.5142914652824402\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6098 - val_loss: 0.5143 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5934Epoch 12/100, Loss: 0.5926631689071655, Val Loss: 0.471571147441864\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5927 - val_loss: 0.4716 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5931Epoch 13/100, Loss: 0.5952581167221069, Val Loss: 0.4684935212135315\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5953 - val_loss: 0.4685 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.5637Epoch 14/100, Loss: 0.5637827515602112, Val Loss: 0.4742996096611023\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5638 - val_loss: 0.4743 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5621Epoch 15/100, Loss: 0.5624524354934692, Val Loss: 0.5302310585975647\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5625 - val_loss: 0.5302 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5552Epoch 16/100, Loss: 0.554521381855011, Val Loss: 0.46942952275276184\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5545 - val_loss: 0.4694 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.5040Epoch 17/100, Loss: 0.5002237558364868, Val Loss: 0.36075785756111145\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5002 - val_loss: 0.3608 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4832Epoch 18/100, Loss: 0.48317423462867737, Val Loss: 0.36083516478538513\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4832 - val_loss: 0.3608 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4847Epoch 19/100, Loss: 0.4844854176044464, Val Loss: 0.3496248722076416\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4845 - val_loss: 0.3496 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4785Epoch 20/100, Loss: 0.4794550836086273, Val Loss: 0.3874736726284027\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.3875 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4611Epoch 21/100, Loss: 0.4610794484615326, Val Loss: 0.34466391801834106\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4611 - val_loss: 0.3447 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4569Epoch 22/100, Loss: 0.4565407931804657, Val Loss: 0.35600653290748596\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4565 - val_loss: 0.3560 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4484Epoch 23/100, Loss: 0.44841858744621277, Val Loss: 0.3450859487056732\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4484 - val_loss: 0.3451 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4535Epoch 24/100, Loss: 0.45247095823287964, Val Loss: 0.31053853034973145\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4525 - val_loss: 0.3105 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4493Epoch 25/100, Loss: 0.44775328040122986, Val Loss: 0.3175380825996399\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4478 - val_loss: 0.3175 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.4435Epoch 26/100, Loss: 0.4412388801574707, Val Loss: 0.2980511486530304\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4412 - val_loss: 0.2981 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4321Epoch 27/100, Loss: 0.432064414024353, Val Loss: 0.3541094958782196\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4321 - val_loss: 0.3541 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4375Epoch 28/100, Loss: 0.4396229088306427, Val Loss: 0.31520694494247437\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4396 - val_loss: 0.3152 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4297Epoch 29/100, Loss: 0.4299057126045227, Val Loss: 0.34980639815330505\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4299 - val_loss: 0.3498 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.4069Epoch 30/100, Loss: 0.40563488006591797, Val Loss: 0.2882286310195923\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4056 - val_loss: 0.2882 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3991Epoch 31/100, Loss: 0.3992559313774109, Val Loss: 0.30698591470718384\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3993 - val_loss: 0.3070 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4015Epoch 32/100, Loss: 0.40149563550949097, Val Loss: 0.27941057085990906\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4015 - val_loss: 0.2794 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3984Epoch 33/100, Loss: 0.39747872948646545, Val Loss: 0.30087512731552124\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3975 - val_loss: 0.3009 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3908Epoch 34/100, Loss: 0.39064130187034607, Val Loss: 0.36440980434417725\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3906 - val_loss: 0.3644 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.3858Epoch 35/100, Loss: 0.3849645256996155, Val Loss: 0.2898934483528137\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3850 - val_loss: 0.2899 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.3796Epoch 36/100, Loss: 0.3783951699733734, Val Loss: 0.2498990148305893\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3784 - val_loss: 0.2499 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3673Epoch 37/100, Loss: 0.37334680557250977, Val Loss: 0.27865055203437805\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3733 - val_loss: 0.2787 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3734Epoch 38/100, Loss: 0.3734366297721863, Val Loss: 0.2634146511554718\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3734 - val_loss: 0.2634 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3718Epoch 39/100, Loss: 0.37193673849105835, Val Loss: 0.26076367497444153\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3719 - val_loss: 0.2608 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.3609Epoch 40/100, Loss: 0.3624463379383087, Val Loss: 0.2455386221408844\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3624 - val_loss: 0.2455 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3548Epoch 41/100, Loss: 0.35424911975860596, Val Loss: 0.24570435285568237\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3542 - val_loss: 0.2457 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3555Epoch 42/100, Loss: 0.35597026348114014, Val Loss: 0.2566448152065277\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3560 - val_loss: 0.2566 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.3578Epoch 43/100, Loss: 0.3570516109466553, Val Loss: 0.24698814749717712\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3571 - val_loss: 0.2470 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3528Epoch 44/100, Loss: 0.3528139591217041, Val Loss: 0.24450793862342834\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3528 - val_loss: 0.2445 - lr: 3.1250e-05\n",
      "Epoch 45/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3516Epoch 45/100, Loss: 0.356060266494751, Val Loss: 0.2420300543308258\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3561 - val_loss: 0.2420 - lr: 3.1250e-05\n",
      "Epoch 46/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3553Epoch 46/100, Loss: 0.35533446073532104, Val Loss: 0.25008177757263184\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3553 - val_loss: 0.2501 - lr: 3.1250e-05\n",
      "Epoch 47/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.3573Epoch 47/100, Loss: 0.35126227140426636, Val Loss: 0.24236224591732025\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3513 - val_loss: 0.2424 - lr: 3.1250e-05\n",
      "Epoch 48/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.3526Epoch 48/100, Loss: 0.3510223925113678, Val Loss: 0.24038484692573547\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3510 - val_loss: 0.2404 - lr: 3.1250e-05\n",
      "Epoch 49/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.3483Epoch 49/100, Loss: 0.3473961055278778, Val Loss: 0.24418866634368896\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3474 - val_loss: 0.2442 - lr: 3.1250e-05\n",
      "Epoch 50/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.3518Epoch 50/100, Loss: 0.3495500087738037, Val Loss: 0.24760942161083221\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3496 - val_loss: 0.2476 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3462Epoch 51/100, Loss: 0.34604838490486145, Val Loss: 0.2430976778268814\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3460 - val_loss: 0.2431 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3445Epoch 52/100, Loss: 0.3434576392173767, Val Loss: 0.23955507576465607\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3435 - val_loss: 0.2396 - lr: 1.5625e-05\n",
      "Epoch 53/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.3432Epoch 53/100, Loss: 0.3435911238193512, Val Loss: 0.23832523822784424\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3436 - val_loss: 0.2383 - lr: 1.5625e-05\n",
      "Epoch 54/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3489Epoch 54/100, Loss: 0.34888046979904175, Val Loss: 0.24287143349647522\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3489 - val_loss: 0.2429 - lr: 1.5625e-05\n",
      "Epoch 55/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3489Epoch 55/100, Loss: 0.34887567162513733, Val Loss: 0.2479705661535263\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3489 - val_loss: 0.2480 - lr: 1.5625e-05\n",
      "Epoch 56/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3436Epoch 56/100, Loss: 0.34366530179977417, Val Loss: 0.24833214282989502\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3437 - val_loss: 0.2483 - lr: 1.5625e-05\n",
      "Epoch 57/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.3504Epoch 57/100, Loss: 0.3482777178287506, Val Loss: 0.24189390242099762\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3483 - val_loss: 0.2419 - lr: 7.8125e-06\n",
      "Epoch 58/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3487Epoch 58/100, Loss: 0.34929195046424866, Val Loss: 0.24813802540302277\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.3493 - val_loss: 0.2481 - lr: 7.8125e-06\n",
      "Epoch 58: early stopping\n",
      "127/127 [==============================] - 0s 653us/step\n",
      "127/127 [==============================] - 0s 605us/step - loss: 0.2851\n",
      "Epoch 1/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 1.3545Epoch 1/100, Loss: 1.3523623943328857, Val Loss: 1.0873080492019653\n",
      "591/591 [==============================] - 3s 2ms/step - loss: 1.3524 - val_loss: 1.0873 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 1.0305Epoch 2/100, Loss: 1.0270963907241821, Val Loss: 0.7730250358581543\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0271 - val_loss: 0.7730 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.9697Epoch 3/100, Loss: 0.9697061777114868, Val Loss: 0.8443579077720642\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9697 - val_loss: 0.8444 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 0.9009Epoch 4/100, Loss: 0.9016332030296326, Val Loss: 0.6428224444389343\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9016 - val_loss: 0.6428 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.8313Epoch 5/100, Loss: 0.826744019985199, Val Loss: 0.6983901262283325\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8267 - val_loss: 0.6984 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7638Epoch 6/100, Loss: 0.7633200287818909, Val Loss: 0.6851164698600769\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7633 - val_loss: 0.6851 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7159Epoch 7/100, Loss: 0.7182965278625488, Val Loss: 0.5772671699523926\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7183 - val_loss: 0.5773 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.6795Epoch 8/100, Loss: 0.680531919002533, Val Loss: 0.5251845121383667\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6805 - val_loss: 0.5252 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.6379Epoch 9/100, Loss: 0.6429151892662048, Val Loss: 0.5091538429260254\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6429 - val_loss: 0.5092 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.6225Epoch 10/100, Loss: 0.6234142780303955, Val Loss: 0.4209766983985901\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6234 - val_loss: 0.4210 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6017Epoch 11/100, Loss: 0.6021068692207336, Val Loss: 0.44662249088287354\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6021 - val_loss: 0.4466 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5916Epoch 12/100, Loss: 0.5913405418395996, Val Loss: 0.5116698741912842\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5913 - val_loss: 0.5117 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5832Epoch 13/100, Loss: 0.5847561955451965, Val Loss: 0.5391078591346741\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5848 - val_loss: 0.5391 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5181Epoch 14/100, Loss: 0.5167939066886902, Val Loss: 0.4919643700122833\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5168 - val_loss: 0.4920 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4962Epoch 15/100, Loss: 0.4997856616973877, Val Loss: 0.39040374755859375\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4998 - val_loss: 0.3904 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.4892Epoch 16/100, Loss: 0.4894820749759674, Val Loss: 0.47514644265174866\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4895 - val_loss: 0.4751 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4809Epoch 17/100, Loss: 0.48097696900367737, Val Loss: 0.3867644965648651\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4810 - val_loss: 0.3868 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4726Epoch 18/100, Loss: 0.47223523259162903, Val Loss: 0.4047276973724365\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4722 - val_loss: 0.4047 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.4677Epoch 19/100, Loss: 0.4681398868560791, Val Loss: 0.35548269748687744\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4681 - val_loss: 0.3555 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4664Epoch 20/100, Loss: 0.46769246459007263, Val Loss: 0.42598608136177063\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4677 - val_loss: 0.4260 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.4642Epoch 21/100, Loss: 0.460528165102005, Val Loss: 0.33182865381240845\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4605 - val_loss: 0.3318 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4473Epoch 22/100, Loss: 0.44732943177223206, Val Loss: 0.5893303751945496\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4473 - val_loss: 0.5893 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.4343Epoch 23/100, Loss: 0.4337674081325531, Val Loss: 0.3662329614162445\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4338 - val_loss: 0.3662 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4407Epoch 24/100, Loss: 0.4395543336868286, Val Loss: 0.6721524000167847\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4396 - val_loss: 0.6722 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4118Epoch 25/100, Loss: 0.41171005368232727, Val Loss: 0.3798965811729431\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4117 - val_loss: 0.3799 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.3958Epoch 26/100, Loss: 0.39599430561065674, Val Loss: 0.330539345741272\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3960 - val_loss: 0.3305 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3856Epoch 27/100, Loss: 0.3854704201221466, Val Loss: 0.32484719157218933\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.3248 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3891Epoch 28/100, Loss: 0.38902100920677185, Val Loss: 0.3072111904621124\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3890 - val_loss: 0.3072 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3887Epoch 29/100, Loss: 0.3881717324256897, Val Loss: 0.5519379377365112\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3882 - val_loss: 0.5519 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3777Epoch 30/100, Loss: 0.376751571893692, Val Loss: 0.303546667098999\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3768 - val_loss: 0.3035 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.3822Epoch 31/100, Loss: 0.37972506880760193, Val Loss: 0.323499470949173\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3797 - val_loss: 0.3235 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3797Epoch 32/100, Loss: 0.38057321310043335, Val Loss: 0.30133575201034546\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3806 - val_loss: 0.3013 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3699Epoch 33/100, Loss: 0.3687247633934021, Val Loss: 0.2773892879486084\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3687 - val_loss: 0.2774 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3661Epoch 34/100, Loss: 0.3654223382472992, Val Loss: 0.35021892189979553\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3654 - val_loss: 0.3502 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3651Epoch 35/100, Loss: 0.3642544448375702, Val Loss: 0.290958046913147\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3643 - val_loss: 0.2910 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3655Epoch 36/100, Loss: 0.36516502499580383, Val Loss: 0.28226059675216675\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3652 - val_loss: 0.2823 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3431Epoch 37/100, Loss: 0.34901130199432373, Val Loss: 0.28118446469306946\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3490 - val_loss: 0.2812 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3405Epoch 38/100, Loss: 0.3406522870063782, Val Loss: 0.2694774270057678\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3407 - val_loss: 0.2695 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.3448Epoch 39/100, Loss: 0.34324967861175537, Val Loss: 0.23824656009674072\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3432 - val_loss: 0.2382 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3375Epoch 40/100, Loss: 0.3373061716556549, Val Loss: 0.2676897346973419\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.2677 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3309Epoch 41/100, Loss: 0.33006471395492554, Val Loss: 0.309073269367218\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3301 - val_loss: 0.3091 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3328Epoch 42/100, Loss: 0.3328360915184021, Val Loss: 0.2653331756591797\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.2653 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3220Epoch 43/100, Loss: 0.3210402727127075, Val Loss: 0.24410618841648102\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3210 - val_loss: 0.2441 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.3239Epoch 44/100, Loss: 0.3216612935066223, Val Loss: 0.23749777674674988\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3217 - val_loss: 0.2375 - lr: 6.2500e-05\n",
      "Epoch 44: early stopping\n",
      "127/127 [==============================] - 0s 961us/step\n",
      "127/127 [==============================] - 0s 748us/step - loss: 0.2858\n",
      "Epoch 1/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 1.3009Epoch 1/100, Loss: 1.3008631467819214, Val Loss: 1.2249226570129395\n",
      "591/591 [==============================] - 4s 5ms/step - loss: 1.3009 - val_loss: 1.2249 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.0487Epoch 2/100, Loss: 1.0484071969985962, Val Loss: 0.789374053478241\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 1.0484 - val_loss: 0.7894 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 1.0110Epoch 3/100, Loss: 1.0117206573486328, Val Loss: 0.8276031017303467\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 1.0117 - val_loss: 0.8276 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.9442Epoch 4/100, Loss: 0.943792998790741, Val Loss: 0.6248102784156799\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.9438 - val_loss: 0.6248 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.8796Epoch 5/100, Loss: 0.8795831203460693, Val Loss: 0.69593346118927\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.8796 - val_loss: 0.6959 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.8162Epoch 6/100, Loss: 0.8168917298316956, Val Loss: 0.8033294081687927\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.8169 - val_loss: 0.8033 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.7520Epoch 7/100, Loss: 0.754902720451355, Val Loss: 0.587297260761261\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.7549 - val_loss: 0.5873 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7182Epoch 8/100, Loss: 0.7181437611579895, Val Loss: 0.5550819039344788\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.7181 - val_loss: 0.5551 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.6809Epoch 9/100, Loss: 0.6816232204437256, Val Loss: 0.7812898755073547\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6816 - val_loss: 0.7813 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6625Epoch 10/100, Loss: 0.6632202863693237, Val Loss: 0.5638854503631592\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6632 - val_loss: 0.5639 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6366Epoch 11/100, Loss: 0.6373865008354187, Val Loss: 0.5049386024475098\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6374 - val_loss: 0.5049 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6354Epoch 12/100, Loss: 0.6354183554649353, Val Loss: 0.5464015603065491\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6354 - val_loss: 0.5464 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.6189Epoch 13/100, Loss: 0.6200563311576843, Val Loss: 0.548562228679657\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6201 - val_loss: 0.5486 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6085Epoch 14/100, Loss: 0.6058284044265747, Val Loss: 0.5265710353851318\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6058 - val_loss: 0.5266 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.5340Epoch 15/100, Loss: 0.5384804606437683, Val Loss: 0.3790399730205536\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5385 - val_loss: 0.3790 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.5150Epoch 16/100, Loss: 0.5149961709976196, Val Loss: 0.395390123128891\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5150 - val_loss: 0.3954 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5005Epoch 17/100, Loss: 0.5007569789886475, Val Loss: 0.5072268843650818\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5008 - val_loss: 0.5072 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4942Epoch 18/100, Loss: 0.4929167330265045, Val Loss: 0.4426006078720093\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4929 - val_loss: 0.4426 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4552Epoch 19/100, Loss: 0.455898642539978, Val Loss: 0.35215580463409424\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4559 - val_loss: 0.3522 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4533Epoch 20/100, Loss: 0.4533539414405823, Val Loss: 0.3685559630393982\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4534 - val_loss: 0.3686 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4331Epoch 21/100, Loss: 0.43349847197532654, Val Loss: 0.30621111392974854\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4335 - val_loss: 0.3062 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4303Epoch 22/100, Loss: 0.42944350838661194, Val Loss: 0.34444671869277954\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4294 - val_loss: 0.3444 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.4234Epoch 23/100, Loss: 0.423069566488266, Val Loss: 0.32650789618492126\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4231 - val_loss: 0.3265 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4209Epoch 24/100, Loss: 0.4203871488571167, Val Loss: 0.35108891129493713\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4204 - val_loss: 0.3511 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4035Epoch 25/100, Loss: 0.40351977944374084, Val Loss: 0.28860944509506226\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4035 - val_loss: 0.2886 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3952Epoch 26/100, Loss: 0.39449629187583923, Val Loss: 0.28129464387893677\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3945 - val_loss: 0.2813 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3855Epoch 27/100, Loss: 0.3860172927379608, Val Loss: 0.28837037086486816\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3860 - val_loss: 0.2884 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3854Epoch 28/100, Loss: 0.38711437582969666, Val Loss: 0.279289186000824\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3871 - val_loss: 0.2793 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3860Epoch 29/100, Loss: 0.38610056042671204, Val Loss: 0.2865116596221924\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3861 - val_loss: 0.2865 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.3790Epoch 30/100, Loss: 0.3795185089111328, Val Loss: 0.28520670533180237\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3795 - val_loss: 0.2852 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3768Epoch 31/100, Loss: 0.3761013448238373, Val Loss: 0.26908913254737854\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3761 - val_loss: 0.2691 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3826Epoch 32/100, Loss: 0.3825853765010834, Val Loss: 0.30396023392677307\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3826 - val_loss: 0.3040 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3737Epoch 33/100, Loss: 0.37312448024749756, Val Loss: 0.28552085161209106\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3731 - val_loss: 0.2855 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3687Epoch 34/100, Loss: 0.3685518205165863, Val Loss: 0.31913843750953674\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3686 - val_loss: 0.3191 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3562Epoch 35/100, Loss: 0.3555213510990143, Val Loss: 0.30326154828071594\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3555 - val_loss: 0.3033 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3616Epoch 36/100, Loss: 0.36163023114204407, Val Loss: 0.2991941571235657\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3616 - val_loss: 0.2992 - lr: 6.2500e-05\n",
      "Epoch 36: early stopping\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 1/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 2.2539Epoch 1/100, Loss: 2.2336676120758057, Val Loss: 1.385613203048706\n",
      "591/591 [==============================] - 3s 1ms/step - loss: 2.2337 - val_loss: 1.3856 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 1.2147Epoch 2/100, Loss: 1.2130026817321777, Val Loss: 1.005332350730896\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.2130 - val_loss: 1.0053 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 1.1016Epoch 3/100, Loss: 1.1024121046066284, Val Loss: 1.113701581954956\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.1024 - val_loss: 1.1137 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0468Epoch 4/100, Loss: 1.0488594770431519, Val Loss: 0.7960271835327148\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0489 - val_loss: 0.7960 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 1.0124Epoch 5/100, Loss: 1.0122206211090088, Val Loss: 0.7538580894470215\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0122 - val_loss: 0.7539 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 1.0011Epoch 6/100, Loss: 1.0008212327957153, Val Loss: 0.7181714773178101\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0008 - val_loss: 0.7182 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.9725Epoch 7/100, Loss: 0.975287675857544, Val Loss: 0.7285857796669006\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9753 - val_loss: 0.7286 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.9473Epoch 8/100, Loss: 0.9482748508453369, Val Loss: 0.695053219795227\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9483 - val_loss: 0.6951 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.9054Epoch 9/100, Loss: 0.9110243320465088, Val Loss: 0.6273345351219177\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9110 - val_loss: 0.6273 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.8895Epoch 10/100, Loss: 0.8902340531349182, Val Loss: 0.6309887170791626\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8902 - val_loss: 0.6310 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.8814Epoch 11/100, Loss: 0.8793947100639343, Val Loss: 0.5825778841972351\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8794 - val_loss: 0.5826 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.8718Epoch 12/100, Loss: 0.8707937598228455, Val Loss: 0.6357687711715698\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8708 - val_loss: 0.6358 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.8529Epoch 13/100, Loss: 0.854774534702301, Val Loss: 0.6192622780799866\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8548 - val_loss: 0.6193 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.8357Epoch 14/100, Loss: 0.8362757563591003, Val Loss: 0.63623046875\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8363 - val_loss: 0.6362 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.8030Epoch 15/100, Loss: 0.8032791018486023, Val Loss: 0.557271420955658\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8033 - val_loss: 0.5573 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7960Epoch 16/100, Loss: 0.7976430058479309, Val Loss: 0.5425440669059753\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7976 - val_loss: 0.5425 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.7924Epoch 17/100, Loss: 0.7895185351371765, Val Loss: 0.550268828868866\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7895 - val_loss: 0.5503 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7942Epoch 18/100, Loss: 0.7920188903808594, Val Loss: 0.5552706718444824\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7920 - val_loss: 0.5553 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "548/591 [==========================>...] - ETA: 0s - loss: 0.7673Epoch 19/100, Loss: 0.7706498503684998, Val Loss: 0.5254659652709961\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7706 - val_loss: 0.5255 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.7699Epoch 20/100, Loss: 0.7749114036560059, Val Loss: 0.5406047701835632\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7749 - val_loss: 0.5406 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.7681Epoch 21/100, Loss: 0.7683226466178894, Val Loss: 0.5545650124549866\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7683 - val_loss: 0.5546 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.7635Epoch 22/100, Loss: 0.7638418674468994, Val Loss: 0.6188865303993225\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7638 - val_loss: 0.6189 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.7483Epoch 23/100, Loss: 0.7472063899040222, Val Loss: 0.5076442360877991\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7472 - val_loss: 0.5076 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7640Epoch 24/100, Loss: 0.7634997367858887, Val Loss: 0.5037042498588562\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7635 - val_loss: 0.5037 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "557/591 [===========================>..] - ETA: 0s - loss: 0.7524Epoch 25/100, Loss: 0.7535998225212097, Val Loss: 0.5239667296409607\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7536 - val_loss: 0.5240 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7527Epoch 26/100, Loss: 0.7508187294006348, Val Loss: 0.5363856554031372\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7508 - val_loss: 0.5364 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.7470Epoch 27/100, Loss: 0.7455815672874451, Val Loss: 0.5195133090019226\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7456 - val_loss: 0.5195 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.7350Epoch 28/100, Loss: 0.7358378767967224, Val Loss: 0.48838910460472107\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7358 - val_loss: 0.4884 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7356Epoch 29/100, Loss: 0.7366763949394226, Val Loss: 0.48017698526382446\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7367 - val_loss: 0.4802 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.7400Epoch 30/100, Loss: 0.7393350005149841, Val Loss: 0.48409342765808105\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7393 - val_loss: 0.4841 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.7328Epoch 31/100, Loss: 0.7327868938446045, Val Loss: 0.48793408274650574\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7328 - val_loss: 0.4879 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.7521Epoch 32/100, Loss: 0.7536521553993225, Val Loss: 0.4776999354362488\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7537 - val_loss: 0.4777 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7294Epoch 33/100, Loss: 0.7272494435310364, Val Loss: 0.48996397852897644\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7272 - val_loss: 0.4900 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7276Epoch 34/100, Loss: 0.7276154160499573, Val Loss: 0.48850733041763306\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7276 - val_loss: 0.4885 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.7327Epoch 35/100, Loss: 0.732578694820404, Val Loss: 0.4810221195220947\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7326 - val_loss: 0.4810 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.7355Epoch 36/100, Loss: 0.7343418002128601, Val Loss: 0.47716450691223145\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7343 - val_loss: 0.4772 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.7313Epoch 37/100, Loss: 0.7374187707901001, Val Loss: 0.4728615880012512\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7374 - val_loss: 0.4729 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7229Epoch 38/100, Loss: 0.7221122980117798, Val Loss: 0.475314199924469\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7221 - val_loss: 0.4753 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.7375Epoch 39/100, Loss: 0.734325110912323, Val Loss: 0.4754440486431122\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7343 - val_loss: 0.4754 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.7279Epoch 40/100, Loss: 0.7283156514167786, Val Loss: 0.48184844851493835\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7283 - val_loss: 0.4818 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7196Epoch 41/100, Loss: 0.7180266380310059, Val Loss: 0.46889328956604004\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7180 - val_loss: 0.4689 - lr: 3.1250e-05\n",
      "Epoch 42/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.7293Epoch 42/100, Loss: 0.7263277769088745, Val Loss: 0.47362127900123596\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7263 - val_loss: 0.4736 - lr: 3.1250e-05\n",
      "Epoch 43/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.7245Epoch 43/100, Loss: 0.7217105627059937, Val Loss: 0.4719942510128021\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7217 - val_loss: 0.4720 - lr: 3.1250e-05\n",
      "Epoch 44/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.7248Epoch 44/100, Loss: 0.7197930812835693, Val Loss: 0.4721338748931885\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7198 - val_loss: 0.4721 - lr: 3.1250e-05\n",
      "Epoch 45/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.7260Epoch 45/100, Loss: 0.7296838760375977, Val Loss: 0.4756530225276947\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7297 - val_loss: 0.4757 - lr: 1.5625e-05\n",
      "Epoch 46/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7340Epoch 46/100, Loss: 0.7312475442886353, Val Loss: 0.47214680910110474\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7312 - val_loss: 0.4721 - lr: 1.5625e-05\n",
      "Epoch 46: early stopping\n",
      "127/127 [==============================] - 0s 566us/step\n",
      "127/127 [==============================] - 0s 558us/step - loss: 0.5100\n",
      "Epoch 1/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 1.8699Epoch 1/100, Loss: 1.8567218780517578, Val Loss: 1.377008080482483\n",
      "591/591 [==============================] - 2s 1ms/step - loss: 1.8567 - val_loss: 1.3770 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "546/591 [==========================>...] - ETA: 0s - loss: 1.1204Epoch 2/100, Loss: 1.117063283920288, Val Loss: 0.8623367547988892\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.1171 - val_loss: 0.8623 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 1.0565Epoch 3/100, Loss: 1.055461049079895, Val Loss: 0.8636459112167358\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0555 - val_loss: 0.8636 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 1.0124Epoch 4/100, Loss: 1.012441635131836, Val Loss: 0.7761577367782593\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0124 - val_loss: 0.7762 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.9876Epoch 5/100, Loss: 0.9876880049705505, Val Loss: 0.7559944987297058\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9877 - val_loss: 0.7560 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.9812Epoch 6/100, Loss: 0.9829720854759216, Val Loss: 0.7632639408111572\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9830 - val_loss: 0.7633 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.9551Epoch 7/100, Loss: 0.9597477316856384, Val Loss: 0.7183177471160889\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9597 - val_loss: 0.7183 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "544/591 [==========================>...] - ETA: 0s - loss: 0.9313Epoch 8/100, Loss: 0.9331042170524597, Val Loss: 0.7638611197471619\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9331 - val_loss: 0.7639 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.8975Epoch 9/100, Loss: 0.9041017889976501, Val Loss: 0.744028627872467\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9041 - val_loss: 0.7440 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "552/591 [===========================>..] - ETA: 0s - loss: 0.8834Epoch 10/100, Loss: 0.8833045959472656, Val Loss: 0.7242038249969482\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8833 - val_loss: 0.7242 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.8403Epoch 11/100, Loss: 0.8407946228981018, Val Loss: 0.8635155558586121\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8408 - val_loss: 0.8635 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.8360Epoch 12/100, Loss: 0.8359935879707336, Val Loss: 0.6241546869277954\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8360 - val_loss: 0.6242 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.8232Epoch 13/100, Loss: 0.8253330588340759, Val Loss: 0.92198646068573\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8253 - val_loss: 0.9220 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "549/591 [==========================>...] - ETA: 0s - loss: 0.8098Epoch 14/100, Loss: 0.8097563982009888, Val Loss: 0.5772462487220764\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8098 - val_loss: 0.5772 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.7921Epoch 15/100, Loss: 0.7920843958854675, Val Loss: 6.654734134674072\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7921 - val_loss: 6.6547 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.7857Epoch 16/100, Loss: 0.7858297824859619, Val Loss: 0.607050359249115\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7858 - val_loss: 0.6071 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7912Epoch 17/100, Loss: 0.7908883094787598, Val Loss: 0.6474968791007996\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7909 - val_loss: 0.6475 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7573Epoch 18/100, Loss: 0.756758451461792, Val Loss: 0.6377938389778137\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7568 - val_loss: 0.6378 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.7428Epoch 19/100, Loss: 0.7430928945541382, Val Loss: 0.583412766456604\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7431 - val_loss: 0.5834 - lr: 2.5000e-04\n",
      "Epoch 19: early stopping\n",
      "127/127 [==============================] - 0s 574us/step\n",
      "127/127 [==============================] - 0s 621us/step - loss: 0.6552\n",
      "Epoch 1/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 1.6510Epoch 1/100, Loss: 1.6172401905059814, Val Loss: 1.2064886093139648\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.6172 - val_loss: 1.2065 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 1.0479Epoch 2/100, Loss: 1.0456541776657104, Val Loss: 0.7979156374931335\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 1.0457 - val_loss: 0.7979 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.9878Epoch 3/100, Loss: 0.989730715751648, Val Loss: 0.774478018283844\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9897 - val_loss: 0.7745 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.9469Epoch 4/100, Loss: 0.9482471346855164, Val Loss: 0.6634875535964966\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9482 - val_loss: 0.6635 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.9246Epoch 5/100, Loss: 0.9245839715003967, Val Loss: 0.7285771369934082\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.9246 - val_loss: 0.7286 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.8913Epoch 6/100, Loss: 0.8913172483444214, Val Loss: 0.7832854986190796\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8913 - val_loss: 0.7833 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.8763Epoch 7/100, Loss: 0.8814476132392883, Val Loss: 0.6058628559112549\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8814 - val_loss: 0.6059 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.8478Epoch 8/100, Loss: 0.85015869140625, Val Loss: 0.6749193072319031\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.8502 - val_loss: 0.6749 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.7889Epoch 9/100, Loss: 0.7924943566322327, Val Loss: 0.6287654042243958\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7925 - val_loss: 0.6288 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7692Epoch 10/100, Loss: 0.769359827041626, Val Loss: 0.5943164825439453\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7694 - val_loss: 0.5943 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.7321Epoch 11/100, Loss: 0.7300064563751221, Val Loss: 0.6932191252708435\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7300 - val_loss: 0.6932 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7203Epoch 12/100, Loss: 0.720366895198822, Val Loss: 0.5816707611083984\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.7204 - val_loss: 0.5817 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.6908Epoch 13/100, Loss: 0.694902241230011, Val Loss: 0.8068580031394958\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6949 - val_loss: 0.8069 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "554/591 [===========================>..] - ETA: 0s - loss: 0.6734Epoch 14/100, Loss: 0.6726704835891724, Val Loss: 1.9443248510360718\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6727 - val_loss: 1.9443 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "550/591 [==========================>...] - ETA: 0s - loss: 0.6555Epoch 15/100, Loss: 0.6568582653999329, Val Loss: 0.5138964056968689\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6569 - val_loss: 0.5139 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.6438Epoch 16/100, Loss: 0.642832338809967, Val Loss: 0.6496249437332153\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6428 - val_loss: 0.6496 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.6520Epoch 17/100, Loss: 0.6485010981559753, Val Loss: 0.5192344188690186\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6485 - val_loss: 0.5192 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6248Epoch 18/100, Loss: 0.6243103742599487, Val Loss: 0.43758609890937805\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6243 - val_loss: 0.4376 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.6048Epoch 19/100, Loss: 0.6052178740501404, Val Loss: 0.47837331891059875\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6052 - val_loss: 0.4784 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.6097Epoch 20/100, Loss: 0.6114773750305176, Val Loss: 0.4765318036079407\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6115 - val_loss: 0.4765 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.5918Epoch 21/100, Loss: 0.5881679654121399, Val Loss: 0.4300476014614105\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5882 - val_loss: 0.4300 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "561/591 [===========================>..] - ETA: 0s - loss: 0.5801Epoch 22/100, Loss: 0.5820574760437012, Val Loss: 0.5040368437767029\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5821 - val_loss: 0.5040 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5787Epoch 23/100, Loss: 0.5792736411094666, Val Loss: 0.481473445892334\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5793 - val_loss: 0.4815 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.5771Epoch 24/100, Loss: 0.5755122303962708, Val Loss: 0.4021971821784973\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5755 - val_loss: 0.4022 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.5671Epoch 25/100, Loss: 0.5668612718582153, Val Loss: 0.47061771154403687\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5669 - val_loss: 0.4706 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5671Epoch 26/100, Loss: 0.5664529204368591, Val Loss: 0.4037824273109436\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5665 - val_loss: 0.4038 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.5515Epoch 27/100, Loss: 0.5501716732978821, Val Loss: 0.41118815541267395\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5502 - val_loss: 0.4112 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.5103Epoch 28/100, Loss: 0.5128393769264221, Val Loss: 0.365110844373703\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5128 - val_loss: 0.3651 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5100Epoch 29/100, Loss: 0.5075317025184631, Val Loss: 0.3860281705856323\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5075 - val_loss: 0.3860 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "566/591 [===========================>..] - ETA: 0s - loss: 0.4999Epoch 30/100, Loss: 0.5000615119934082, Val Loss: 0.37901872396469116\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5001 - val_loss: 0.3790 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4946Epoch 31/100, Loss: 0.493185818195343, Val Loss: 0.3600637912750244\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4932 - val_loss: 0.3601 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4998Epoch 32/100, Loss: 0.5005653500556946, Val Loss: 0.36513182520866394\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5006 - val_loss: 0.3651 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4863Epoch 33/100, Loss: 0.4839956760406494, Val Loss: 0.3399536907672882\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4840 - val_loss: 0.3400 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4847Epoch 34/100, Loss: 0.4847404658794403, Val Loss: 0.3828202486038208\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4847 - val_loss: 0.3828 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4739Epoch 35/100, Loss: 0.4739980101585388, Val Loss: 0.35982850193977356\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4740 - val_loss: 0.3598 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4798Epoch 36/100, Loss: 0.4797055721282959, Val Loss: 0.3464696705341339\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4797 - val_loss: 0.3465 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.4535Epoch 37/100, Loss: 0.45986685156822205, Val Loss: 0.3521166145801544\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4599 - val_loss: 0.3521 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4493Epoch 38/100, Loss: 0.4490148723125458, Val Loss: 0.32260236144065857\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4490 - val_loss: 0.3226 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4588Epoch 39/100, Loss: 0.4547393321990967, Val Loss: 0.31207942962646484\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4547 - val_loss: 0.3121 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4465Epoch 40/100, Loss: 0.44816815853118896, Val Loss: 0.3173776865005493\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4482 - val_loss: 0.3174 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.4390Epoch 41/100, Loss: 0.4399443566799164, Val Loss: 0.3234440088272095\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4399 - val_loss: 0.3234 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4420Epoch 42/100, Loss: 0.4423152804374695, Val Loss: 0.3259148597717285\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4423 - val_loss: 0.3259 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4312Epoch 43/100, Loss: 0.428493469953537, Val Loss: 0.31258338689804077\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4285 - val_loss: 0.3126 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4304Epoch 44/100, Loss: 0.4266296625137329, Val Loss: 0.2988332211971283\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4266 - val_loss: 0.2988 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4299Epoch 45/100, Loss: 0.4343032240867615, Val Loss: 0.29601094126701355\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4343 - val_loss: 0.2960 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.4352Epoch 46/100, Loss: 0.43292003870010376, Val Loss: 0.28410857915878296\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4329 - val_loss: 0.2841 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "551/591 [==========================>...] - ETA: 0s - loss: 0.4346Epoch 47/100, Loss: 0.4286748170852661, Val Loss: 0.2936404347419739\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4287 - val_loss: 0.2936 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "555/591 [===========================>..] - ETA: 0s - loss: 0.4316Epoch 48/100, Loss: 0.42926961183547974, Val Loss: 0.29581427574157715\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4293 - val_loss: 0.2958 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4226Epoch 49/100, Loss: 0.4213249087333679, Val Loss: 0.2789127230644226\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4213 - val_loss: 0.2789 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.4242Epoch 50/100, Loss: 0.42285534739494324, Val Loss: 0.2828252613544464\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4229 - val_loss: 0.2828 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4247Epoch 51/100, Loss: 0.42565929889678955, Val Loss: 0.28543832898139954\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4257 - val_loss: 0.2854 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4192Epoch 52/100, Loss: 0.41918373107910156, Val Loss: 0.2987231910228729\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4192 - val_loss: 0.2987 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "553/591 [===========================>..] - ETA: 0s - loss: 0.4126Epoch 53/100, Loss: 0.41864049434661865, Val Loss: 0.2895587682723999\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4186 - val_loss: 0.2896 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "558/591 [===========================>..] - ETA: 0s - loss: 0.4253Epoch 54/100, Loss: 0.4203838109970093, Val Loss: 0.27637559175491333\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4204 - val_loss: 0.2764 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4209Epoch 55/100, Loss: 0.42038094997406006, Val Loss: 0.2829505503177643\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4204 - val_loss: 0.2830 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4142Epoch 56/100, Loss: 0.4137445092201233, Val Loss: 0.28203099966049194\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4137 - val_loss: 0.2820 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "560/591 [===========================>..] - ETA: 0s - loss: 0.4244Epoch 57/100, Loss: 0.4227066934108734, Val Loss: 0.28631919622421265\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4227 - val_loss: 0.2863 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "565/591 [===========================>..] - ETA: 0s - loss: 0.4136Epoch 58/100, Loss: 0.41744640469551086, Val Loss: 0.2930969297885895\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4174 - val_loss: 0.2931 - lr: 3.1250e-05\n",
      "Epoch 59/100\n",
      "562/591 [===========================>..] - ETA: 0s - loss: 0.4105Epoch 59/100, Loss: 0.4103829860687256, Val Loss: 0.27848905324935913\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.4104 - val_loss: 0.2785 - lr: 3.1250e-05\n",
      "Epoch 59: early stopping\n",
      "127/127 [==============================] - 0s 629us/step\n",
      "127/127 [==============================] - 0s 597us/step - loss: 0.3195\n",
      "Epoch 1/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 1.4243Epoch 1/100, Loss: 1.4168546199798584, Val Loss: 1.2323641777038574\n",
      "591/591 [==============================] - 2s 2ms/step - loss: 1.4169 - val_loss: 1.2324 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 1.0473Epoch 2/100, Loss: 1.0482491254806519, Val Loss: 0.7648626565933228\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0482 - val_loss: 0.7649 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.9938Epoch 3/100, Loss: 0.9935847520828247, Val Loss: 0.7261624932289124\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9936 - val_loss: 0.7262 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "564/591 [===========================>..] - ETA: 0s - loss: 0.9420Epoch 4/100, Loss: 0.9419313073158264, Val Loss: 0.6767266988754272\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9419 - val_loss: 0.6767 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "559/591 [===========================>..] - ETA: 0s - loss: 0.8981Epoch 5/100, Loss: 0.8925423622131348, Val Loss: 0.6287442445755005\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8925 - val_loss: 0.6287 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8348Epoch 6/100, Loss: 0.8339243531227112, Val Loss: 0.700127363204956\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8339 - val_loss: 0.7001 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.7737Epoch 7/100, Loss: 0.775664210319519, Val Loss: 0.63929682970047\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7757 - val_loss: 0.6393 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.7338Epoch 8/100, Loss: 0.7347397804260254, Val Loss: 0.5008463263511658\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7347 - val_loss: 0.5008 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.6970Epoch 9/100, Loss: 0.7034889459609985, Val Loss: 1.447562336921692\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7035 - val_loss: 1.4476 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6855Epoch 10/100, Loss: 0.6857662796974182, Val Loss: 0.501404345035553\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6858 - val_loss: 0.5014 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.6574Epoch 11/100, Loss: 0.6573168039321899, Val Loss: 0.592482328414917\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6573 - val_loss: 0.5925 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.5985Epoch 12/100, Loss: 0.5981427431106567, Val Loss: 0.42440053820610046\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5981 - val_loss: 0.4244 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5776Epoch 13/100, Loss: 0.5796688199043274, Val Loss: 0.40586090087890625\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5797 - val_loss: 0.4059 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.5629Epoch 14/100, Loss: 0.5609930157661438, Val Loss: 0.3960750102996826\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5610 - val_loss: 0.3961 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5570Epoch 15/100, Loss: 0.5570748448371887, Val Loss: 0.4506908059120178\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5571 - val_loss: 0.4507 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.5342Epoch 16/100, Loss: 0.5321646332740784, Val Loss: 0.43889784812927246\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5322 - val_loss: 0.4389 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5320Epoch 17/100, Loss: 0.5301943421363831, Val Loss: 0.4273884892463684\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5302 - val_loss: 0.4274 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.4939Epoch 18/100, Loss: 0.49129024147987366, Val Loss: 0.3535674810409546\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4913 - val_loss: 0.3536 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.4776Epoch 19/100, Loss: 0.4783037304878235, Val Loss: 0.33109426498413086\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4783 - val_loss: 0.3311 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4800Epoch 20/100, Loss: 0.4813828468322754, Val Loss: 0.3511630594730377\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4814 - val_loss: 0.3512 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4640Epoch 21/100, Loss: 0.46335023641586304, Val Loss: 0.3047776520252228\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4634 - val_loss: 0.3048 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.4523Epoch 22/100, Loss: 0.45271775126457214, Val Loss: 0.3459058105945587\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4527 - val_loss: 0.3459 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "571/591 [===========================>..] - ETA: 0s - loss: 0.4491Epoch 23/100, Loss: 0.4475746154785156, Val Loss: 0.3130844235420227\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4476 - val_loss: 0.3131 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4517Epoch 24/100, Loss: 0.45126664638519287, Val Loss: 0.3245985805988312\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4513 - val_loss: 0.3246 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4363Epoch 25/100, Loss: 0.43571537733078003, Val Loss: 0.302056223154068\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4357 - val_loss: 0.3021 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4292Epoch 26/100, Loss: 0.42642641067504883, Val Loss: 0.279471218585968\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4264 - val_loss: 0.2795 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4154Epoch 27/100, Loss: 0.4142562747001648, Val Loss: 0.30240297317504883\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4143 - val_loss: 0.3024 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4196Epoch 28/100, Loss: 0.4216003715991974, Val Loss: 0.2889743149280548\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4216 - val_loss: 0.2890 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4190Epoch 29/100, Loss: 0.4186093807220459, Val Loss: 0.284382164478302\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4186 - val_loss: 0.2844 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.4067Epoch 30/100, Loss: 0.40617018938064575, Val Loss: 0.2694154381752014\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4062 - val_loss: 0.2694 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.4018Epoch 31/100, Loss: 0.39910435676574707, Val Loss: 0.28608477115631104\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3991 - val_loss: 0.2861 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4080Epoch 32/100, Loss: 0.40879470109939575, Val Loss: 0.28260764479637146\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4088 - val_loss: 0.2826 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3986Epoch 33/100, Loss: 0.39745259284973145, Val Loss: 0.2687414884567261\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3975 - val_loss: 0.2687 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.3970Epoch 34/100, Loss: 0.39708608388900757, Val Loss: 0.26058724522590637\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3971 - val_loss: 0.2606 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3944Epoch 35/100, Loss: 0.39421504735946655, Val Loss: 0.2668825089931488\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3942 - val_loss: 0.2669 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.4023Epoch 36/100, Loss: 0.4021954834461212, Val Loss: 0.2697887420654297\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.2698 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.3925Epoch 37/100, Loss: 0.3994804322719574, Val Loss: 0.27311307191848755\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3995 - val_loss: 0.2731 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "572/591 [============================>.] - ETA: 0s - loss: 0.3912Epoch 38/100, Loss: 0.39037004113197327, Val Loss: 0.26613089442253113\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3904 - val_loss: 0.2661 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3920Epoch 39/100, Loss: 0.3920484483242035, Val Loss: 0.26408690214157104\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.2641 - lr: 3.1250e-05\n",
      "Epoch 39: early stopping\n",
      "127/127 [==============================] - 0s 698us/step\n",
      "127/127 [==============================] - 0s 637us/step - loss: 0.3066\n",
      "Epoch 1/100\n",
      "578/591 [============================>.] - ETA: 0s - loss: 1.3649Epoch 1/100, Loss: 1.3634482622146606, Val Loss: 1.1458576917648315\n",
      "591/591 [==============================] - 3s 3ms/step - loss: 1.3634 - val_loss: 1.1459 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 1.0688Epoch 2/100, Loss: 1.0684438943862915, Val Loss: 0.8551996350288391\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0684 - val_loss: 0.8552 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 1.0178Epoch 3/100, Loss: 1.0183802843093872, Val Loss: 0.7987167835235596\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 1.0184 - val_loss: 0.7987 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.9589Epoch 4/100, Loss: 0.9598855972290039, Val Loss: 0.6697352528572083\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9599 - val_loss: 0.6697 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.9084Epoch 5/100, Loss: 0.9074687957763672, Val Loss: 0.7423335909843445\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.9075 - val_loss: 0.7423 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.8408Epoch 6/100, Loss: 0.8394942283630371, Val Loss: 0.7007133364677429\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.8395 - val_loss: 0.7007 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.7828Epoch 7/100, Loss: 0.7832706570625305, Val Loss: 0.7017626762390137\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.7833 - val_loss: 0.7018 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6742Epoch 8/100, Loss: 0.6752035021781921, Val Loss: 0.46902623772621155\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6752 - val_loss: 0.4690 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6380Epoch 9/100, Loss: 0.6431364417076111, Val Loss: 0.6304746866226196\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6431 - val_loss: 0.6305 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.6207Epoch 10/100, Loss: 0.6205462217330933, Val Loss: 0.4720405340194702\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.6205 - val_loss: 0.4720 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "574/591 [============================>.] - ETA: 0s - loss: 0.5981Epoch 11/100, Loss: 0.5972331166267395, Val Loss: 0.44442927837371826\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5972 - val_loss: 0.4444 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.5881Epoch 12/100, Loss: 0.5873197913169861, Val Loss: 0.4546944200992584\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5873 - val_loss: 0.4547 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "567/591 [===========================>..] - ETA: 0s - loss: 0.5720Epoch 13/100, Loss: 0.5740119218826294, Val Loss: 0.46588513255119324\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5740 - val_loss: 0.4659 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5567Epoch 14/100, Loss: 0.5549187660217285, Val Loss: 0.4302155375480652\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5549 - val_loss: 0.4302 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.5364Epoch 15/100, Loss: 0.5409601330757141, Val Loss: 0.40311476588249207\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5410 - val_loss: 0.4031 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "579/591 [============================>.] - ETA: 0s - loss: 0.5244Epoch 16/100, Loss: 0.5229384899139404, Val Loss: 0.3959280252456665\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5229 - val_loss: 0.3959 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5177Epoch 17/100, Loss: 0.5170981287956238, Val Loss: 0.4038105010986328\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5171 - val_loss: 0.4038 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5130Epoch 18/100, Loss: 0.5120822191238403, Val Loss: 0.4204699695110321\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5121 - val_loss: 0.4205 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5053Epoch 19/100, Loss: 0.5051925778388977, Val Loss: 0.3928576409816742\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5052 - val_loss: 0.3929 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4997Epoch 20/100, Loss: 0.5008317232131958, Val Loss: 0.39482203125953674\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.5008 - val_loss: 0.3948 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4813Epoch 21/100, Loss: 0.48133060336112976, Val Loss: 0.38422635197639465\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.3842 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4764Epoch 22/100, Loss: 0.4760655462741852, Val Loss: 0.43296095728874207\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4761 - val_loss: 0.4330 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.4710Epoch 23/100, Loss: 0.4691932797431946, Val Loss: 0.3674418032169342\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4692 - val_loss: 0.3674 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4762Epoch 24/100, Loss: 0.4753325879573822, Val Loss: 0.3352976441383362\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4753 - val_loss: 0.3353 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.4682Epoch 25/100, Loss: 0.46763113141059875, Val Loss: 0.37412190437316895\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4676 - val_loss: 0.3741 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.4582Epoch 26/100, Loss: 0.45750904083251953, Val Loss: 0.3887290358543396\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4575 - val_loss: 0.3887 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.4466Epoch 27/100, Loss: 0.4456248879432678, Val Loss: 0.3473605513572693\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4456 - val_loss: 0.3474 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.4142Epoch 28/100, Loss: 0.4160135090351105, Val Loss: 0.32871177792549133\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4160 - val_loss: 0.3287 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4052Epoch 29/100, Loss: 0.4051237106323242, Val Loss: 0.3067169189453125\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.4051 - val_loss: 0.3067 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.3986Epoch 30/100, Loss: 0.39853665232658386, Val Loss: 0.3512742221355438\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.3513 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3990Epoch 31/100, Loss: 0.3986096978187561, Val Loss: 0.3253095746040344\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3986 - val_loss: 0.3253 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3920Epoch 32/100, Loss: 0.39306774735450745, Val Loss: 0.3124520778656006\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3931 - val_loss: 0.3125 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3636Epoch 33/100, Loss: 0.362098753452301, Val Loss: 0.27870672941207886\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3621 - val_loss: 0.2787 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "576/591 [============================>.] - ETA: 0s - loss: 0.3590Epoch 34/100, Loss: 0.35862573981285095, Val Loss: 0.3041815459728241\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3586 - val_loss: 0.3042 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "569/591 [===========================>..] - ETA: 0s - loss: 0.3552Epoch 35/100, Loss: 0.3552604913711548, Val Loss: 0.3101109564304352\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3553 - val_loss: 0.3101 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3560Epoch 36/100, Loss: 0.3550877273082733, Val Loss: 0.24610981345176697\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3551 - val_loss: 0.2461 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3505Epoch 37/100, Loss: 0.35625773668289185, Val Loss: 0.2726796567440033\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3563 - val_loss: 0.2727 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3531Epoch 38/100, Loss: 0.3531210124492645, Val Loss: 0.29467666149139404\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3531 - val_loss: 0.2947 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3533Epoch 39/100, Loss: 0.35252004861831665, Val Loss: 0.27063223719596863\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3525 - val_loss: 0.2706 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3395Epoch 40/100, Loss: 0.33877015113830566, Val Loss: 0.24738362431526184\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3388 - val_loss: 0.2474 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.3315Epoch 41/100, Loss: 0.3311399817466736, Val Loss: 0.23865725100040436\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3311 - val_loss: 0.2387 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.3288Epoch 42/100, Loss: 0.32947924733161926, Val Loss: 0.2492348849773407\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3295 - val_loss: 0.2492 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "573/591 [============================>.] - ETA: 0s - loss: 0.3294Epoch 43/100, Loss: 0.32799914479255676, Val Loss: 0.2493327409029007\n",
      "591/591 [==============================] - 1s 3ms/step - loss: 0.3280 - val_loss: 0.2493 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3274Epoch 44/100, Loss: 0.32807657122612, Val Loss: 0.2302963137626648\n",
      "591/591 [==============================] - 2s 3ms/step - loss: 0.3281 - val_loss: 0.2303 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "575/591 [============================>.] - ETA: 0s - loss: 0.3271Epoch 45/100, Loss: 0.3323443830013275, Val Loss: 0.2406349927186966\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3323 - val_loss: 0.2406 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "577/591 [============================>.] - ETA: 0s - loss: 0.3303Epoch 46/100, Loss: 0.3287261724472046, Val Loss: 0.2513730823993683\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.2514 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3240Epoch 47/100, Loss: 0.3239124119281769, Val Loss: 0.2560794949531555\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3239 - val_loss: 0.2561 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "580/591 [============================>.] - ETA: 0s - loss: 0.3209Epoch 48/100, Loss: 0.3193689286708832, Val Loss: 0.24299564957618713\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3194 - val_loss: 0.2430 - lr: 3.1250e-05\n",
      "Epoch 49/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3132Epoch 49/100, Loss: 0.31313374638557434, Val Loss: 0.23694978654384613\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "591/591 [==============================] - 1s 2ms/step - loss: 0.3131 - val_loss: 0.2369 - lr: 3.1250e-05\n",
      "Epoch 49: early stopping\n",
      "127/127 [==============================] - 0s 914us/step\n",
      "127/127 [==============================] - 0s 850us/step - loss: 0.2784\n",
      "Epoch 1/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 1.3267Epoch 1/100, Loss: 1.3252484798431396, Val Loss: 1.2234508991241455\n",
      "591/591 [==============================] - 5s 6ms/step - loss: 1.3252 - val_loss: 1.2235 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 1.0803Epoch 2/100, Loss: 1.077828288078308, Val Loss: 0.8369646668434143\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 1.0778 - val_loss: 0.8370 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 1.0345Epoch 3/100, Loss: 1.0349748134613037, Val Loss: 0.9200339317321777\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 1.0350 - val_loss: 0.9200 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.9620Epoch 4/100, Loss: 0.9625635743141174, Val Loss: 0.8202354311943054\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.9626 - val_loss: 0.8202 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.8885Epoch 5/100, Loss: 0.8875285387039185, Val Loss: 0.6686248779296875\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.8875 - val_loss: 0.6686 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.8326Epoch 6/100, Loss: 0.832406759262085, Val Loss: 0.7929536700248718\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.8324 - val_loss: 0.7930 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.7837Epoch 7/100, Loss: 0.7848971486091614, Val Loss: 0.6798272728919983\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.7849 - val_loss: 0.6798 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.7528Epoch 8/100, Loss: 0.7529358267784119, Val Loss: 0.818290114402771\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.7529 - val_loss: 0.8183 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6614Epoch 9/100, Loss: 0.6616990566253662, Val Loss: 0.6172495484352112\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6617 - val_loss: 0.6172 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6338Epoch 10/100, Loss: 0.633857011795044, Val Loss: 0.6979444026947021\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6339 - val_loss: 0.6979 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.6109Epoch 11/100, Loss: 0.6115837693214417, Val Loss: 0.5415437817573547\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6116 - val_loss: 0.5415 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.6025Epoch 12/100, Loss: 0.6018900871276855, Val Loss: 0.49340808391571045\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.6019 - val_loss: 0.4934 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.5843Epoch 13/100, Loss: 0.5849233865737915, Val Loss: 0.6716691851615906\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.5849 - val_loss: 0.6717 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5644Epoch 14/100, Loss: 0.564326822757721, Val Loss: 1.1638100147247314\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5643 - val_loss: 1.1638 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5505Epoch 15/100, Loss: 0.5508811473846436, Val Loss: 0.513702929019928\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5509 - val_loss: 0.5137 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.5052Epoch 16/100, Loss: 0.5039095282554626, Val Loss: 0.3861687183380127\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.5039 - val_loss: 0.3862 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.4936Epoch 17/100, Loss: 0.4928092062473297, Val Loss: 0.3820621967315674\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4928 - val_loss: 0.3821 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4810Epoch 18/100, Loss: 0.4810135066509247, Val Loss: 0.38952189683914185\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4810 - val_loss: 0.3895 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4756Epoch 19/100, Loss: 0.47562968730926514, Val Loss: 0.3712949752807617\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.4756 - val_loss: 0.3713 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4717Epoch 20/100, Loss: 0.4715319573879242, Val Loss: 0.3938296139240265\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4715 - val_loss: 0.3938 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.4515Epoch 21/100, Loss: 0.45148995518684387, Val Loss: 0.34446975588798523\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4515 - val_loss: 0.3445 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "588/591 [============================>.] - ETA: 0s - loss: 0.4470Epoch 22/100, Loss: 0.4468514323234558, Val Loss: 0.39830639958381653\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4469 - val_loss: 0.3983 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4387Epoch 23/100, Loss: 0.43920889496803284, Val Loss: 0.3375924825668335\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4392 - val_loss: 0.3376 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.4450Epoch 24/100, Loss: 0.4438699185848236, Val Loss: 0.4180988073348999\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4439 - val_loss: 0.4181 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.4385Epoch 25/100, Loss: 0.4382520616054535, Val Loss: 0.34181585907936096\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4383 - val_loss: 0.3418 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.4330Epoch 26/100, Loss: 0.4330730736255646, Val Loss: 0.49025583267211914\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.4331 - val_loss: 0.4903 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3984Epoch 27/100, Loss: 0.39837121963500977, Val Loss: 0.3749316930770874\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.3984 - val_loss: 0.3749 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3989Epoch 28/100, Loss: 0.3991061747074127, Val Loss: 0.3125448226928711\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.3991 - val_loss: 0.3125 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.3921Epoch 29/100, Loss: 0.39228278398513794, Val Loss: 0.36316391825675964\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.3923 - val_loss: 0.3632 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.3847Epoch 30/100, Loss: 0.3843413293361664, Val Loss: 0.33313271403312683\n",
      "591/591 [==============================] - 3s 6ms/step - loss: 0.3843 - val_loss: 0.3331 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3920Epoch 31/100, Loss: 0.3920435905456543, Val Loss: 0.2869377136230469\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3920 - val_loss: 0.2869 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3869Epoch 32/100, Loss: 0.38785314559936523, Val Loss: 0.3254775106906891\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3879 - val_loss: 0.3255 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3810Epoch 33/100, Loss: 0.3810492753982544, Val Loss: 0.28861498832702637\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3810 - val_loss: 0.2886 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.3731Epoch 34/100, Loss: 0.37239837646484375, Val Loss: 0.3264794647693634\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3724 - val_loss: 0.3265 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3596Epoch 35/100, Loss: 0.35864192247390747, Val Loss: 0.28923532366752625\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3586 - val_loss: 0.2892 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.3615Epoch 36/100, Loss: 0.36151009798049927, Val Loss: 0.27864399552345276\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3615 - val_loss: 0.2786 - lr: 6.2500e-05\n",
      "Epoch 37/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3594Epoch 37/100, Loss: 0.3592160642147064, Val Loss: 0.2698642611503601\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3592 - val_loss: 0.2699 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.3533Epoch 38/100, Loss: 0.3533521890640259, Val Loss: 0.27410176396369934\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3534 - val_loss: 0.2741 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.3528Epoch 39/100, Loss: 0.35195329785346985, Val Loss: 0.2616056203842163\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3520 - val_loss: 0.2616 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.3494Epoch 40/100, Loss: 0.3496141731739044, Val Loss: 0.262803852558136\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3496 - val_loss: 0.2628 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3436Epoch 41/100, Loss: 0.3433976173400879, Val Loss: 0.252456396818161\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3434 - val_loss: 0.2525 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3446Epoch 42/100, Loss: 0.3446182310581207, Val Loss: 0.2714116871356964\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3446 - val_loss: 0.2714 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3427Epoch 43/100, Loss: 0.341667115688324, Val Loss: 0.2847509980201721\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3417 - val_loss: 0.2848 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "583/591 [============================>.] - ETA: 0s - loss: 0.3433Epoch 44/100, Loss: 0.3429575264453888, Val Loss: 0.25218284130096436\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3430 - val_loss: 0.2522 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.3468Epoch 45/100, Loss: 0.3469259440898895, Val Loss: 0.26297369599342346\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3469 - val_loss: 0.2630 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.3448Epoch 46/100, Loss: 0.34452036023139954, Val Loss: 0.2723695933818817\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "591/591 [==============================] - 3s 5ms/step - loss: 0.3445 - val_loss: 0.2724 - lr: 6.2500e-05\n",
      "Epoch 46: early stopping\n",
      "127/127 [==============================] - 0s 2ms/step\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.2973\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with H=1 and Nh=16:\n",
      "MSE: 1.0377976894378662\n",
      "MAE: 1.0377975353700324\n",
      "RMSE: 5.6418429161009795\n",
      "R-squared: 0.5812174803083071\n",
      "MAPE: 1206.7070842435085%\n",
      "Model with H=1 and Nh=32:\n",
      "MSE: 0.9512962698936462\n",
      "MAE: 0.9512965798746599\n",
      "RMSE: 5.592968552186616\n",
      "R-squared: 0.6260371898517145\n",
      "MAPE: 661.2895949506111%\n",
      "Model with H=1 and Nh=64:\n",
      "MSE: 0.7937303185462952\n",
      "MAE: 0.7937306358643141\n",
      "RMSE: 5.497228530172455\n",
      "R-squared: 0.7298805521092772\n",
      "MAPE: 702.9496928929065%\n",
      "Model with H=1 and Nh=128:\n",
      "MSE: 0.7428430914878845\n",
      "MAE: 0.7428430431635823\n",
      "RMSE: 5.4770785611272785\n",
      "R-squared: 0.7565046919976752\n",
      "MAPE: 700.4852208737718%\n",
      "Model with H=1 and Nh=256:\n",
      "MSE: 0.7106107473373413\n",
      "MAE: 0.7106108079125096\n",
      "RMSE: 5.462265521274528\n",
      "R-squared: 0.7677192207248856\n",
      "MAPE: 762.0172281874479%\n",
      "Model with H=1 and Nh=512:\n",
      "MSE: 0.7157402634620667\n",
      "MAE: 0.7157403760520135\n",
      "RMSE: 5.468381432853199\n",
      "R-squared: 0.7668290819824516\n",
      "MAPE: 827.6471042046282%\n",
      "Model with H=2 and Nh=16:\n",
      "MSE: 0.7234553098678589\n",
      "MAE: 0.7234552477680674\n",
      "RMSE: 5.43091722464164\n",
      "R-squared: 0.70343644206779\n",
      "MAPE: 910.7531352390779%\n",
      "Model with H=2 and Nh=32:\n",
      "MSE: 0.7467362880706787\n",
      "MAE: 0.7467361423230554\n",
      "RMSE: 5.412193450922523\n",
      "R-squared: -13.123873685302577\n",
      "MAPE: 212.2241053514344%\n",
      "Model with H=2 and Nh=64:\n",
      "MSE: 0.5728654861450195\n",
      "MAE: 0.5728654369017142\n",
      "RMSE: 5.310653809487911\n",
      "R-squared: 0.7640791947385676\n",
      "MAPE: 829.1565704486311%\n",
      "Model with H=2 and Nh=128:\n",
      "MSE: 0.4949701428413391\n",
      "MAE: 0.4949701317481791\n",
      "RMSE: 5.295446908754846\n",
      "R-squared: 0.7948438650319427\n",
      "MAPE: 123.26211816537409%\n",
      "Model with H=2 and Nh=256:\n",
      "MSE: 0.3480604887008667\n",
      "MAE: 0.3480604382060623\n",
      "RMSE: 5.197653359843967\n",
      "R-squared: 0.8183201297347035\n",
      "MAPE: 382.6507889948333%\n",
      "Model with H=2 and Nh=512:\n",
      "MSE: 0.3458557426929474\n",
      "MAE: 0.3458558183866267\n",
      "RMSE: 5.16600848166019\n",
      "R-squared: 0.8238703964774577\n",
      "MAPE: 207.2245015679252%\n",
      "Model with H=3 and Nh=16:\n",
      "MSE: 0.6334913372993469\n",
      "MAE: 0.63349138668844\n",
      "RMSE: 5.380386022300742\n",
      "R-squared: 0.7312757275826525\n",
      "MAPE: 1040.0978678339702%\n",
      "Model with H=3 and Nh=32:\n",
      "MSE: 0.5238948464393616\n",
      "MAE: 0.5238947478638277\n",
      "RMSE: 5.283111771421689\n",
      "R-squared: 0.7559982956335703\n",
      "MAPE: 179.00383771944698%\n",
      "Model with H=3 and Nh=64:\n",
      "MSE: 0.3675934374332428\n",
      "MAE: 0.36759335497119583\n",
      "RMSE: 5.221868276726683\n",
      "R-squared: 0.7807014820239417\n",
      "MAPE: 194.77988745757463%\n",
      "Model with H=3 and Nh=128:\n",
      "MSE: 0.32407134771347046\n",
      "MAE: 0.32407131047404397\n",
      "RMSE: 5.161355884770611\n",
      "R-squared: 0.8022013138976801\n",
      "MAPE: 393.6568884262721%\n",
      "Model with H=3 and Nh=256:\n",
      "MSE: 0.28713974356651306\n",
      "MAE: 0.2871398206789961\n",
      "RMSE: 5.128979099586819\n",
      "R-squared: 0.8257737653620171\n",
      "MAPE: 54.93846383419043%\n",
      "Model with H=3 and Nh=512:\n",
      "MSE: 0.30234214663505554\n",
      "MAE: 0.30234211741092143\n",
      "RMSE: 5.16704751709155\n",
      "R-squared: 0.8209378328587462\n",
      "MAPE: 750.2109072146997%\n",
      "Model with H=4 and Nh=16:\n",
      "MSE: 0.5083134770393372\n",
      "MAE: 0.5083134872901952\n",
      "RMSE: 5.283353461277968\n",
      "R-squared: 0.7535216718623964\n",
      "MAPE: 258.0478674278932%\n",
      "Model with H=4 and Nh=32:\n",
      "MSE: 0.5420749187469482\n",
      "MAE: 0.5420748645713466\n",
      "RMSE: 5.31033036896737\n",
      "R-squared: 0.7325244782525792\n",
      "MAPE: 1018.8511225215871%\n",
      "Model with H=4 and Nh=64:\n",
      "MSE: 0.3767337501049042\n",
      "MAE: 0.3767337519706666\n",
      "RMSE: 5.231457670906006\n",
      "R-squared: 0.7774374246624477\n",
      "MAPE: 164.04062549456208%\n",
      "Model with H=4 and Nh=128:\n",
      "MSE: 0.31163865327835083\n",
      "MAE: 0.3116387536278419\n",
      "RMSE: 5.175717196960172\n",
      "R-squared: 0.8105155615441638\n",
      "MAPE: 147.68027064039612%\n",
      "Model with H=4 and Nh=256:\n",
      "MSE: 0.30668383836746216\n",
      "MAE: 0.3066837771239858\n",
      "RMSE: 5.141165991166607\n",
      "R-squared: 0.8164014808321706\n",
      "MAPE: 559.6913295866171%\n",
      "Model with H=4 and Nh=512:\n",
      "MSE: 0.27261117100715637\n",
      "MAE: 0.27261115457732804\n",
      "RMSE: 5.050260280056099\n",
      "R-squared: 0.8310635400577945\n",
      "MAPE: 219.01405274735865%\n",
      "Model with H=5 and Nh=16:\n",
      "MSE: 0.5777732133865356\n",
      "MAE: 0.5777732222923959\n",
      "RMSE: 5.32091462754457\n",
      "R-squared: 0.7127587008825014\n",
      "MAPE: 1044.8892567480527%\n",
      "Model with H=5 and Nh=32:\n",
      "MSE: 0.46241945028305054\n",
      "MAE: 0.46241956331650047\n",
      "RMSE: 5.254964584875408\n",
      "R-squared: 0.753670678636576\n",
      "MAPE: 284.63263186959864%\n",
      "Model with H=5 and Nh=64:\n",
      "MSE: 0.34128645062446594\n",
      "MAE: 0.3412863122122982\n",
      "RMSE: 5.194862687371621\n",
      "R-squared: 0.776831285406077\n",
      "MAPE: 699.9443978270921%\n",
      "Model with H=5 and Nh=128:\n",
      "MSE: 0.3180390000343323\n",
      "MAE: 0.3180389942935678\n",
      "RMSE: 5.132978324291064\n",
      "R-squared: -2.72774247949134\n",
      "MAPE: 611.6487184518661%\n",
      "Model with H=5 and Nh=256:\n",
      "MSE: 0.2801358103752136\n",
      "MAE: 0.2801358213155168\n",
      "RMSE: 5.026519992502772\n",
      "R-squared: 0.820967778952924\n",
      "MAPE: 100.91028945006666%\n",
      "Model with H=5 and Nh=512:\n",
      "MSE: 0.3109571039676666\n",
      "MAE: 0.3109570023313323\n",
      "RMSE: 5.111501227210346\n",
      "R-squared: 0.8140510452626257\n",
      "MAPE: 868.1747864750671%\n",
      "Model with H=6 and Nh=16:\n",
      "MSE: 0.5324184894561768\n",
      "MAE: 0.5324185571853143\n",
      "RMSE: 5.296428474192969\n",
      "R-squared: 0.5042065626877797\n",
      "MAPE: 663.3762542332272%\n",
      "Model with H=6 and Nh=32:\n",
      "MSE: 0.47341078519821167\n",
      "MAE: 0.47341080941624925\n",
      "RMSE: 5.259659127255284\n",
      "R-squared: 0.7553914189587467\n",
      "MAPE: 841.9008484581357%\n",
      "Model with H=6 and Nh=64:\n",
      "MSE: 0.3479732573032379\n",
      "MAE: 0.3479733640417766\n",
      "RMSE: 5.205123216694744\n",
      "R-squared: 0.7322958709182315\n",
      "MAPE: 835.5355309771655%\n",
      "Model with H=6 and Nh=128:\n",
      "MSE: 0.28507715463638306\n",
      "MAE: 0.28507719740570475\n",
      "RMSE: 5.110026819412018\n",
      "R-squared: 0.8173310877190064\n",
      "MAPE: 248.01431630417693%\n",
      "Model with H=6 and Nh=256:\n",
      "MSE: 0.2857619524002075\n",
      "MAE: 0.28576206159666534\n",
      "RMSE: 4.967784203372175\n",
      "R-squared: 0.8302917094861236\n",
      "MAPE: 573.0459312012474%\n",
      "Model with H=6 and Nh=512:\n",
      "MSE: 0.31252914667129517\n",
      "MAE: 0.31252908154946146\n",
      "RMSE: 5.172097793028162\n",
      "R-squared: 0.8215786323957156\n",
      "MAPE: 130.6642132945673%\n",
      "Model with H=7 and Nh=16:\n",
      "MSE: 0.5099577903747559\n",
      "MAE: 0.5099577726896329\n",
      "RMSE: 5.280075154445872\n",
      "R-squared: 0.7268129321038251\n",
      "MAPE: 492.46235655940654%\n",
      "Model with H=7 and Nh=32:\n",
      "MSE: 0.6552000045776367\n",
      "MAE: 0.6551998921912113\n",
      "RMSE: 6.1354807924196315\n",
      "R-squared: -32794.670391456304\n",
      "MAPE: 1627.3858560854922%\n",
      "Model with H=7 and Nh=64:\n",
      "MSE: 0.319475919008255\n",
      "MAE: 0.31947588194854154\n",
      "RMSE: 5.189582469030333\n",
      "R-squared: 0.8053831992578875\n",
      "MAPE: 385.50759112086814%\n",
      "Model with H=7 and Nh=128:\n",
      "MSE: 0.30655673146247864\n",
      "MAE: 0.3065568344205855\n",
      "RMSE: 5.127079403800854\n",
      "R-squared: -4.669175673286703\n",
      "MAPE: 828.3564474537586%\n",
      "Model with H=7 and Nh=256:\n",
      "MSE: 0.2783644199371338\n",
      "MAE: 0.27836445123206593\n",
      "RMSE: 4.891390425627333\n",
      "R-squared: 0.8197958693194143\n",
      "MAPE: 78.29381513431868%\n",
      "Model with H=7 and Nh=512:\n",
      "MSE: 0.29733365774154663\n",
      "MAE: 0.2973336318963693\n",
      "RMSE: 5.083597629273709\n",
      "R-squared: 0.8197064844329579\n",
      "MAPE: 286.53675891729677%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter\\dataset\\simulation_results_30.csv\")\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# # Scale input features to range [0, 1]\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Split the dataset into training, validation, and testing sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define custom scaling ranges for each input feature\n",
    "L_min, L_max = 30e-6, 2000e-6\n",
    "C_min, C_max = 20e-6, 1000e-6\n",
    "fsw_min, fsw_max = 20e3, 200e3\n",
    "\n",
    "# Apply Min-Max normalization separately for each input feature\n",
    "X_scaled = np.zeros_like(X, dtype=float)  # Initialize scaled data array\n",
    "\n",
    "# Scale L\n",
    "X_scaled[:, 0] = (X[:, 0] - L_min) / (L_max - L_min)\n",
    "\n",
    "# Scale C\n",
    "X_scaled[:, 1] = (X[:, 1] - C_min) / (C_max - C_min)\n",
    "\n",
    "# Scale fsw\n",
    "X_scaled[:, 2] = (X[:, 2] - fsw_min) / (fsw_max - fsw_min)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "# Initialize a dictionary to store the training history for each configuration\n",
    "histories = {}\n",
    "\n",
    "# Initialize dictionaries to store the evaluation metrics for each configuration\n",
    "mses = {}\n",
    "maes = {}\n",
    "rmses = {}\n",
    "r2_scores = {}\n",
    "mapes = {}\n",
    "\n",
    "# Define a list of different values for H and Nh to try\n",
    "# H_values = [1, 2, 3, 4, 5]  # Number of hidden layers\n",
    "# Nh_values = [32, 64, 128]  # Number of neurons per hidden layer\n",
    "# Additional values for H and Nh\n",
    "H_values = [1, 2, 3, 4, 5, 6, 7]  # Number of hidden layers\n",
    "# Nh_values = [16, 32, 64, 128, 256]  # Number of neurons per hidden layer\n",
    "# H_values = [1, 2, 3, 4, 5]  # Number of hidden layers\n",
    "Nh_values = [16, 32, 64, 128, 256, 512]  # Number of neurons per hidden layer\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over different values of H and Nh\n",
    "for H in H_values:\n",
    "    for Nh in Nh_values:\n",
    "        # Initialize a Sequential model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Add the first hidden layer with batch normalization\n",
    "        model.add(tf.keras.layers.Dense(Nh, activation='relu', input_shape=(3,)))  \n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # Add subsequent hidden layers with batch normalization\n",
    "        for _ in range(H - 1):\n",
    "            model.add(tf.keras.layers.Dense(Nh, activation='relu'))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        # Output layer (assuming it follows the hidden layers)\n",
    "        model.add(tf.keras.layers.Dense(6, activation='softplus'))  \n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "        lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "        # # Train the model and store the training history\n",
    "        # history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0, callbacks=[PrintEpochInfo()])\n",
    "        # Train the model with custom callback to print information at the end of each epoch and early stopping\n",
    "        history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val),    \n",
    "                    callbacks=[PrintEpochInfo(), lr_scheduler, early_stopping], verbose=1)\n",
    "        \n",
    "        histories[(H, Nh)] = history\n",
    "        \n",
    "        # Evaluate the model on the testing set\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the testing set and calculate MSE\n",
    "        mse = model.evaluate(X_test, y_test)\n",
    "        mses[(H, Nh)] = mse\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        maes[(H, Nh)] = mae\n",
    "\n",
    "        # Calculate Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        rmses[(H, Nh)] = rmse\n",
    "\n",
    "        # Calculate R-squared (R2) Score\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        r2_scores[(H, Nh)] = r2\n",
    "\n",
    "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "        mapes[(H, Nh)] = mape\n",
    "\n",
    "# Plot the validation loss for each configuration\n",
    "for (H, Nh), history in histories.items():\n",
    "    plt.plot(history.history['val_loss'], label=f'H={H}, Nh={Nh}')\n",
    "\n",
    "plt.title('Validation Loss for Different Model Configurations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the MSE for each configuration\n",
    "# Print the evaluation metrics for each configuration\n",
    "for (H, Nh), mse in mses.items():\n",
    "    print(f'Model with H={H} and Nh={Nh}:')\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'MAE: {maes[(H, Nh)]}')\n",
    "    print(f'RMSE: {rmses[(H, Nh)]}')\n",
    "    print(f'R-squared: {r2_scores[(H, Nh)]}')\n",
    "    print(f'MAPE: {mapes[(H, Nh)]}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Combination (7, 256): Overall rank 1\n",
      "2. Combination (5, 256): Overall rank 2\n",
      "3. Combination (4, 512): Overall rank 3\n",
      "4. Combination (3, 256): Overall rank 4\n",
      "5. Combination (6, 256): Overall rank 5\n",
      "6. Combination (6, 128): Overall rank 6\n",
      "7. Combination (7, 512): Overall rank 7\n",
      "8. Combination (6, 512): Overall rank 8\n",
      "9. Combination (4, 128): Overall rank 9\n",
      "10. Combination (2, 512): Overall rank 10\n",
      "11. Combination (4, 256): Overall rank 11\n",
      "12. Combination (3, 512): Overall rank 12\n",
      "13. Combination (5, 512): Overall rank 13\n",
      "14. Combination (3, 128): Overall rank 14\n",
      "15. Combination (7, 64): Overall rank 15\n",
      "16. Combination (2, 256): Overall rank 16\n",
      "17. Combination (3, 64): Overall rank 17\n",
      "18. Combination (4, 64): Overall rank 18\n",
      "19. Combination (7, 128): Overall rank 19\n",
      "20. Combination (5, 64): Overall rank 20\n",
      "21. Combination (2, 128): Overall rank 21\n",
      "22. Combination (5, 128): Overall rank 22\n",
      "23. Combination (5, 32): Overall rank 23\n",
      "24. Combination (3, 32): Overall rank 24\n",
      "25. Combination (4, 16): Overall rank 25\n",
      "26. Combination (6, 64): Overall rank 26\n",
      "27. Combination (7, 16): Overall rank 27\n",
      "28. Combination (6, 32): Overall rank 28\n",
      "29. Combination (2, 64): Overall rank 29\n",
      "30. Combination (6, 16): Overall rank 30\n",
      "31. Combination (4, 32): Overall rank 31\n",
      "32. Combination (1, 256): Overall rank 32\n",
      "33. Combination (1, 512): Overall rank 33\n",
      "34. Combination (2, 32): Overall rank 34\n",
      "35. Combination (1, 128): Overall rank 35\n",
      "36. Combination (3, 16): Overall rank 36\n",
      "37. Combination (5, 16): Overall rank 37\n",
      "38. Combination (1, 64): Overall rank 38\n",
      "39. Combination (2, 16): Overall rank 39\n",
      "40. Combination (1, 32): Overall rank 40\n",
      "41. Combination (7, 32): Overall rank 41\n",
      "42. Combination (1, 16): Overall rank 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Sort configurations based on MSE in ascending order\n",
    "sorted_configs_mse = sorted(mses.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on MAE in ascending order\n",
    "sorted_configs_mae = sorted(maes.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on RMSE in ascending order\n",
    "sorted_configs_rmse = sorted(rmses.items(), key=lambda x: x[1])\n",
    "\n",
    "# Sort configurations based on R-squared in descending order\n",
    "sorted_configs_r2 = sorted(r2_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort configurations based on MAPE in ascending order\n",
    "sorted_configs_mape = sorted(mapes.items(), key=lambda x: x[1])\n",
    "\n",
    "# Rank each combination based on its position in the sorted lists\n",
    "ranking = {}\n",
    "for i, (config, _) in enumerate(sorted_configs_mse):\n",
    "    ranking[config] = i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_mae):\n",
    "    # ranking.setdefault(config, 0)\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_rmse):\n",
    "    # ranking.setdefault(config, 0)\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_r2):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "for i, (config, _) in enumerate(sorted_configs_mape):\n",
    "    ranking[config] += i + 1\n",
    "\n",
    "# Sort configurations based on their overall ranking\n",
    "sorted_ranking = sorted(ranking.items(), key=lambda x: x[1])\n",
    "\n",
    "# Display the rankings\n",
    "for rank, (config, _) in enumerate(sorted_ranking, 1):\n",
    "    print(f'{rank}. Combination {config}: Overall rank {rank}')\n",
    "\n",
    "\n",
    "\n",
    "# # Create the 'ranking' folder if it doesn't exist\n",
    "# folder_path = 'ranking'\n",
    "# if not os.path.exists(folder_path):\n",
    "#     os.makedirs(folder_path)\n",
    "\n",
    "# # Create a DataFrame for the rankings\n",
    "# rankings_df = pd.DataFrame(sorted_ranking, columns=['Combination', 'Overall_Rank'])\n",
    "\n",
    "# # Define the file path for saving the CSV file\n",
    "# file_path = os.path.join(folder_path, 'rankings_7.csv')\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# rankings_df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(f\"Rankings saved to '{file_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try spesific N and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 823us/step\n",
      "Predictions:\n",
      "[[9.8065503e-02 1.9379230e+00 3.1266792e+00 3.1070087e+00 1.5493479e-02\n",
      "  9.3392313e-01]\n",
      " [2.5161378e+00 2.3007963e+01 5.2021902e-02 4.9977858e-02 4.9360591e-04\n",
      "  1.8853541e-02]\n",
      " [5.2074087e-01 9.7542324e+00 3.4731293e+00 3.4720724e+00 3.0186672e-03\n",
      "  2.2565408e+00]\n",
      " [3.4473437e-01 3.3883892e+01 8.3637438e+00 8.4030476e+00 5.0582858e-03\n",
      "  5.1808867e+00]\n",
      " [3.3670667e-01 2.7935392e+01 5.9978909e+00 5.9636526e+00 6.0081785e-03\n",
      "  3.9974644e+00]]\n",
      "True Values:\n",
      "[[8.24785561e-02 1.66608588e+00 3.20658853e+00 3.20658853e+00\n",
      "  1.78710824e-02 9.70550404e-01]\n",
      " [2.91178140e+00 2.29786406e+01 2.41527236e-01 2.41527236e-01\n",
      "  3.22857138e-04 7.06838663e-02]\n",
      " [5.65812618e-01 9.54536162e+00 3.61795960e+00 3.61795960e+00\n",
      "  3.17092808e-03 2.32065136e+00]\n",
      " [2.90574366e-01 3.40931078e+01 8.29842095e+00 8.29842095e+00\n",
      "  9.66771510e-03 5.39112881e+00]\n",
      " [2.20010220e-01 2.78614891e+01 6.05613890e+00 6.05613890e+00\n",
      "  7.62088483e-03 4.04490585e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Specify the combination you want to call\n",
    "H = 7\n",
    "Nh = 256\n",
    "\n",
    "# Retrieve the model for the specified combination\n",
    "selected_model = histories[(H, Nh)].model\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "predictions = selected_model.predict(X_test)\n",
    "\n",
    "# Print the first few predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Print the corresponding true values\n",
    "print(\"True Values:\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "169/175 [===========================>..] - ETA: 0s - loss: 1.2480Epoch 1/50, Loss: 1.231235146522522, Val Loss: 1.8220438957214355\n",
      "175/175 [==============================] - 1s 2ms/step - loss: 1.2312 - val_loss: 1.8220\n",
      "Epoch 2/50\n",
      "122/175 [===================>..........] - ETA: 0s - loss: 0.6972Epoch 2/50, Loss: 0.7014151215553284, Val Loss: 0.9145978689193726\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.7014 - val_loss: 0.9146\n",
      "Epoch 3/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.5131Epoch 3/50, Loss: 0.5160526037216187, Val Loss: 0.46311748027801514\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.4631\n",
      "Epoch 4/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.4365Epoch 4/50, Loss: 0.4438672661781311, Val Loss: 0.3551362156867981\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.3551\n",
      "Epoch 5/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.4051Epoch 5/50, Loss: 0.39809802174568176, Val Loss: 0.27421027421951294\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.2742\n",
      "Epoch 6/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3905Epoch 6/50, Loss: 0.3696433901786804, Val Loss: 0.23357708752155304\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.2336\n",
      "Epoch 7/50\n",
      "120/175 [===================>..........] - ETA: 0s - loss: 0.3785Epoch 7/50, Loss: 0.3614891767501831, Val Loss: 0.2676929831504822\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.2677\n",
      "Epoch 8/50\n",
      "122/175 [===================>..........] - ETA: 0s - loss: 0.3688Epoch 8/50, Loss: 0.3536348044872284, Val Loss: 0.28044015169143677\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.2804\n",
      "Epoch 9/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.3813Epoch 9/50, Loss: 0.38275325298309326, Val Loss: 0.22241562604904175\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.2224\n",
      "Epoch 10/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3469Epoch 10/50, Loss: 0.3367161452770233, Val Loss: 0.20399343967437744\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.2040\n",
      "Epoch 11/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.2917Epoch 11/50, Loss: 0.30450257658958435, Val Loss: 0.26718461513519287\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3045 - val_loss: 0.2672\n",
      "Epoch 12/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3166Epoch 12/50, Loss: 0.3127875030040741, Val Loss: 0.22523555159568787\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2252\n",
      "Epoch 13/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3288Epoch 13/50, Loss: 0.30825453996658325, Val Loss: 0.17230653762817383\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.1723\n",
      "Epoch 14/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3460Epoch 14/50, Loss: 0.3198173940181732, Val Loss: 0.17088446021080017\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.1709\n",
      "Epoch 15/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.3205Epoch 15/50, Loss: 0.3197115659713745, Val Loss: 0.16297918558120728\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.1630\n",
      "Epoch 16/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.2982Epoch 16/50, Loss: 0.30214208364486694, Val Loss: 0.39342427253723145\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.3934\n",
      "Epoch 17/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.2864Epoch 17/50, Loss: 0.30565494298934937, Val Loss: 0.24115633964538574\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.2412\n",
      "Epoch 18/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3129Epoch 18/50, Loss: 0.310596227645874, Val Loss: 0.3759227693080902\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3759\n",
      "Epoch 19/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.3164Epoch 19/50, Loss: 0.32059040665626526, Val Loss: 0.187202587723732\n",
      "175/175 [==============================] - 0s 986us/step - loss: 0.3206 - val_loss: 0.1872\n",
      "Epoch 20/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.3113Epoch 20/50, Loss: 0.29898276925086975, Val Loss: 0.14650674164295197\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2990 - val_loss: 0.1465\n",
      "Epoch 21/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2741Epoch 21/50, Loss: 0.2615576684474945, Val Loss: 0.13898125290870667\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2616 - val_loss: 0.1390\n",
      "Epoch 22/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2843Epoch 22/50, Loss: 0.28860586881637573, Val Loss: 0.3281770944595337\n",
      "175/175 [==============================] - 0s 974us/step - loss: 0.2886 - val_loss: 0.3282\n",
      "Epoch 23/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.2889Epoch 23/50, Loss: 0.297443151473999, Val Loss: 0.15489579737186432\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2974 - val_loss: 0.1549\n",
      "Epoch 24/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2881Epoch 24/50, Loss: 0.2703944444656372, Val Loss: 0.1906571239233017\n",
      "175/175 [==============================] - 0s 997us/step - loss: 0.2704 - val_loss: 0.1907\n",
      "Epoch 25/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2854Epoch 25/50, Loss: 0.27868422865867615, Val Loss: 0.16762082278728485\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2787 - val_loss: 0.1676\n",
      "Epoch 26/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.2875Epoch 26/50, Loss: 0.27255505323410034, Val Loss: 0.1672709882259369\n",
      "175/175 [==============================] - 0s 980us/step - loss: 0.2726 - val_loss: 0.1673\n",
      "Epoch 27/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2720Epoch 27/50, Loss: 0.2735123038291931, Val Loss: 0.13589221239089966\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2735 - val_loss: 0.1359\n",
      "Epoch 28/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2615Epoch 28/50, Loss: 0.26962050795555115, Val Loss: 0.3326718807220459\n",
      "175/175 [==============================] - 0s 979us/step - loss: 0.2696 - val_loss: 0.3327\n",
      "Epoch 29/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.3202Epoch 29/50, Loss: 0.30579763650894165, Val Loss: 0.1902272254228592\n",
      "175/175 [==============================] - 0s 980us/step - loss: 0.3058 - val_loss: 0.1902\n",
      "Epoch 30/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2885Epoch 30/50, Loss: 0.28246670961380005, Val Loss: 0.2069484442472458\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2825 - val_loss: 0.2069\n",
      "Epoch 31/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2955Epoch 31/50, Loss: 0.2838130593299866, Val Loss: 0.22748403251171112\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2838 - val_loss: 0.2275\n",
      "Epoch 32/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.3042Epoch 32/50, Loss: 0.28965553641319275, Val Loss: 0.15516367554664612\n",
      "175/175 [==============================] - 0s 983us/step - loss: 0.2897 - val_loss: 0.1552\n",
      "Epoch 33/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2516Epoch 33/50, Loss: 0.2748776972293854, Val Loss: 0.14020894467830658\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2749 - val_loss: 0.1402\n",
      "Epoch 34/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2601Epoch 34/50, Loss: 0.24987231194972992, Val Loss: 0.15120172500610352\n",
      "175/175 [==============================] - 0s 994us/step - loss: 0.2499 - val_loss: 0.1512\n",
      "Epoch 35/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2724Epoch 35/50, Loss: 0.27415692806243896, Val Loss: 0.19605940580368042\n",
      "175/175 [==============================] - 0s 997us/step - loss: 0.2742 - val_loss: 0.1961\n",
      "Epoch 36/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2551Epoch 36/50, Loss: 0.24621020257472992, Val Loss: 0.1122371181845665\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2462 - val_loss: 0.1122\n",
      "Epoch 37/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2515Epoch 37/50, Loss: 0.25234130024909973, Val Loss: 0.15384754538536072\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2523 - val_loss: 0.1538\n",
      "Epoch 38/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2612Epoch 38/50, Loss: 0.2554391622543335, Val Loss: 0.13795459270477295\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2554 - val_loss: 0.1380\n",
      "Epoch 39/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2401Epoch 39/50, Loss: 0.23901231586933136, Val Loss: 0.09406702220439911\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2390 - val_loss: 0.0941\n",
      "Epoch 40/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2665Epoch 40/50, Loss: 0.26533034443855286, Val Loss: 0.13237258791923523\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2653 - val_loss: 0.1324\n",
      "Epoch 41/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2626Epoch 41/50, Loss: 0.2642378807067871, Val Loss: 0.1104949414730072\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2642 - val_loss: 0.1105\n",
      "Epoch 42/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.2514Epoch 42/50, Loss: 0.2529561221599579, Val Loss: 0.1098017767071724\n",
      "175/175 [==============================] - 0s 998us/step - loss: 0.2530 - val_loss: 0.1098\n",
      "Epoch 43/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2365Epoch 43/50, Loss: 0.2350158840417862, Val Loss: 0.0780756026506424\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2350 - val_loss: 0.0781\n",
      "Epoch 44/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2365Epoch 44/50, Loss: 0.23952028155326843, Val Loss: 0.16567617654800415\n",
      "175/175 [==============================] - 0s 992us/step - loss: 0.2395 - val_loss: 0.1657\n",
      "Epoch 45/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2372Epoch 45/50, Loss: 0.2383454293012619, Val Loss: 0.1360943764448166\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2383 - val_loss: 0.1361\n",
      "Epoch 46/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2561Epoch 46/50, Loss: 0.24771438539028168, Val Loss: 0.10556324571371078\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2477 - val_loss: 0.1056\n",
      "Epoch 47/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2387Epoch 47/50, Loss: 0.26340335607528687, Val Loss: 0.17849047482013702\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2634 - val_loss: 0.1785\n",
      "Epoch 48/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2531Epoch 48/50, Loss: 0.255118191242218, Val Loss: 0.11796895414590836\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2551 - val_loss: 0.1180\n",
      "Epoch 49/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2251Epoch 49/50, Loss: 0.228265181183815, Val Loss: 0.16878430545330048\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2283 - val_loss: 0.1688\n",
      "Epoch 50/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2355Epoch 50/50, Loss: 0.23975418508052826, Val Loss: 0.2517523169517517\n",
      "175/175 [==============================] - 0s 986us/step - loss: 0.2398 - val_loss: 0.2518\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter\\dataset\\modified_8000_dataset.csv\")\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Scale input features to range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 1\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),         \n",
    "    tf.keras.layers.Dense(6, activation='softplus')                               # Output layer with 6 neurons\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 1\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),         \n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),         \n",
    "#     tf.keras.layers.Dense(6, activation='softplus')                               # Output layer with 6 neurons\n",
    "# ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with custom callback to print information at the end of each epoch\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[PrintEpochInfo()], verbose=1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 606us/step - loss: 0.1632\n",
      "Mean Squared Error on Testing Set: 0.16320562362670898\n",
      "38/38 [==============================] - 0s 499us/step\n",
      "Predictions:\n",
      "[[0.20654827 6.483947   2.6345077  2.6322153  0.01708872 1.3133281 ]\n",
      " [0.12445674 4.282261   3.5189834  3.520649   0.00829306 1.0214925 ]\n",
      " [0.14707306 6.546033   3.3875675  3.4100778  0.00850959 1.2893308 ]\n",
      " [0.06795892 4.3753366  2.3814638  2.3742485  0.01889426 2.3163981 ]\n",
      " [0.06067272 0.20301414 2.079679   2.104424   0.14704187 2.053334  ]]\n",
      "True Values:\n",
      "[[0.26001435 4.23264405 2.74299141 2.74299141 0.01217656 1.29224261]\n",
      " [0.19503469 3.32206237 3.59940991 3.59940991 0.00902757 0.95760415]\n",
      " [0.23933468 4.23420079 3.51435724 3.51435724 0.00733677 1.291914  ]\n",
      " [0.08042573 4.24613856 2.40471154 2.40471154 0.02217949 2.34150863]\n",
      " [0.07897327 0.58997409 2.02413572 2.02413572 0.16765618 2.15456352]]\n",
      "Mean Absolute Error (MAE): 0.13914263041641098\n",
      "Root Mean Squared Error (RMSE): 0.4039871578126952\n",
      "R-squared (R2) Score: 0.9244097104016503\n",
      "Mean Absolute Percentage Error (MAPE): 15.323075467441463%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the first few predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Print the corresponding true values\n",
    "print(\"True Values:\")\n",
    "print(y_test[:5])\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate R-squared (R2) Score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'R-squared (R2) Score: {r2}')\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save(\"ripples_prediction_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
