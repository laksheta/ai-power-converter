{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ga_ml as ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your scaling function\n",
    "def custom_scaling(input_values):\n",
    "    input_values_scaled = np.zeros_like(input_values, dtype=float)  # Initialize scaled data array\n",
    "    input_values_scaled[:, 0] = (input_values[:, 0] - L_min) / (L_max - L_min)  # Scale L\n",
    "    input_values_scaled[:, 1] = (input_values[:, 1] - C_min) / (C_max - C_min)  # Scale C\n",
    "    input_values_scaled[:, 2] = (input_values[:, 2] - fsw_min) / (fsw_max - fsw_min)  # Scale fsw\n",
    "    return input_values_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fitness(individual):\n",
    "\n",
    "    N = individual[0]\n",
    "    H = individual[1]\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(3,)))  # Scalarization layer\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(H):\n",
    "        model.add(tf.keras.layers.Dense(N, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softplus'))  # Output layer with 6 neurons\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "    # Train the model with given hyperparameters\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), \n",
    "                        callbacks=[PrintEpochInfo(), lr_scheduler, early_stopping], verbose=0)\n",
    "    \n",
    "    # Evaluate the model on validation data to compute the fitness value\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    # Fitness value is the inverse of the validation loss (lower loss is better)\n",
    "    fitness = 1 / (val_loss + 1e-10)  # Adding a small value to prevent division by zero\n",
    "    \n",
    "    return fitness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search N and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter-1\\dataset\\simulation_results_new.csv\")\n",
    "\n",
    "# Define custom scaling ranges for each input feature\n",
    "L_min, L_max = 30e-6, 2000e-6\n",
    "C_min, C_max = 20e-6, 1000e-6\n",
    "fsw_min, fsw_max = 20e3, 200e3\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Apply custom scaling\n",
    "X_scaled = custom_scaling(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Assuming you have already defined your machine learning model and dataset\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32   7]\n",
      " [512   3]\n",
      " [512   7]\n",
      " [128   8]\n",
      " [ 64   5]\n",
      " [ 32   6]\n",
      " [256   5]\n",
      " [ 32   6]]\n"
     ]
    }
   ],
   "source": [
    "# Define the general values for N\n",
    "neuron_values = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "\n",
    "# Define the range for H\n",
    "H_min, H_max = 2, 8\n",
    "\n",
    "mutation_ranges = [neuron_values,   # Possible values for N (number of neurons)\n",
    "                   (H_min, H_max)]              # Range for H (number of layers)\n",
    "\n",
    "# Define the size of the population\n",
    "sol_per_pop = 8#12\n",
    "num_parents_mating = 4\n",
    "pop_size = (sol_per_pop, 2)  # We have 3 variables: N, C, fsw\n",
    "num_generations = 5\n",
    "\n",
    "# Generate random values for N and H\n",
    "N_values = np.random.choice(neuron_values, size=(sol_per_pop, 1))  # Random N values\n",
    "H_values = np.random.randint(H_min, H_max + 1, size=(sol_per_pop, 1))  # Random H values\n",
    "\n",
    "# Combine N and H values into the initial population\n",
    "initial_population = np.column_stack((N_values, H_values))\n",
    "\n",
    "print(initial_population)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation :  0\n",
      "Epoch 1/100, Loss: 0.7544298768043518, Val Loss: 0.44074857234954834\n",
      "Epoch 2/100, Loss: 0.44530659914016724, Val Loss: 0.3406551480293274\n",
      "Epoch 3/100, Loss: 0.3852047622203827, Val Loss: 0.30399593710899353\n",
      "Epoch 4/100, Loss: 0.34886518120765686, Val Loss: 0.2627183794975281\n",
      "Epoch 5/100, Loss: 0.31506162881851196, Val Loss: 0.24494072794914246\n",
      "Epoch 6/100, Loss: 0.2823949456214905, Val Loss: 0.22520685195922852\n",
      "Epoch 7/100, Loss: 0.27202078700065613, Val Loss: 0.1980844885110855\n",
      "Epoch 8/100, Loss: 0.2573493421077728, Val Loss: 0.17867957055568695\n",
      "Epoch 9/100, Loss: 0.2481323927640915, Val Loss: 0.1707584112882614\n",
      "Epoch 10/100, Loss: 0.2423565685749054, Val Loss: 0.1746949404478073\n",
      "Epoch 11/100, Loss: 0.23595945537090302, Val Loss: 0.15342150628566742\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 11: early stopping\n",
      "127/127 [==============================] - 0s 610us/step\n",
      "Epoch 1/100, Loss: 0.9589999914169312, Val Loss: 0.49539080262184143\n",
      "Epoch 2/100, Loss: 0.35977840423583984, Val Loss: 0.27094581723213196\n",
      "Epoch 3/100, Loss: 0.29324230551719666, Val Loss: 0.2541549801826477\n",
      "Epoch 4/100, Loss: 0.2853314280509949, Val Loss: 0.2580072283744812\n",
      "Epoch 5/100, Loss: 0.26962023973464966, Val Loss: 0.29298919439315796\n",
      "Epoch 6/100, Loss: 0.2630920112133026, Val Loss: 0.23870940506458282\n",
      "Epoch 7/100, Loss: 0.2544749975204468, Val Loss: 0.2197452187538147\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 7: early stopping\n",
      "127/127 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100, Loss: 1.9790246486663818, Val Loss: 0.7351979613304138\n",
      "Epoch 2/100, Loss: 0.4600461423397064, Val Loss: 0.31778189539909363\n",
      "Epoch 3/100, Loss: 0.3485332131385803, Val Loss: 0.37971433997154236\n",
      "Epoch 4/100, Loss: 0.3224349915981293, Val Loss: 0.322658896446228\n",
      "Epoch 5/100, Loss: 0.31042125821113586, Val Loss: 0.31558388471603394\n",
      "Epoch 6/100, Loss: 0.32913175225257874, Val Loss: 0.318146288394928\n",
      "Epoch 7/100, Loss: 0.3307547867298126, Val Loss: 0.31782886385917664\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 7: early stopping\n",
      "127/127 [==============================] - 0s 3ms/step\n",
      "Epoch 1/100, Loss: 1.175176978111267, Val Loss: 0.809630811214447\n",
      "Epoch 2/100, Loss: 0.6754443645477295, Val Loss: 0.4788679778575897\n",
      "Epoch 3/100, Loss: 0.4350493550300598, Val Loss: 0.3270346224308014\n",
      "Epoch 4/100, Loss: 0.34518882632255554, Val Loss: 0.263756662607193\n",
      "Epoch 5/100, Loss: 0.30073782801628113, Val Loss: 0.2317507416009903\n",
      "Epoch 6/100, Loss: 0.2792198359966278, Val Loss: 0.27403903007507324\n",
      "Epoch 7/100, Loss: 0.26708731055259705, Val Loss: 0.21474780142307281\n",
      "Epoch 8/100, Loss: 0.2598034143447876, Val Loss: 0.21502481400966644\n",
      "Epoch 9/100, Loss: 0.2629737854003906, Val Loss: 0.22154825925827026\n",
      "Epoch 10/100, Loss: 0.2618541121482849, Val Loss: 0.2500655949115753\n",
      "Epoch 11/100, Loss: 0.24026863276958466, Val Loss: 0.21085408329963684\n",
      "Epoch 12/100, Loss: 0.23859842121601105, Val Loss: 0.17956851422786713\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 12: early stopping\n",
      "127/127 [==============================] - 0s 734us/step\n",
      "Epoch 1/100, Loss: 0.7218909859657288, Val Loss: 0.41475650668144226\n",
      "Epoch 2/100, Loss: 0.4430602192878723, Val Loss: 0.33406588435173035\n",
      "Epoch 3/100, Loss: 0.36709529161453247, Val Loss: 0.2512091100215912\n",
      "Epoch 4/100, Loss: 0.3101920187473297, Val Loss: 0.225071981549263\n",
      "Epoch 5/100, Loss: 0.28590649366378784, Val Loss: 0.19076856970787048\n",
      "Epoch 6/100, Loss: 0.26286908984184265, Val Loss: 0.17865891754627228\n",
      "Epoch 7/100, Loss: 0.24623490869998932, Val Loss: 0.17028763890266418\n",
      "Epoch 8/100, Loss: 0.23841698467731476, Val Loss: 0.1672980636358261\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 8: early stopping\n",
      "127/127 [==============================] - 0s 585us/step\n",
      "Epoch 1/100, Loss: 0.7645059823989868, Val Loss: 0.4262758791446686\n",
      "Epoch 2/100, Loss: 0.4216242730617523, Val Loss: 0.30415078997612\n",
      "Epoch 3/100, Loss: 0.357438325881958, Val Loss: 0.29879841208457947\n",
      "Epoch 4/100, Loss: 0.3253500461578369, Val Loss: 0.24696582555770874\n",
      "Epoch 5/100, Loss: 0.2926822006702423, Val Loss: 0.20670859515666962\n",
      "Epoch 6/100, Loss: 0.2740577757358551, Val Loss: 0.20170363783836365\n",
      "Epoch 7/100, Loss: 0.26078638434410095, Val Loss: 0.17603528499603271\n",
      "Epoch 8/100, Loss: 0.25169819593429565, Val Loss: 0.17956116795539856\n",
      "Epoch 9/100, Loss: 0.2463076114654541, Val Loss: 0.17091110348701477\n",
      "Epoch 10/100, Loss: 0.2418530434370041, Val Loss: 0.17049171030521393\n",
      "Epoch 11/100, Loss: 0.23726563155651093, Val Loss: 0.15554052591323853\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 11: early stopping\n",
      "127/127 [==============================] - 0s 576us/step\n",
      "Epoch 1/100, Loss: 1.1030443906784058, Val Loss: 0.6910334229469299\n",
      "Epoch 2/100, Loss: 0.4713124632835388, Val Loss: 0.29490235447883606\n",
      "Epoch 3/100, Loss: 0.3180580735206604, Val Loss: 0.2808944582939148\n",
      "Epoch 4/100, Loss: 0.2808986306190491, Val Loss: 0.2754526138305664\n",
      "Epoch 5/100, Loss: 0.2663024365901947, Val Loss: 0.2626185119152069\n",
      "Epoch 6/100, Loss: 0.26891180872917175, Val Loss: 0.2216608077287674\n",
      "Epoch 7/100, Loss: 0.26255831122398376, Val Loss: 0.21208728849887848\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 7: early stopping\n",
      "127/127 [==============================] - 0s 742us/step\n",
      "Epoch 1/100, Loss: 0.7347555756568909, Val Loss: 0.4160884618759155\n",
      "Epoch 2/100, Loss: 0.4300450086593628, Val Loss: 0.33865320682525635\n",
      "Epoch 3/100, Loss: 0.357991099357605, Val Loss: 0.2744683027267456\n",
      "Epoch 4/100, Loss: 0.3242337703704834, Val Loss: 0.2333952635526657\n",
      "Epoch 5/100, Loss: 0.2935398817062378, Val Loss: 0.20253805816173553\n",
      "Epoch 6/100, Loss: 0.27899301052093506, Val Loss: 0.18138572573661804\n",
      "Epoch 7/100, Loss: 0.2661808133125305, Val Loss: 0.2147260159254074\n",
      "Epoch 8/100, Loss: 0.25766777992248535, Val Loss: 0.1700054407119751\n",
      "Epoch 9/100, Loss: 0.2511185109615326, Val Loss: 0.16775590181350708\n",
      "Epoch 10/100, Loss: 0.24160732328891754, Val Loss: 0.17538556456565857\n",
      "Epoch 11/100, Loss: 0.23622387647628784, Val Loss: 0.15265268087387085\n",
      "Epoch 12/100, Loss: 0.23404870927333832, Val Loss: 0.136552631855011\n",
      "Epoch 13/100, Loss: 0.2286584973335266, Val Loss: 0.15383434295654297\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 13: early stopping\n",
      "127/127 [==============================] - 0s 618us/step\n",
      "Fitness\n",
      "[4.31082069 3.03238205 2.82118203 4.27626996 5.86949137 5.46303099\n",
      " 3.35373171 6.66157754]\n",
      "Best result :  2.821182031557335\n",
      "Best solution :  [512   7]\n",
      "Best fitness :  2.821182031557335\n",
      "Parents\n",
      "[[512.   7.]\n",
      " [512.   3.]\n",
      " [256.   5.]\n",
      " [128.   8.]]\n",
      "Crossover\n",
      "[[512.   3.   3.]\n",
      " [512.   5.   5.]\n",
      " [256.   8.   8.]\n",
      " [128.   7.   7.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mutation_ranges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(offspring_crossover)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Adding some variations to the offspring using mutation.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# offspring_mutation = ga.mutation(offspring_crossover, 2)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m offspring_mutation \u001b[38;5;241m=\u001b[39m ga\u001b[38;5;241m.\u001b[39mmutation(offspring_crossover, \u001b[43mmutation_ranges\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(offspring_mutation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mutation_ranges' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Genetic Algorithm optimization\n",
    "best_outputs = []\n",
    "best_solutions = []  # Store the best solution for each generation\n",
    "best_fitnesses = []  # Store the best fitness for each generation\n",
    "new_population = initial_population\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    print(\"Generation : \", generation)\n",
    "    \n",
    "    # Measuring the fitness of each chromosome in the population.\n",
    "    fitness_values = []\n",
    "    delta_IL_percent_values = []\n",
    "    delta_Vo_percent_values = []\n",
    "    for individual in new_population:\n",
    "        fitness_value = calculate_fitness(individual)\n",
    "        fitness_values.append(fitness_value)\n",
    "\n",
    "    fitness = np.array(fitness_values)\n",
    "\n",
    "    \n",
    "    print(\"Fitness\")\n",
    "    print(fitness)\n",
    "\n",
    "    best_outputs.append(np.min(fitness))\n",
    "    print(\"Best result : \", np.min(fitness))\n",
    "\n",
    "    # Find the index of the best solution in this generation\n",
    "    best_solution_idx = np.argmin(fitness)\n",
    "    best_solution = new_population[best_solution_idx]\n",
    "    best_solutions.append(best_solution)\n",
    "    best_fitness = fitness[best_solution_idx]\n",
    "    best_fitnesses.append(best_fitness)\n",
    "    print(\"Best solution : \", best_solution)\n",
    "    print(\"Best fitness : \", best_fitness)\n",
    "\n",
    "    # Selecting the best parents in the population for mating.\n",
    "    parents = ga.select_mating_pool(new_population, fitness, num_parents_mating)\n",
    "    print(\"Parents\")\n",
    "    print(parents)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = ga.crossover(parents, offspring_size=(pop_size[0]-parents.shape[0], 3))\n",
    "    print(\"Crossover\")\n",
    "    print(offspring_crossover)\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    # offspring_mutation = ga.mutation(offspring_crossover, 2)\n",
    "    offspring_mutation = ga.mutation(offspring_crossover, mutation_ranges, 1)\n",
    "    print(\"Mutation\")\n",
    "    print(offspring_mutation)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    new_population[0:parents.shape[0], :] = parents\n",
    "    new_population[parents.shape[0]:, :] = offspring_mutation\n",
    "\n",
    "# Getting the best solution after iterating finishing all generations.\n",
    "# At first, the fitness is calculated for each solution in the final generation.\n",
    "fitness_values = []\n",
    "\n",
    "for individual in new_population:\n",
    "    fitness_value= calculate_fitness(individual)\n",
    "    fitness_values.append(fitness_value)\n",
    "\n",
    "fitness = np.array(fitness_values)\n",
    "\n",
    "\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "best_match_idx = np.where(fitness == np.min(fitness))\n",
    "\n",
    "print(\"Best solution : \", new_population[best_match_idx, :])\n",
    "print(\"Best solution fitness : \", fitness[best_match_idx])\n",
    "\n",
    "\n",
    "plt.plot(best_outputs)\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "169/175 [===========================>..] - ETA: 0s - loss: 1.2480Epoch 1/50, Loss: 1.231235146522522, Val Loss: 1.8220438957214355\n",
      "175/175 [==============================] - 1s 2ms/step - loss: 1.2312 - val_loss: 1.8220\n",
      "Epoch 2/50\n",
      "122/175 [===================>..........] - ETA: 0s - loss: 0.6972Epoch 2/50, Loss: 0.7014151215553284, Val Loss: 0.9145978689193726\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.7014 - val_loss: 0.9146\n",
      "Epoch 3/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.5131Epoch 3/50, Loss: 0.5160526037216187, Val Loss: 0.46311748027801514\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.4631\n",
      "Epoch 4/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.4365Epoch 4/50, Loss: 0.4438672661781311, Val Loss: 0.3551362156867981\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.3551\n",
      "Epoch 5/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.4051Epoch 5/50, Loss: 0.39809802174568176, Val Loss: 0.27421027421951294\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.2742\n",
      "Epoch 6/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3905Epoch 6/50, Loss: 0.3696433901786804, Val Loss: 0.23357708752155304\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.2336\n",
      "Epoch 7/50\n",
      "120/175 [===================>..........] - ETA: 0s - loss: 0.3785Epoch 7/50, Loss: 0.3614891767501831, Val Loss: 0.2676929831504822\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.2677\n",
      "Epoch 8/50\n",
      "122/175 [===================>..........] - ETA: 0s - loss: 0.3688Epoch 8/50, Loss: 0.3536348044872284, Val Loss: 0.28044015169143677\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.2804\n",
      "Epoch 9/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.3813Epoch 9/50, Loss: 0.38275325298309326, Val Loss: 0.22241562604904175\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.2224\n",
      "Epoch 10/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3469Epoch 10/50, Loss: 0.3367161452770233, Val Loss: 0.20399343967437744\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.2040\n",
      "Epoch 11/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.2917Epoch 11/50, Loss: 0.30450257658958435, Val Loss: 0.26718461513519287\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3045 - val_loss: 0.2672\n",
      "Epoch 12/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3166Epoch 12/50, Loss: 0.3127875030040741, Val Loss: 0.22523555159568787\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2252\n",
      "Epoch 13/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3288Epoch 13/50, Loss: 0.30825453996658325, Val Loss: 0.17230653762817383\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.1723\n",
      "Epoch 14/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3460Epoch 14/50, Loss: 0.3198173940181732, Val Loss: 0.17088446021080017\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.1709\n",
      "Epoch 15/50\n",
      "121/175 [===================>..........] - ETA: 0s - loss: 0.3205Epoch 15/50, Loss: 0.3197115659713745, Val Loss: 0.16297918558120728\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.1630\n",
      "Epoch 16/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.2982Epoch 16/50, Loss: 0.30214208364486694, Val Loss: 0.39342427253723145\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.3934\n",
      "Epoch 17/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.2864Epoch 17/50, Loss: 0.30565494298934937, Val Loss: 0.24115633964538574\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.2412\n",
      "Epoch 18/50\n",
      "123/175 [====================>.........] - ETA: 0s - loss: 0.3129Epoch 18/50, Loss: 0.310596227645874, Val Loss: 0.3759227693080902\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.3106 - val_loss: 0.3759\n",
      "Epoch 19/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.3164Epoch 19/50, Loss: 0.32059040665626526, Val Loss: 0.187202587723732\n",
      "175/175 [==============================] - 0s 986us/step - loss: 0.3206 - val_loss: 0.1872\n",
      "Epoch 20/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.3113Epoch 20/50, Loss: 0.29898276925086975, Val Loss: 0.14650674164295197\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2990 - val_loss: 0.1465\n",
      "Epoch 21/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2741Epoch 21/50, Loss: 0.2615576684474945, Val Loss: 0.13898125290870667\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2616 - val_loss: 0.1390\n",
      "Epoch 22/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2843Epoch 22/50, Loss: 0.28860586881637573, Val Loss: 0.3281770944595337\n",
      "175/175 [==============================] - 0s 974us/step - loss: 0.2886 - val_loss: 0.3282\n",
      "Epoch 23/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.2889Epoch 23/50, Loss: 0.297443151473999, Val Loss: 0.15489579737186432\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2974 - val_loss: 0.1549\n",
      "Epoch 24/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2881Epoch 24/50, Loss: 0.2703944444656372, Val Loss: 0.1906571239233017\n",
      "175/175 [==============================] - 0s 997us/step - loss: 0.2704 - val_loss: 0.1907\n",
      "Epoch 25/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2854Epoch 25/50, Loss: 0.27868422865867615, Val Loss: 0.16762082278728485\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2787 - val_loss: 0.1676\n",
      "Epoch 26/50\n",
      "129/175 [=====================>........] - ETA: 0s - loss: 0.2875Epoch 26/50, Loss: 0.27255505323410034, Val Loss: 0.1672709882259369\n",
      "175/175 [==============================] - 0s 980us/step - loss: 0.2726 - val_loss: 0.1673\n",
      "Epoch 27/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2720Epoch 27/50, Loss: 0.2735123038291931, Val Loss: 0.13589221239089966\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2735 - val_loss: 0.1359\n",
      "Epoch 28/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2615Epoch 28/50, Loss: 0.26962050795555115, Val Loss: 0.3326718807220459\n",
      "175/175 [==============================] - 0s 979us/step - loss: 0.2696 - val_loss: 0.3327\n",
      "Epoch 29/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.3202Epoch 29/50, Loss: 0.30579763650894165, Val Loss: 0.1902272254228592\n",
      "175/175 [==============================] - 0s 980us/step - loss: 0.3058 - val_loss: 0.1902\n",
      "Epoch 30/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2885Epoch 30/50, Loss: 0.28246670961380005, Val Loss: 0.2069484442472458\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2825 - val_loss: 0.2069\n",
      "Epoch 31/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2955Epoch 31/50, Loss: 0.2838130593299866, Val Loss: 0.22748403251171112\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2838 - val_loss: 0.2275\n",
      "Epoch 32/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.3042Epoch 32/50, Loss: 0.28965553641319275, Val Loss: 0.15516367554664612\n",
      "175/175 [==============================] - 0s 983us/step - loss: 0.2897 - val_loss: 0.1552\n",
      "Epoch 33/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2516Epoch 33/50, Loss: 0.2748776972293854, Val Loss: 0.14020894467830658\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.2749 - val_loss: 0.1402\n",
      "Epoch 34/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2601Epoch 34/50, Loss: 0.24987231194972992, Val Loss: 0.15120172500610352\n",
      "175/175 [==============================] - 0s 994us/step - loss: 0.2499 - val_loss: 0.1512\n",
      "Epoch 35/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2724Epoch 35/50, Loss: 0.27415692806243896, Val Loss: 0.19605940580368042\n",
      "175/175 [==============================] - 0s 997us/step - loss: 0.2742 - val_loss: 0.1961\n",
      "Epoch 36/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2551Epoch 36/50, Loss: 0.24621020257472992, Val Loss: 0.1122371181845665\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2462 - val_loss: 0.1122\n",
      "Epoch 37/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2515Epoch 37/50, Loss: 0.25234130024909973, Val Loss: 0.15384754538536072\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2523 - val_loss: 0.1538\n",
      "Epoch 38/50\n",
      "128/175 [====================>.........] - ETA: 0s - loss: 0.2612Epoch 38/50, Loss: 0.2554391622543335, Val Loss: 0.13795459270477295\n",
      "175/175 [==============================] - 0s 977us/step - loss: 0.2554 - val_loss: 0.1380\n",
      "Epoch 39/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2401Epoch 39/50, Loss: 0.23901231586933136, Val Loss: 0.09406702220439911\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2390 - val_loss: 0.0941\n",
      "Epoch 40/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2665Epoch 40/50, Loss: 0.26533034443855286, Val Loss: 0.13237258791923523\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2653 - val_loss: 0.1324\n",
      "Epoch 41/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2626Epoch 41/50, Loss: 0.2642378807067871, Val Loss: 0.1104949414730072\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2642 - val_loss: 0.1105\n",
      "Epoch 42/50\n",
      "124/175 [====================>.........] - ETA: 0s - loss: 0.2514Epoch 42/50, Loss: 0.2529561221599579, Val Loss: 0.1098017767071724\n",
      "175/175 [==============================] - 0s 998us/step - loss: 0.2530 - val_loss: 0.1098\n",
      "Epoch 43/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2365Epoch 43/50, Loss: 0.2350158840417862, Val Loss: 0.0780756026506424\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.2350 - val_loss: 0.0781\n",
      "Epoch 44/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2365Epoch 44/50, Loss: 0.23952028155326843, Val Loss: 0.16567617654800415\n",
      "175/175 [==============================] - 0s 992us/step - loss: 0.2395 - val_loss: 0.1657\n",
      "Epoch 45/50\n",
      "127/175 [====================>.........] - ETA: 0s - loss: 0.2372Epoch 45/50, Loss: 0.2383454293012619, Val Loss: 0.1360943764448166\n",
      "175/175 [==============================] - 0s 985us/step - loss: 0.2383 - val_loss: 0.1361\n",
      "Epoch 46/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2561Epoch 46/50, Loss: 0.24771438539028168, Val Loss: 0.10556324571371078\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2477 - val_loss: 0.1056\n",
      "Epoch 47/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2387Epoch 47/50, Loss: 0.26340335607528687, Val Loss: 0.17849047482013702\n",
      "175/175 [==============================] - 0s 988us/step - loss: 0.2634 - val_loss: 0.1785\n",
      "Epoch 48/50\n",
      "125/175 [====================>.........] - ETA: 0s - loss: 0.2531Epoch 48/50, Loss: 0.255118191242218, Val Loss: 0.11796895414590836\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2551 - val_loss: 0.1180\n",
      "Epoch 49/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2251Epoch 49/50, Loss: 0.228265181183815, Val Loss: 0.16878430545330048\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.2283 - val_loss: 0.1688\n",
      "Epoch 50/50\n",
      "126/175 [====================>.........] - ETA: 0s - loss: 0.2355Epoch 50/50, Loss: 0.23975418508052826, Val Loss: 0.2517523169517517\n",
      "175/175 [==============================] - 0s 986us/step - loss: 0.2398 - val_loss: 0.2518\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"D:\\Krishna\\ai-power-converter\\dataset\\modified_8000_dataset.csv\")\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Scale input features to range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 1\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),          # Hidden layer 2\n",
    "    tf.keras.layers.BatchNormalization(),         \n",
    "    tf.keras.layers.Dense(6, activation='softplus')                               # Output layer with 6 neurons\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.BatchNormalization(input_shape=(3,)),  # Scalarization layer\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 1\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),         \n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),                  # Batch normalization\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),          # Hidden layer 2\n",
    "#     tf.keras.layers.BatchNormalization(),         \n",
    "#     tf.keras.layers.Dense(6, activation='softplus')                               # Output layer with 6 neurons\n",
    "# ])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with custom callback to print information at the end of each epoch\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[PrintEpochInfo()], verbose=1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 606us/step - loss: 0.1632\n",
      "Mean Squared Error on Testing Set: 0.16320562362670898\n",
      "38/38 [==============================] - 0s 499us/step\n",
      "Predictions:\n",
      "[[0.20654827 6.483947   2.6345077  2.6322153  0.01708872 1.3133281 ]\n",
      " [0.12445674 4.282261   3.5189834  3.520649   0.00829306 1.0214925 ]\n",
      " [0.14707306 6.546033   3.3875675  3.4100778  0.00850959 1.2893308 ]\n",
      " [0.06795892 4.3753366  2.3814638  2.3742485  0.01889426 2.3163981 ]\n",
      " [0.06067272 0.20301414 2.079679   2.104424   0.14704187 2.053334  ]]\n",
      "True Values:\n",
      "[[0.26001435 4.23264405 2.74299141 2.74299141 0.01217656 1.29224261]\n",
      " [0.19503469 3.32206237 3.59940991 3.59940991 0.00902757 0.95760415]\n",
      " [0.23933468 4.23420079 3.51435724 3.51435724 0.00733677 1.291914  ]\n",
      " [0.08042573 4.24613856 2.40471154 2.40471154 0.02217949 2.34150863]\n",
      " [0.07897327 0.58997409 2.02413572 2.02413572 0.16765618 2.15456352]]\n",
      "Mean Absolute Error (MAE): 0.13914263041641098\n",
      "Root Mean Squared Error (RMSE): 0.4039871578126952\n",
      "R-squared (R2) Score: 0.9244097104016503\n",
      "Mean Absolute Percentage Error (MAPE): 15.323075467441463%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the first few predictions\n",
    "print(\"Predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# Print the corresponding true values\n",
    "print(\"True Values:\")\n",
    "print(y_test[:5])\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate R-squared (R2) Score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'R-squared (R2) Score: {r2}')\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the testing set\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Testing Set:\", mse)\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save(\"ripples_prediction_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Population (before replacing):\n",
      "[[ 32   6]\n",
      " [128   6]\n",
      " [ 64   2]\n",
      " [512   8]\n",
      " [ 16   2]\n",
      " [128   8]\n",
      " [ 64   8]\n",
      " [256   5]\n",
      " [ 64   3]\n",
      " [ 64   7]\n",
      " [ 16   6]\n",
      " [512   7]]\n",
      "\n",
      "Initial Population (after replacing N and H):\n",
      "[[ 16   6]\n",
      " [128   5]\n",
      " [ 32   5]\n",
      " [ 16   7]\n",
      " [ 16   2]\n",
      " [512   4]\n",
      " [ 64   5]\n",
      " [ 32   8]\n",
      " [ 16   2]\n",
      " [128   3]\n",
      " [512   4]\n",
      " [256   3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the general values for N\n",
    "neuron_values = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "# Define the range for H\n",
    "H_min, H_max = 2, 8\n",
    "\n",
    "# Define the size of the population\n",
    "sol_per_pop = 12\n",
    "pop_size = (sol_per_pop, 3)  # We have 3 variables: N, C, fsw\n",
    "\n",
    "# Generate random values for N and H\n",
    "N_values = np.random.choice(neuron_values, size=(sol_per_pop, 1))  # Random N values\n",
    "H_values = np.random.randint(H_min, H_max + 1, size=(sol_per_pop, 1))  # Random H values\n",
    "\n",
    "# Combine N and H values into the initial population\n",
    "initial_population = np.column_stack((N_values, H_values))\n",
    "\n",
    "print(\"Initial Population (before replacing):\")\n",
    "print(initial_population)\n",
    "\n",
    "# Now let's say you generate new N and H values\n",
    "# For demonstration, I'm just regenerating the same values, but in practice, these would be generated differently\n",
    "new_N_values = np.random.choice(neuron_values, size=(sol_per_pop, 1))  # New random N values\n",
    "new_H_values = np.random.randint(H_min, H_max + 1, size=(sol_per_pop, 1))  # New random H values\n",
    "\n",
    "# Replace the existing N and H values in the initial population with the new ones\n",
    "initial_population[:, :2] = np.column_stack((new_N_values, new_H_values))\n",
    "\n",
    "print(\"\\nInitial Population (after replacing N and H):\")\n",
    "print(initial_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your scaling function\n",
    "def custom_scaling(input_values):\n",
    "    input_values_scaled = np.zeros_like(input_values, dtype=float)  # Initialize scaled data array\n",
    "    input_values_scaled[:, 0] = (input_values[:, 0] - L_min) / (L_max - L_min)  # Scale L\n",
    "    input_values_scaled[:, 1] = (input_values[:, 1] - C_min) / (C_max - C_min)  # Scale C\n",
    "    input_values_scaled[:, 2] = (input_values[:, 2] - fsw_min) / (fsw_max - fsw_min)  # Scale fsw\n",
    "    return input_values_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6268067955970764, Val Loss: 0.28709328174591064\n",
      "Epoch 2/100, Loss: 0.30734753608703613, Val Loss: 0.23412251472473145\n",
      "Epoch 3/100, Loss: 0.2656424641609192, Val Loss: 0.19972191751003265\n",
      "Epoch 4/100, Loss: 0.2500516474246979, Val Loss: 0.1918286830186844\n",
      "Epoch 5/100, Loss: 0.24158038198947906, Val Loss: 0.19080698490142822\n",
      "Epoch 6/100, Loss: 0.2319944053888321, Val Loss: 0.16897359490394592\n",
      "Epoch 7/100, Loss: 0.2259361296892166, Val Loss: 0.15812624990940094\n",
      "Epoch 8/100, Loss: 0.2241038829088211, Val Loss: 0.16767439246177673\n",
      "Epoch 9/100, Loss: 0.21949326992034912, Val Loss: 0.15066780149936676\n",
      "Epoch 10/100, Loss: 0.21718266606330872, Val Loss: 0.1523561030626297\n",
      "Epoch 11/100, Loss: 0.21554400026798248, Val Loss: 0.14291980862617493\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 11: early stopping\n",
      "127/127 [==============================] - 1s 3ms/step\n",
      "Epoch 1/100, Loss: 0.6204947829246521, Val Loss: 0.35535943508148193\n",
      "Epoch 2/100, Loss: 0.34918004274368286, Val Loss: 0.26104235649108887\n",
      "Epoch 3/100, Loss: 0.2894410789012909, Val Loss: 0.20319178700447083\n",
      "Epoch 4/100, Loss: 0.2608550488948822, Val Loss: 0.18803071975708008\n",
      "Epoch 5/100, Loss: 0.24454574286937714, Val Loss: 0.1844235509634018\n",
      "Epoch 6/100, Loss: 0.23353132605552673, Val Loss: 0.16016614437103271\n",
      "Epoch 7/100, Loss: 0.2281254082918167, Val Loss: 0.1658460646867752\n",
      "Epoch 8/100, Loss: 0.22204744815826416, Val Loss: 0.1487879753112793\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 8: early stopping\n",
      "127/127 [==============================] - 1s 3ms/step\n",
      "Epoch 1/100, Loss: 0.7537437081336975, Val Loss: 0.46224528551101685\n",
      "Epoch 2/100, Loss: 0.4283120036125183, Val Loss: 0.3112802505493164\n",
      "Epoch 3/100, Loss: 0.32027608156204224, Val Loss: 0.2218572050333023\n",
      "Epoch 4/100, Loss: 0.2743234634399414, Val Loss: 0.189781054854393\n",
      "Epoch 5/100, Loss: 0.25126323103904724, Val Loss: 0.17520272731781006\n",
      "Epoch 6/100, Loss: 0.24205082654953003, Val Loss: 0.17880849540233612\n",
      "Epoch 7/100, Loss: 0.23615650832653046, Val Loss: 0.15900617837905884\n",
      "Epoch 8/100, Loss: 0.2303704023361206, Val Loss: 0.16571156680583954\n",
      "Epoch 9/100, Loss: 0.2293955683708191, Val Loss: 0.1816219985485077\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 9: early stopping\n",
      "127/127 [==============================] - 1s 4ms/step\n",
      "Fitness Values:\n",
      "[4.40203037 4.77284004 5.27658071]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load your simulation data\n",
    "# Replace \"your_dataset.csv\" with the path to your dataset\n",
    "df = pd.read_csv(r\"E:\\ai-power-converter\\dataset\\simulation_results_new.csv\")\n",
    "\n",
    "# Define custom scaling ranges for each input feature\n",
    "L_min, L_max = 30e-6, 2000e-6\n",
    "C_min, C_max = 20e-6, 1000e-6\n",
    "fsw_min, fsw_max = 20e3, 200e3\n",
    "\n",
    "# Extract input features (L, C, fsw)\n",
    "X = df[['L', 'C', 'fsw']].values\n",
    "\n",
    "# Extract output (ripples)\n",
    "y = df[[\"delta_current\", \"delta_voltage\", \"Pl_s1\", \"Pl_s2\", \"Pl_C\", \"Pl_L_Cu\"]].values  # Adjust column names as per your dataset\n",
    "\n",
    "# Apply custom scaling\n",
    "X_scaled = custom_scaling(X)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Assuming you have already defined your machine learning model and dataset\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "# Define a custom callback to print custom information at the end of each epoch\n",
    "class PrintEpochInfo(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}, Loss: {logs['loss']}, Val Loss: {logs['val_loss']}\")\n",
    "\n",
    "def compute_fitness(N, H):\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(3,)))  # Scalarization layer\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(H):\n",
    "        model.add(tf.keras.layers.Dense(N, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softplus'))  # Output layer with 6 neurons\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "    # Train the model with given hyperparameters\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), \n",
    "                        callbacks=[PrintEpochInfo(), lr_scheduler, early_stopping], verbose=0)\n",
    "    \n",
    "    # Evaluate the model on validation data to compute the fitness value\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    # Fitness value is the inverse of the validation loss (lower loss is better)\n",
    "    fitness = 1 / (val_loss + 1e-10)  # Adding a small value to prevent division by zero\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "# Example initial population\n",
    "initial_population = np.array([[32, 2],\n",
    "                               [64, 3],\n",
    "                               [128, 4]])\n",
    "\n",
    "# Compute fitness values for each individual in the initial population\n",
    "fitness_values = np.zeros(initial_population.shape[0])\n",
    "for i in range(initial_population.shape[0]):\n",
    "    N = initial_population[i, 0]\n",
    "    H = initial_population[i, 1]\n",
    "    fitness_values[i] = compute_fitness(N, H)\n",
    "\n",
    "print(\"Fitness Values:\")\n",
    "print(fitness_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(offspring_crossover, mutation_ranges, num_mutations=1):\n",
    "    mutated_offspring = np.copy(offspring_crossover)\n",
    "    num_genes = mutated_offspring.shape[1]\n",
    "    \n",
    "    # Mutation for N (number of neurons)\n",
    "    for idx in range(mutated_offspring.shape[0]):  # Iterate over each individual in the population\n",
    "        # Mutation for N: randomly select a value from the provided range\n",
    "        mutated_N = np.random.choice(mutation_ranges[0])\n",
    "        mutated_offspring[idx, 0] = mutated_N\n",
    "    \n",
    "    # Mutation for H (number of layers)\n",
    "    for idx in range(mutated_offspring.shape[0]):  # Iterate over each individual in the population\n",
    "        # Mutation for H: randomly increment or decrement H by 1\n",
    "        mutation_value_H = np.random.choice([-1, 1])\n",
    "        mutated_H = mutated_offspring[idx, 1] + mutation_value_H\n",
    "        \n",
    "        # Ensure the mutated value of H remains within the provided range\n",
    "        mutated_H = np.clip(mutated_H, min(mutation_ranges[1]), max(mutation_ranges[1]))\n",
    "        mutated_offspring[idx, 1] = mutated_H\n",
    "    \n",
    "    return mutated_offspring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutated Offspring:\n",
      "[[ 64   2]\n",
      " [ 64   7]\n",
      " [ 16   3]\n",
      " [ 64   5]\n",
      " [ 32   8]\n",
      " [ 64   4]\n",
      " [ 16   6]\n",
      " [128   2]\n",
      " [128   5]\n",
      " [ 32   2]\n",
      " [128   7]\n",
      " [128   3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "offspring_crossover = np.array([[32, 3],   # Sample individual 1: N=32, H=3\n",
    "                                [64, 6],   # Sample individual 2: N=64, H=6\n",
    "                                [128, 2],  # Sample individual 3: N=128, H=2\n",
    "                                [16, 4],   # Sample individual 4: N=16, H=4\n",
    "                                [32, 7],   # Sample individual 5: N=32, H=7\n",
    "                                [64, 3],   # Sample individual 6: N=64, H=3\n",
    "                                [64, 5],   # Sample individual 7: N=64, H=5\n",
    "                                [128, 2],  # Sample individual 8: N=128, H=2\n",
    "                                [32, 6],   # Sample individual 9: N=32, H=6\n",
    "                                [128, 3],  # Sample individual 10: N=128, H=3\n",
    "                                [64, 8],   # Sample individual 11: N=64, H=8\n",
    "                                [32, 4]])  # Sample individual 12: N=32, H=4\n",
    "\n",
    "\n",
    "# Define mutation ranges for N and H\n",
    "mutation_ranges = [[16, 32, 64, 128],   # Possible values for N (number of neurons)\n",
    "                   (2, 8)]              # Range for H (number of layers)\n",
    "\n",
    "\n",
    "\n",
    "# Number of mutations to perform\n",
    "num_mutations = 1\n",
    "\n",
    "# Function call to perform mutation\n",
    "mutated_offspring = mutation(offspring_crossover, mutation_ranges, num_mutations)\n",
    "\n",
    "# Display mutated offspring\n",
    "print(\"Mutated Offspring:\")\n",
    "print(mutated_offspring)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
